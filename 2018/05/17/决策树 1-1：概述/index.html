<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="ml,tree," />





  <link rel="alternate" href="/atom.xml" title="Hi!" type="application/atom+xml" />






<meta name="description" content="决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。 决策树介绍通俗来说，决策树分类的思想类似于找对象，先想象一个女孩的母亲">
<meta name="keywords" content="ml,tree">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树 1-1：概述">
<meta property="og:url" content="http://conghuai.me/2018/05/17/决策树 1-1：概述/index.html">
<meta property="og:site_name" content="Hi!">
<meta property="og:description" content="决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。 决策树介绍通俗来说，决策树分类的思想类似于找对象，先想象一个女孩的母亲">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160930.jpg">
<meta property="og:image" content="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160935.jpg">
<meta property="og:image" content="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160936.jpg">
<meta property="og:image" content="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160934.jpg">
<meta property="og:image" content="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160938.jpg">
<meta property="og:image" content="http://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_0011.png">
<meta property="og:updated_time" content="2018-11-22T06:15:44.994Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树 1-1：概述">
<meta name="twitter:description" content="决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。 决策树介绍通俗来说，决策树分类的思想类似于找对象，先想象一个女孩的母亲">
<meta name="twitter:image" content="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160930.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://conghuai.me/2018/05/17/决策树 1-1：概述/"/>





  <title>决策树 1-1：概述 | Hi!</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hi!</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://conghuai.me/2018/05/17/决策树 1-1：概述/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="独木舟">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hi!">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">决策树 1-1：概述</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-17T23:29:57+08:00">
                2018-05-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/分类算法/" itemprop="url" rel="index">
                    <span itemprop="name">分类算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/05/17/决策树 1-1：概述/" class="leancloud_visitors" data-flag-title="决策树 1-1：概述">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  6,031 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  22 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。</p>
<h1 id="决策树介绍"><a href="#决策树介绍" class="headerlink" title="决策树介绍"></a>决策树介绍</h1><p>通俗来说，决策树分类的思想类似于找对象，先想象一个女孩的母亲要给这个女孩介绍男朋友，于是有了下面的对话:</p>
<p>女儿： 多大年纪了</p>
<p>母亲：26</p>
<p>女儿：长得帅不帅</p>
<p>母亲：挺帅的</p>
<p>女儿：收入高不高</p>
<p>母亲：不算太高，中等情况。</p>
<p>女儿：是公务员不？</p>
<p>母亲：是，在税务局上班呢。</p>
<p>女儿：那好，我去见见。</p>
<p>这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员将男人分为两个类别：见或不见。我们可以将上面的决策过程抽象成一颗决策树:</p>
<p><img src="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160930.jpg" alt="1_3"></p>
<p>上图完整表达了这个女孩决定是否见一个约会对象的策略， 其中绿色节点表示判断条件，橙色节点表示决策过程，箭头表示在一个判断条件在不同情况下的决策路径，途中红色表示了上面例子中女孩的决策过程。有了上面的直观的认识，我们可以正式定义决策树：</p>
<p>决策树是一个树结构。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。</p>
<p>不同于贝叶斯算法，决策树的构造过程不依赖领域知识，它使用属性选择度量来选择将元组最好的划分成不同的属性。所谓决策树的构造就是进行属性选择度量确定各个特征属性之间的拓扑结构。</p>
<p>构造决策树的关键步骤是分裂属性。所谓分类属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能的“纯”。也就是尽量让一个分裂子集中待分裂项属于同一类别。分裂属性分为三种不同情况:</p>
<ol>
<li>属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。</li>
<li>属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。</li>
<li>属性是连续值，此时确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支。</li>
</ol>
<p>构造决策树的关键性内容是进行属性选择度量，属性选择度量是一种选择分裂准则， 是将给定的类标记的训练集合的数据划分进行最好的划分成个体类的启发式方法，它决定了拓扑结构及分裂点split_point的选择。</p>
<p>属性选择度量算法有很多，一般使用自顶向下递归分治法，并采用不回溯的贪心策略。经典算法有ID3和C4.5。</p>
<h1 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h1><blockquote>
<p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。</p>
</blockquote>
<p>假设，我们现在有如下的数据，该数据通过表格来展示:</p>
<table>
<thead>
<tr>
<th>日期</th>
<th>天气</th>
<th>湿度</th>
<th>风级</th>
<th>打球</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>晴</td>
<td>高</td>
<td>弱</td>
<td>否</td>
</tr>
<tr>
<td>2</td>
<td>晴</td>
<td>高</td>
<td>强</td>
<td>否</td>
</tr>
<tr>
<td>3</td>
<td>阴</td>
<td>高</td>
<td>弱</td>
<td>是</td>
</tr>
<tr>
<td>4</td>
<td>雨</td>
<td>高</td>
<td>弱</td>
<td>是</td>
</tr>
<tr>
<td>5</td>
<td>雨</td>
<td>正常</td>
<td>弱</td>
<td>是</td>
</tr>
<tr>
<td>6</td>
<td>雨</td>
<td>正常</td>
<td>弱</td>
<td>是</td>
</tr>
<tr>
<td>7</td>
<td>阴</td>
<td>正常</td>
<td>强</td>
<td>是</td>
</tr>
<tr>
<td>8</td>
<td>晴</td>
<td>高</td>
<td>弱</td>
<td>否</td>
</tr>
<tr>
<td>9</td>
<td>晴</td>
<td>正常</td>
<td>弱</td>
<td>是</td>
</tr>
<tr>
<td>10</td>
<td>雨</td>
<td>正常</td>
<td>弱</td>
<td>是</td>
</tr>
<tr>
<td>11</td>
<td>晴</td>
<td>正常</td>
<td>强</td>
<td>是</td>
</tr>
<tr>
<td>12</td>
<td>阴</td>
<td>高</td>
<td>强</td>
<td>是</td>
</tr>
<tr>
<td>13</td>
<td>阴</td>
<td>正常</td>
<td>弱</td>
<td>是</td>
</tr>
<tr>
<td>14</td>
<td>雨</td>
<td>高</td>
<td>强</td>
<td>否</td>
</tr>
</tbody>
</table>
<p>决策树是采用分治思想来做的，把数据集按照各个属性进行分割，属性的不同取值可以得到不同的子数据集，做完分割后，判断数据点被完美区分开了没，如果区分开了，说明该次划分具有比较好的划分性。算法应该考虑用什么样的指标来判断这次划分的好坏。以及如何在多个属性中选择最好的划分属性。</p>
<h2 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h2><p>将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上的内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备。</p>
<h2 id="决策树与条件概率分布"><a href="#决策树与条件概率分布" class="headerlink" title="决策树与条件概率分布"></a>决策树与条件概率分布</h2><p>决策树还表示给定特征条件下类的概率分布，这一条件概率分布定义在特征空间的一个划分上，将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$，X取值于给定划分下单元的集合，Y取值于类的集合。各叶节点上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。</p>
<h2 id="决策树的学习"><a href="#决策树的学习" class="headerlink" title="决策树的学习"></a>决策树的学习</h2><p>决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这一个过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根节点，选择一个最优特征，按照这一特征训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到对应的叶结点中取；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点，如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。</p>
<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。给出例子为如下</p>
<p><img src="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160935.jpg" alt="Screen Shot 2018-05-19 at 15.33.07"></p>
<h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>在信息论与概率统计中，熵是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：</p>
<p>$$P(X = x_i)=p_i,i=1,2,…,n$$</p>
<p>则随机变量$X$的熵定义为</p>
<p>$$H(X) = - \sum_{i=1}^np_ilogp_i$$</p>
<p>熵越大，随机变量的不确定性就越大，从定义可验证</p>
<p>$$0\leq H(p)\leq logn$$</p>
<h2 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h2><p>设随机变量$(X,Y)$，其联合概率分布为</p>
<p>$$P(X=x_i,Y=y_j)=p_{ij}\text{ , i=1,2,…,n; j=1,2,…,m}$$</p>
<p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为$X$给定条件下$Y$的条件概率分布的上对$X$的数学期望</p>
<p>$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$$</p>
<p>这里，$p_i=P(X=x_i)\text{ i=1,2,..,n}$</p>
<h2 id="信息增益值"><a href="#信息增益值" class="headerlink" title="信息增益值"></a>信息增益值</h2><p>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即</p>
<p>$$g(D,A)=H(D)-H(D|A)$$</p>
<p>一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类的特征的互信息。</p>
<p>计算步骤：</p>
<ol>
<li><p>计算数据集D的经验熵$H(D)$</p>
<p>$$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{|C_k|}{D}$$</p>
<p>套用上面的公式得：$H(D)=-\frac{9}{15}log_2\frac{9}{15}-\frac{6}{15}log_2\frac{6}{15}=0.971$</p>
</li>
<li><p>计算特征A对数据集D的经验条件熵$H(D|A)$</p>
<p>$$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}$$</p>
<p>特征有$A_1,A_2,A_3,A_4$分别表示年龄、有工作、有自己的房子和信贷情况，则</p>
<p>$$\begin{align} g(D,A_1) &amp;= H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)] \\  &amp;= 0.971-[\frac{5}{15}(-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5})+\frac{5}{15}(-\frac{3}{5}log_2\frac{3}{5}-\frac{2}{5}log_2\frac{2}{5})+\frac{5}{15}(-\frac{4}{5}log_2\frac{4}{5}-\frac{1}{5}log_2\frac{1}{5})] \\&amp;= 0.971-0.888 \\&amp;=0.083   \\ \end{align}$$</p>
<p>$$\begin{align} g(D,A_2) &amp;= H(D)-[\frac{5}{15}H(D_1)+\frac{10}{15}H(D_2)] \\  &amp;= 0.971-[\frac{5}{15}*0+\frac{10}{15}(-\frac{4}{10}log_2\frac{4}{10}-\frac{6}{10}log_2\frac{6}{10})]  \\&amp;=0.324  \\ \end{align}$$</p>
<p>$$g(D,A_3) =0.420$$</p>
<p>$$g(D,A_4) =0.363$$</p>
</li>
<li><p>最后，比较个特征的信息增益值，选择信息增益值最大的特征作为最优特征。</p>
</li>
</ol>
<h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><p>对于取值非常多的特征，其划分后，得到的信息增益值比较大，换句话说，信息增益这一个度量指标偏向于多值特征。采用信息增益比可以对这一问题进行校正：</p>
<p>$$g_R(D,A)=\frac{g(D,A)}{H(D)}$$</p>
<h2 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h2><p>假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为</p>
<p>$$Gini(D)=1-\sum_{k=1}^{|y|}p_k^2$$</p>
<p>在特征a的条件下，集合D的基尼指数定义为</p>
<p>$$Gini_index(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$$</p>
<h2 id="均方差（回归）"><a href="#均方差（回归）" class="headerlink" title="均方差（回归）"></a>均方差（回归）</h2><h1 id="终止条件"><a href="#终止条件" class="headerlink" title="终止条件"></a>终止条件</h1><p>递归的选择属性分裂子树，那么什么时候递归终止呢？其终止条件如下：</p>
<ol>
<li># of leaves.</li>
<li>tree depth.</li>
<li># of instances in current node.</li>
</ol>
<h1 id="常见算法"><a href="#常见算法" class="headerlink" title="常见算法"></a>常见算法</h1><p>经典的决策树算法诞生于以下经典论文</p>
<ol>
<li>J. Quinlan, 1986, ID3（Classification）</li>
<li>J. Quinlan, 1993, C4.5（Classification）</li>
<li>L. Breiman and J. Friedman, 1984, CART（Classification &amp; Regression）</li>
</ol>
<h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><ul>
<li>以信息增益度量属性选择，选择分裂后信息增益最大的属性进行划分；</li>
<li>属性只能取离散值(为了使决策树能应用于连续属性值情况，可以使用ID3的一个扩展算法C4.5算法)；</li>
<li>偏向多值属性。</li>
</ul>
<h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>这次我们每次进行选取特征属性的时候，不再使用ID3算法的信息增益，而是使用了信息增益率这个概念。</p>
<ul>
<li>使用信息增益率；</li>
<li>偏向少值属性；</li>
<li>不直接选择增益率最大的候选划分属性，候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</li>
</ul>
<h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>分类与回归树（classification and regression tree, CART）模型既可以用于分类也可以用于回归。</p>
<ul>
<li>假设决策树是二叉树；</li>
<li>对回归树用平方误差最小化准则；对分类树用基尼指数最小化准则，进行特征选择，生成二叉树；</li>
</ul>
<h3 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h3><blockquote>
<p>一个回归树对应着输入空间的一个划分以及在划分的单元上的输出值，假设已将输入空间划分为M个单元$R_1,R_2,…,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为：</p>
<p>$$f(x)=\sum_{m=1}^Mc_mI(x\in R_m)$$</p>
<p>当输入空间的划分确定时，可以用平方误差最小的准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值$\hat{c_m}$是$R_m$上的所有输入实例$x_i$对应的输出$y_i$的均值，即</p>
<p>$$\hat{c_m}=ave(y_i|x_i\in R_m)$$</p>
</blockquote>
<p>问题是怎样对输入空间进行划分呢？</p>
<p>这里采用启发式的方法，选择第j个变量$x^{(j)}$和它取的值$s$，作为切分变量和切分点，并定义两个区域：</p>
<p>$$R_1(j,s)=\{x|x^{(j)}\leq s\}\text{ 和 } R_2(j,s)=\{x|x^{(j)}&gt;s\}$$</p>
<p>然后寻找最优切分变量$j$和最优切分点$s$，具体的，求解：</p>
<p>$$min_{j,s}[min_{c_1}\sum_{x_1\in R_1(j,s)}(y_i-c_1)^2+min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$</p>
<p>对固定输入变量$j$可以找到最优切分点$s$：</p>
<p>$$\hat{c_1}=ave(y_i|x_i\in R(j,s))\text{ 和 } \hat{c_2}=ave(y_i|x_i\in R_2(j,s))$$</p>
<p>遍历所有的输入变量，找到最优的切分变量$j$，构成一个对(j,s)，依次将输入空间划分为两个区域。<img src="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160936.jpg" alt="回归树"></p>
<p><img src="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160934.jpg" alt="生成"></p>
<h3 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h3><p>用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<p><img src="https://media-1253434227.cos.ap-chengdu.myqcloud.com/2018-05-22-160938.jpg" alt="分类树"></p>
<h2 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h2><p>决策树为什么要剪枝？原因是避免决策树过拟合样本。前面的算法生产的决策树非常详细并且庞大，每个属性都被详细地加以考虑，决策树的树叶节点所覆盖的训练样本都是纯的，因此用这个决策树来对训练样本进行分类的话，你会发现对于训练样本而言，这个树表现完好，误差率极地且能够正确的对训练样本集中的样本进行分类。但是，对于测试数据的表现就没有想象的那么好，或者极差，这就是所谓的过拟合问题。</p>
<p>现在的问题在于，如何在原生的过拟合决策树的基础上，生成简化版的决策树？可以通过剪枝的方法来简化过拟合的决策树。</p>
<p>剪枝可以分为两种：预剪枝和后剪枝。</p>
<ul>
<li><p>预剪枝</p>
<p>及早的停止树增长。</p>
</li>
<li><p>后剪枝</p>
<p>在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。</p>
</li>
</ul>
<p>剪枝的准则是如何确定决策树的规模，可以参考的剪枝思路有以下几个:</p>
<ol>
<li>使用训练集合和验证集合，来评估剪枝方法在修剪节点上的效用；</li>
<li>使用所有的训练集合进行训练，但是用统计测试来估计修剪特定节点是否会改善训练集合外的数据的评估性能，如使用Chi-Square测试来进一步扩展节点是否能改善整个分类数据的性能，还是仅仅改善了当前训练集合数据上的性能。</li>
</ol>
<p>后剪枝的算法有很多，这里列举出常用的三种算法:</p>
<h2 id="Reduced-Error-Pruning-REP-错误率降低剪枝"><a href="#Reduced-Error-Pruning-REP-错误率降低剪枝" class="headerlink" title="Reduced-Error Pruning(REP, 错误率降低剪枝)"></a>Reduced-Error Pruning(REP, 错误率降低剪枝)</h2><p>该剪枝方法考虑将树上的每个节点作为修剪的候选对象，决定是否修剪这个节点有如下步骤组成：</p>
<ul>
<li>删除以该节点为根的子树；</li>
<li>使其成为叶子结点；</li>
<li>赋予该结点关联的训练数据最常见分类；</li>
<li>当修剪后的树对于验证集合的性能不会比原来的树差时，才真正删除该结点。</li>
<li>因为训练集合的过拟合，使得验证集合数据能够对其进行修正，反复进行上面的操作，从底向上的处理结点，删除那些能够最大限度的提高验证集合的精度的节点，直到进一步修剪有害为止。</li>
</ul>
<p>REP是最简单的后剪枝方法之一，不过在数据量比较少的情况下，REP方法趋于过拟合而较少使用。这是因为训练数据集合中的特性在剪枝过程中被忽略，所以在验证数据集合比训练数据集合小的多时，要注意这个问题。尽管REP有这个缺点，不过REP仍然作为一种基准来评价其他剪枝算法的性能。它对于两阶段决策树学习方法的优点和缺点提供了一种很好的学习思路。由于验证集合没有参与决策树的创建，所以用REP剪枝后的决策树对于测试样例的偏差要好很多，能够解决一定程度的过拟合问题。</p>
<h2 id="Pessimistic-Error-Pruning-PEP-悲观剪枝"><a href="#Pessimistic-Error-Pruning-PEP-悲观剪枝" class="headerlink" title="Pessimistic Error Pruning(PEP, 悲观剪枝)"></a>Pessimistic Error Pruning(PEP, 悲观剪枝)</h2><p>PEP剪枝算法是在C4.5决策树算法中提出的，把一颗子树(具有多个叶子节点)用一个叶子节点来替代的话，比起REP剪枝法，它不需要一个单独的测试数据集。对于这一部分，仍然需要更进一步的学习，后续如果有时间在进行相关的研究。</p>
<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>前面对决策树的概念，分类进行了介绍，并扩展了集成学习的一些思想和算法。这里我们来学习一下sklearn这个机器学习类库中的关于决策树的一些说明和工具。后面会用一些例子在实际项目中使用决策树这个算法，以及算法融合的思想。之前介绍的决策树算法有ID3，C4.5和CART，sklearn中采用的是最佳的CART算法。</p>
<h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><p>优点：</p>
<ul>
<li>需要较少的数据预处理；</li>
<li>支持类别特征；</li>
<li>容易理解和实现。</li>
</ul>
<p>缺点：</p>
<ul>
<li>容易过拟合；</li>
<li>模型拟合能力不强。</li>
</ul>
<h1 id="实战sklearn"><a href="#实战sklearn" class="headerlink" title="实战sklearn"></a>实战sklearn</h1><p>决策树是一个非参数的监督式学习方法，主要用于分类和回归。算法的目标是通过推断数据特征，学习决策规则从而创建一个预测目标变量的模型。如下所示，决策树通过一系列if-then-else决策规则，近似估计一个正弦曲线。</p>
<p><img src="http://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_0011.png" alt=""></p>
<p>决策树的优势:</p>
<ul>
<li>简单易懂，原理清晰，决策树可以实现可视化。</li>
<li>数据准备简单。其他的方法需要实现数据归一化，创建虚拟变量，删除空白变量。(注意：这个模块不支持缺失值)</li>
<li>使用决策树的代价是数据点的对数级别。</li>
<li>能够处理数值和分类数据</li>
<li>能够处理多路输出问题</li>
<li>使用白盒子模型(内部结构可以直接观测的模型)。一个给定的情况是可以观测的，那么就可以用布尔逻辑解释这个结果。相反，如果在一个黑盒模型(ANN)，结果可能很难解释</li>
<li>可以通过统计学检验验证模型。这也使得模型的可靠性计算变得可能</li>
<li>即使模型假设违反产生数据的真实模型，表现性能依旧很好。</li>
</ul>
<p>决策树劣势:</p>
<ul>
<li>可能会建立过于复杂的规则，即过拟合。为避免这个问题，剪枝、设置叶节点的最小样本数量、设置决策树的最大深度有时候是必要的。</li>
<li>决策树有时候是不稳定的，因为数据微小的变动，可能生成完全不同的决策树。 可以通过总体平均(ensemble)减缓这个问题。应该指的是多次实验。</li>
<li>学习最优决策树是一个NP完全问题。所以，实际决策树学习算法是基于试探性算法，例如在每个节点实现局部最优值的贪心算法。这样的算法是无法保证返回一个全局最优的决策树。可以通过随机选择特征和样本训练多个决策树来缓解这个问题。</li>
<li>有些问题学习起来非常难，因为决策树很难表达。如：异或问题、奇偶校验或多路复用器问题</li>
<li>如果有些因素占据支配地位，决策树是有偏的。因此建议在拟合决策树之前先平衡数据的影响因子。</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>我们先介绍一下DecisionTreeClassifier这个类，它用于多分类问题。</p>
<h3 id="类签名"><a href="#类签名" class="headerlink" title="类签名"></a>类签名</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">tree</span>.<span class="title">DecisionTreeClassifier</span><span class="params">(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=None, class_weight=None, presort=False)</span></span></span><br></pre></td></tr></table></figure>
<h3 id="构造器参数说明"><a href="#构造器参数说明" class="headerlink" title="构造器参数说明"></a>构造器参数说明</h3><ul>
<li><p><code>criterion</code>: string, optional(default=’gini’)</p>
<p>度量划分属性质量:”gini”指基尼不纯度;”entropy”指信息增益。</p>
</li>
<li><p><code>splitter</code>: string, optional(default=”best”)</p>
<p>每个节点选择划分属性的策略: “best”选择最好的划分，”random”选择最好的随机划分</p>
</li>
<li><p><code>max_depth</code> : int or None, optional(default=None)</p>
<p>树的最大深度</p>
</li>
<li><p><code>min_samples_split</code> : int, float, optional(default=2)</p>
<p>划分一个内部节点所需的最少样本数。</p>
</li>
<li><p><code>min_samples_leaf</code> ：int, float, optional(default=1)</p>
<p>划分一个叶子节点所需的最少样本数。</p>
</li>
<li><p><code>min_weight_fraction_leaf</code> : float, optional(default=0)</p>
<p>限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。</p>
</li>
<li><p><code>max_features</code>: int, float, string or None, optional(default=None)</p>
<p>选择划分时所用的最大特征数目</p>
</li>
<li><p><code>random_state</code>: int, RandomState instances o None, optional(default=None)</p>
<p>随机种子</p>
</li>
<li><p><code>max_leaf_nodes</code>: int or None, optional(default=None)</p>
<p>最大的叶子数量。</p>
</li>
<li><p><code>min_impurity_decrease</code> : float, optional(default=0)</p>
<p>如果划分后的纯度减少量大于这个值，那么这个节点就会被划分。</p>
</li>
<li><p><code>min_impurity_split</code>: float</p>
<p>阈值，用于控制节点是否进一步划分，如果大于这个值则需要进一步划分，否则就停止划分。已经deprecated掉了，用上面那个参数。</p>
</li>
<li><p><code>class_weight</code> :dict, list of dicts, “balanced” or None, default=None</p>
<p>与预测类别相关的权重dict, 以{class_label: weight}这种方式传入，当然了，如果有多个类别输出，那就传入这种dict的list。如果传入的是balance，那么将用根据样本数来决定类别的权重。</p>
<p>n_samples / (n_classes * np.bincount(y))</p>
</li>
<li><p><code>presort</code>: bool, optional(default=None)</p>
<p>是否预划分。如果预划分，那么在小数据集上可能加快速度，在大数据集上可能会变慢。</p>
</li>
</ul>
<h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><ul>
<li><code>classes_</code><ul>
<li>类标</li>
</ul>
</li>
<li><code>feature_importances_</code><ul>
<li>特征的重要性。越高，特征越重要。</li>
</ul>
</li>
<li><code>max_featrues_</code><ul>
<li>推断的价值  不知道是啥</li>
</ul>
</li>
<li><code>n_classes_</code><ul>
<li>类的数量</li>
</ul>
</li>
<li><code>n_features_</code><ul>
<li>特征的数量</li>
</ul>
</li>
<li><code>n_outputs_</code><ul>
<li>输出的数量</li>
</ul>
</li>
<li><code>tree_</code><ul>
<li>底层树对象</li>
</ul>
</li>
</ul>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul>
<li><code>apply(X[, check_input])</code><ul>
<li>返回样本被预测为叶子的索引</li>
</ul>
</li>
<li><code>decision_path(X[, check_input])</code><ul>
<li>返回样本的决策路径</li>
</ul>
</li>
<li><code>fit(X, y[, sample_weight, check_input, …])</code><ul>
<li>从(X,y)训练数据中创建一颗决策树</li>
</ul>
</li>
<li><code>get_params([deep])</code><ul>
<li>获得参数</li>
</ul>
</li>
<li><code>predict(X[, check_input])</code><ul>
<li>预测样本类别或连续值</li>
</ul>
</li>
<li><code>predict_log_proba(X)</code><ul>
<li>预测输入数据的log概率</li>
</ul>
</li>
<li><code>predict_proba(X,[, check_input])</code><ul>
<li>预测输入数据的概率</li>
</ul>
</li>
<li>score(X,y[, sample_weight])<ul>
<li>返回平均正确率</li>
</ul>
</li>
<li>set_params(**params)<ul>
<li>设置参数</li>
</ul>
</li>
</ul>
<h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><h3 id="类签名-1"><a href="#类签名-1" class="headerlink" title="类签名"></a>类签名</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">tree</span>.<span class="title">DecisionTreeRegressor</span><span class="params">(criterion=’mse’, splitter=’best’, max_depth=None, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=None, presort=False)</span>[<span class="title">source</span>]</span></span><br></pre></td></tr></table></figure>
<p>这些参数，属性和方法与上面介绍的类似。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” <em>清华大学出版社, 北京</em> (2012).</p>
<p>[2]. <a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="noopener">算法杂货铺——分类算法之决策树(Decision tree)</a></p>
<p>[3]. <a href="https://zhuanlan.zhihu.com/p/26760551" target="_blank" rel="noopener">知乎专栏</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/ml/" rel="tag"># ml</a>
          
            <a href="/tags/tree/" rel="tag"># tree</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/16/Classic-Convolutional-Network/" rel="next" title="Classic Convolutional Network">
                <i class="fa fa-chevron-left"></i> Classic Convolutional Network
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/05/20/Recurrent-Neural-Network-Foundation/" rel="prev" title="Recurrent Neural Network Foundation">
                Recurrent Neural Network Foundation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpeg"
                alt="独木舟" />
            
              <p class="site-author-name" itemprop="name">独木舟</p>
              <p class="site-description motion-element" itemprop="description">Algorithm/Machine Learning/Deep Learning/NLP</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">41</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/caiconghuai" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:caiconghuai@gmail.com" target="_blank" title="Email">
                      
                        <i class="fa fa-fw fa-envelope"></i>Email</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树介绍"><span class="nav-number">1.</span> <span class="nav-text">决策树介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树模型"><span class="nav-number">2.</span> <span class="nav-text">决策树模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树与if-then规则"><span class="nav-number">2.1.</span> <span class="nav-text">决策树与if-then规则</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树与条件概率分布"><span class="nav-number">2.2.</span> <span class="nav-text">决策树与条件概率分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树的学习"><span class="nav-number">2.3.</span> <span class="nav-text">决策树的学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#特征选择"><span class="nav-number">3.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#熵"><span class="nav-number">3.1.</span> <span class="nav-text">熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#条件熵"><span class="nav-number">3.2.</span> <span class="nav-text">条件熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#信息增益值"><span class="nav-number">3.3.</span> <span class="nav-text">信息增益值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#信息增益比"><span class="nav-number">3.4.</span> <span class="nav-text">信息增益比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基尼指数"><span class="nav-number">3.5.</span> <span class="nav-text">基尼指数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#均方差（回归）"><span class="nav-number">3.6.</span> <span class="nav-text">均方差（回归）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#终止条件"><span class="nav-number">4.</span> <span class="nav-text">终止条件</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#常见算法"><span class="nav-number">5.</span> <span class="nav-text">常见算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ID3"><span class="nav-number">5.1.</span> <span class="nav-text">ID3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C4-5"><span class="nav-number">5.2.</span> <span class="nav-text">C4.5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CART"><span class="nav-number">5.3.</span> <span class="nav-text">CART</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#回归树的生成"><span class="nav-number">5.3.1.</span> <span class="nav-text">回归树的生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类树的生成"><span class="nav-number">5.3.2.</span> <span class="nav-text">分类树的生成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#剪枝"><span class="nav-number">5.4.</span> <span class="nav-text">剪枝</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reduced-Error-Pruning-REP-错误率降低剪枝"><span class="nav-number">5.5.</span> <span class="nav-text">Reduced-Error Pruning(REP, 错误率降低剪枝)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pessimistic-Error-Pruning-PEP-悲观剪枝"><span class="nav-number">5.6.</span> <span class="nav-text">Pessimistic Error Pruning(PEP, 悲观剪枝)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Preface"><span class="nav-number">6.</span> <span class="nav-text">Preface</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#优缺点"><span class="nav-number">7.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实战sklearn"><span class="nav-number">8.</span> <span class="nav-text">实战sklearn</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#分类"><span class="nav-number">8.1.</span> <span class="nav-text">分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#类签名"><span class="nav-number">8.1.1.</span> <span class="nav-text">类签名</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#构造器参数说明"><span class="nav-number">8.1.2.</span> <span class="nav-text">构造器参数说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#属性"><span class="nav-number">8.1.3.</span> <span class="nav-text">属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#方法"><span class="nav-number">8.1.4.</span> <span class="nav-text">方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#回归"><span class="nav-number">8.2.</span> <span class="nav-text">回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#类签名-1"><span class="nav-number">8.2.1.</span> <span class="nav-text">类签名</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">9.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">独木舟</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("gDGQAvMXvJB5uSIhpUvw90hy-gzGzoHsz", "36U43UE0rMrmve1e0J8NOBhA");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
