<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>蔡聪怀</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://conghuai.me/"/>
  <updated>2018-09-25T03:46:25.701Z</updated>
  <id>http://conghuai.me/</id>
  
  <author>
    <name>Conghuai Cai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>支持向量机 1-3：非线性支持向量机</title>
    <link href="http://conghuai.me/2018/09/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%201-3%EF%BC%9A%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://conghuai.me/2018/09/25/支持向量机 1-3：非线性支持向量机/</id>
    <published>2018-09-25T03:36:12.000Z</published>
    <updated>2018-09-25T03:46:25.701Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090106.jpg" alt="|center|"></p><h1 id="非线性支持向量机与核函数"><a href="#非线性支持向量机与核函数" class="headerlink" title="非线性支持向量机与核函数"></a>非线性支持向量机与核函数</h1><p>对解线性分类问题，线性分类支持向量机是一种非常有效的方法。但是，有时候分类问题是非线性的，这时可以使用非线性支持向量机。其主要特点是利用核技巧。</p><h2 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-074025.jpg" alt="|center|"></p><p>如图所示的数据集，其理想的分界应该是一个“圆圈”而不是一条线（超平面）。如果用$X_1$和$X_2$来表示这个二维平面的两个坐标的话，一条二次曲线的方差可以写成主要的形式</p><p>$$a_1X_1+a_2X_1^2+a_3X_2+a_4X_2^2+a_5X_1X_2+a_6=0$$</p><p>注意上面的形式，如果我们构建另外一个五维的空间，其中五个坐标的值分别为$Z_1=X_1,Z_2=X_1^2,Z_3=X_2,Z_4=X_2^2,Z_5=X_1X_2$，那么显然，上面的方程在新的坐标系下可以写作</p><p>$$\sum_{i=1}^5a_iZ_i+a_6=0$$</p><p>关于新的坐标$Z$，这正是一个超平面的方差，也就是说，如果我们做一个映射$\varphi :R^2\rightarrow R^5$，将上面的规则映射为$Z$，那么在新的空间中原来的数据将变成线性可分的。</p><p>上面的例子说明，用线性分类方法求解非线性分类问题分为两步：（1）首先使用一个变换将原空间中的数据映射到新空间；（2）然后再新空间里用线性分类学习方法从训练数据中学习分类模型。</p><blockquote><p>核函数的定义</p><p>设$\chi $是输入空间（欧式空间$R^n$的子集或离散集合），又设$H$为特征空间，如果存在一个从$\chi$到$H$的映射</p><p>$$\varphi (x):\chi\rightarrow H$$</p><p>使得对所有$x,z\in\chi$，函数$K(x,z)$满足条件</p><p>$$K(x,z)=\varphi(x)\cdot \varphi(z)$$</p><p>则称$K(x,z)$为核函数，$\varphi$(x)为映射函数，式中$\varphi(x)\cdot \varphi(z)$为$\varphi(x)$和$\varphi(z)$的内积。</p></blockquote><p>核技巧的想法是，在学习与预测中只定义核函数$K(x,z)$，而不显式的定义映射函数$\varphi$。</p><h2 id="核技巧在支持向量机中的应用"><a href="#核技巧在支持向量机中的应用" class="headerlink" title="核技巧在支持向量机中的应用"></a>核技巧在支持向量机中的应用</h2><h3 id="对偶问题目标函数"><a href="#对偶问题目标函数" class="headerlink" title="对偶问题目标函数"></a>对偶问题目标函数</h3><p>$$W(\alpha)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i$$</p><h3 id="分类决策函数"><a href="#分类决策函数" class="headerlink" title="分类决策函数"></a>分类决策函数</h3><p>$$f(x) = sign(\sum_{i=1}^N\alpha_i^<em>y_i\varphi(x_i)\cdot\varphi(x)+b^</em>)=sign(\sum_{i=1}^N\alpha_i^<em>y_iK(x_i,x)+b^</em>)$$</p><p>这等价于经过映射函数$\varphi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积$x_i\cdot x_j$变换为特征空间中的内积$\varphi(x_i)\cdot \varphi(x_j)$，在新的特征空间里从训练样本中学习线性支持向量机。当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型。</p><p>也就是说，在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间中进行的，不需要显示的定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。</p><h2 id="核函数类型"><a href="#核函数类型" class="headerlink" title="核函数类型"></a>核函数类型</h2><p>先看一下正定核的一些定义，通常所说的核函数就是正定核函数。</p><blockquote><p>设K：$\chi \times  \chi \rightarrow R$是对称函数，则$K(x,z)$为正定核函数的充要条件是对任意$x_i\in \chi$，$i=1,2,…,m$，$K(x,z)$对应的$Gram$矩阵：</p><p>$$K=[K(x_i,x_j)]_{m\times m}$$</p><p>是半正定矩阵。</p></blockquote><p>这一定义在构造核函数时很有用，但对于一个具体函数$K(x,z)$来说，检验它是否为正定核函数并不容易，因为要求对任意有限输入集$\{x_1,x_2,…,x_m\}$验证$K$对应的$Gram$矩阵是否为半正定的。</p><h3 id="多项式核函数"><a href="#多项式核函数" class="headerlink" title="多项式核函数"></a>多项式核函数</h3><p>$$K(x,z)=(x\cdot z+1)^p$$</p><p>对应的支持向量机是一个$p$次多项式分类器，再此情形下，分类决策函数变为</p><p>$$f(x)=sign(\sum_{i=1}^Na_i^\<em> y_i(x_i\cdot x+1)^p+b^\</em> )$$</p><h3 id="高斯核函数"><a href="#高斯核函数" class="headerlink" title="高斯核函数"></a>高斯核函数</h3><p>$$K(x,z)=exp(-\frac{||x-z||^2}{2\sigma^2})$$</p><p>对应的支持向量机是高斯径向基函数(radial basis function)分类器，在此情形下，分类决策函数变为</p><p>$$f(x) = sign(\sum_{i=1}^Na_i^\<em> y_iexp(-\frac{||x-z||^2}{2\sigma^2})+b^\</em> )$$</p><h3 id="核函数选择"><a href="#核函数选择" class="headerlink" title="核函数选择"></a>核函数选择</h3><table><thead><tr><th style="text-align:center">Time of learning</th><th style="text-align:center">linear &lt; poly &lt; rbf</th></tr></thead><tbody><tr><td style="text-align:center">Ability of  fit any data</td><td style="text-align:center">linear &lt; poly &lt; rbf</td></tr><tr><td style="text-align:center">Risk of Overfitting</td><td style="text-align:center">linear &lt; poly &lt; rbf</td></tr><tr><td style="text-align:center">risk of underfitting</td><td style="text-align:center">rbf &lt; poly &lt; linear</td></tr><tr><td style="text-align:center">number of hyperparameters</td><td style="text-align:center">linear(0) &lt; rbf(2) &lt; poly(3)</td></tr><tr><td style="text-align:center">how “local” is particular kernel</td><td style="text-align:center">linear &lt; poly &lt; rbf</td></tr></tbody></table><p>民间口诀</p><p>初级：高维用线性，不行换特征；低维试线性，不行换高斯。</p><p>中级：线性试试看，不行换高斯，卡方有奇效，绝招MKL；</p><p>玩家：Kernel度量相似性，自己做啊自己做。</p><h2 id="模型理解"><a href="#模型理解" class="headerlink" title="模型理解"></a>模型理解</h2><p>利用核技巧，可以将线性分类的学习方法应用到非线性分类问题中去。将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-074028.jpg" alt="|center|"></p><h1 id="Soft-Binary-Classification"><a href="#Soft-Binary-Classification" class="headerlink" title="Soft Binary Classification"></a>Soft Binary Classification</h1><p>考虑Hinge损失函数形式的SVM目标函数</p><p>$$min_{b,w}\ \ C\cdot \sum_{i=1}^Nmax(0, 1-y_i(w\cdot x_i+b))+\frac{1}{2}||w||^2$$</p><p>把该目标函数与下面的形式进行对比</p><p>$$min\ \ \frac{1}{2}w^Tw+C\sum \hat{err}$$</p><p>所以，软间隔的SVM也可以写成带有L2正则化的优化目标函数。</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">minimize</th><th style="text-align:center">constraint</th></tr></thead><tbody><tr><td style="text-align:center">regularization by constraint</td><td style="text-align:center">$E_{in}$</td><td style="text-align:center">$w^Tw\leq C$</td></tr><tr><td style="text-align:center">Hard-margin SVM</td><td style="text-align:center">$w^Tw$</td><td style="text-align:center">$E_{in} = 0$</td></tr><tr><td style="text-align:center">L2 regularization</td><td style="text-align:center">$\frac{\lambda}{N}w^Tw+E_{in}$</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">Soft-margin SVM</td><td style="text-align:center">$\frac{1}{2}w^Tw+CN\hat{E_{in}}$</td></tr></tbody></table><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-74029.jpg" alt="|center|"></p><p>从上面的分析，我们可以看出来，如果我们解决了一个软间隔的SVM问题，就相当于解决了一个L2正则化的模型，反之也成立，那么我们是否可以结合两者进行处理呢？</p><p>结合SVM模型和逻辑回归模型，分如下两个步骤</p><ol><li>SVM flavor：求解SVM中的$w_{svm}$和$b_{svm}$；</li><li>LogReg flavor：调整超平面来满足极大似然估计。</li></ol><p>这样，得到的新的目标函数为</p><p>$$min_{A,B}\ \ \frac{1}{N}\sum_{i=1}^Nlog[1+exp(-y_n(A\cdot (w_{svm}^T\varphi(x_n)+b_{svm})+B))]$$</p><h1 id="Kernel-Logistic-Regression"><a href="#Kernel-Logistic-Regression" class="headerlink" title="Kernel Logistic Regression"></a>Kernel Logistic Regression</h1><blockquote><p>如果我们现在要解决的带有L2正则化的线性模型</p><p>$$min_w \ \ \frac{\lambda}{N}w^Tw+\frac{1}{N}\sum_{n=1}^Nerr(y_n, w^Tz_n)$$</p><p>那么最优$w$，会是输入的线性组合，即$w^*=\sum_{i=1}^N\beta_nz_n$</p></blockquote><p>如果是这样的话，那么我们可以将求解最佳$w$的问题，转化为求解最佳的$\beta$</p><p>$$min_{\beta}\ \frac{\lambda}{N}\sum_{i=1}^N\sum_{j=1}^N\beta_i\beta_jK(x_i,x_j)+\frac{1}{N}\sum_{i=1}^Nlog(1+exp(-y_n\sum_{j=1}^N\beta_jK(x_i,x_j)))$$</p><p>对于该模型可以有如下关于$\beta$的解释</p><ul><li>$\sum_{i=1}^N\beta_iK(x_i,x_j)$可以看做是$\beta$变量和转换过的数据做线性组合$(K(x_1,x_n),K(x_2,x_n),…,K(x_N,x_n))$</li><li>$\sum_{i=1}^N\sum_{j=1}^N\beta_i\beta_jK(x_i,x_j)$：关于$\beta$的特殊形式的正则化$\beta^TK\beta$</li><li>Kernel logistic regression 可以理解为关于$\beta$的线性模型，该模型用核函数做转换，并带有核函数的正则化。</li><li>和SVM不同的是，$\beta_i$通常都不是0</li></ul><h1 id="Support-Vector-Regression"><a href="#Support-Vector-Regression" class="headerlink" title="Support Vector Regression"></a>Support Vector Regression</h1><h2 id="Kernel-Ridge-Regression"><a href="#Kernel-Ridge-Regression" class="headerlink" title="Kernel Ridge Regression"></a>Kernel Ridge Regression</h2><blockquote><p>如果我们现在要解决的带有L2正则化的线性模型</p><p>$$min_w \ \ \frac{\lambda}{N}w^Tw+\frac{1}{N}\sum_{n=1}^Nerr(y_n, w^Tx_n)$$</p><p>那么最优$w$，会是输入的线性组合，即$w^*=\sum_{i=1}^N\beta_nx_n$</p></blockquote><p>现在，要解决的是回归的问题，我们令$err(y,w^Tx)=(y-w^Tx)^2$，带有正则化的回归模型被称为ridge regression，现在，我们看一下是否可以结合kernel函数和ridge regression？</p><p>将最佳解$w^*=\sum_{i=1}^N\beta_nx_n$带入到目标函数中</p><p>$$min_\beta\ \ \frac{\lambda}{N}\sum_{i=1}^N\sum_{j=1}^N\beta_i\beta_jK(x_n,x_m)+\frac{1}{N}\sum_{i=1}^N[y_i-\sum_{j=1}^N\beta_jK(x_i,x_j)]^2$$</p><p>我们可以使用梯度下降法求解上述表达式。求解结果为：$\beta=(\lambda I+K)^{-1}y$</p><p>所以，借助核函数，我们可以很轻易的解决非线性的回归问题。</p><p>比较线性回归和Kernel Ridge Regression</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-74026.jpg" alt="|center|"></p><h2 id="Tube-Regression"><a href="#Tube-Regression" class="headerlink" title="Tube Regression"></a>Tube Regression</h2><p>在之前的线性回归模型中，我们距离的计算公式为：$|s-y|$。我们现在借助SVM的想法，设置一个间隔，如果预测值里真实值的距离在该间隔内，我们就不要计算该损失。</p><p>$$err(y,s)=max(0,|s-y|-\varepsilon )$$</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-074030.jpg" alt="|center|"></p><p>通过这样的定义，我们希望借助SVM的求解方式，来得到稀疏的$\beta$。</p><h3 id="Tube-versus-Squared-Regression"><a href="#Tube-versus-Squared-Regression" class="headerlink" title="Tube versus Squared Regression"></a>Tube versus Squared Regression</h3><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-74031.jpg" alt="|center|"></p><p>模仿SVM的优化问题，我们将上述写成</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-074031.jpg" alt="|center|"></p><h1 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h1><h2 id="sklearn-svm"><a href="#sklearn-svm" class="headerlink" title="sklearn.svm"></a>sklearn.svm</h2><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><ul><li>linear: $&lt;x,x’&gt;$</li><li>polynomial:$(\gamma &lt;x,x’&gt;+r)^d$</li><li>rbf: $exp(-\gamma||x-x’||^2)$</li><li>sigmoid: $tanh(\gamma&lt;x,x’&gt;+r)$</li></ul><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><table><thead><tr><th></th><th style="text-align:center">SVC</th><th style="text-align:center">NuSVC</th><th style="text-align:center">LinearSVC</th></tr></thead><tbody><tr><td>Multi-class</td><td style="text-align:center">one-vs-one</td><td style="text-align:center">one-vs-one</td><td style="text-align:center">one-vs-rest</td></tr><tr><td>kernel</td><td style="text-align:center">rbf</td><td style="text-align:center">rbf</td><td style="text-align:center">linear</td></tr></tbody></table><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” <em>清华大学出版社, 北京</em> (2012).</p><p>[2]. <a href="https://stackoverflow.com/questions/33778297/support-vector-machine-kernel-types" target="_blank" rel="noopener">Support Vector Machine kernel types</a></p><p>[3]. <a href="https://www.zhihu.com/question/21883548" target="_blank" rel="noopener">SVM的核函数如何选取？</a></p><p>[4]. <a href="https://www.youtube.com/watch?v=oOi7kqUTqxw&amp;index=10&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2" target="_blank" rel="noopener">林轩田 机器学习基石</a></p><p>[5]. <a href="http://blog.pluskid.org/?page_id=683" target="_blank" rel="noopener">漫谈支持向量机系列</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090106.jpg&quot; alt=&quot;|center|&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;非线性支持向量机与核函数&quot;&gt;&lt;a href=&quot;#非线性支持向量机与核函数&quot; class
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机 1-2：线性支持向量机</title>
    <link href="http://conghuai.me/2018/09/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%201-2%EF%BC%9A%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://conghuai.me/2018/09/25/支持向量机 1-2：线性支持向量机/</id>
    <published>2018-09-25T03:30:12.000Z</published>
    <updated>2018-09-25T03:50:11.655Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性支持向量机与软间隔最大化"><a href="#线性支持向量机与软间隔最大化" class="headerlink" title="线性支持向量机与软间隔最大化"></a>线性支持向量机与软间隔最大化</h1><h2 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h2><p>线性可分问题的支持向量机学习方法，对线性不可分训练数据是不适用的，因为这时上述方法中的不等式约束并不能都成立。怎么才能将它扩展到线性不可分问题呢？这就需要修改硬间隔最大化，使其成为软间隔最大化。</p><p>线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件。为了解决这个问题，可以对每个样本点$(x_i,y_i)$引进一个松弛变量$\xi_i \geq 0$，使得函数间隔加上松弛变量大于等于1，这样约束条件为</p><p>$$y_i(w\cdot x_i+b)+\xi_i\geq1 \rightarrow y_i(w\cdot x_i+b)\geq 1-\xi_i$$</p><p>同时，对每个松弛变量$\xi_i$，支付一个代价$\xi_i$，目标函数变成</p><p>$$\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i$$</p><p>这里，$C&gt;0$称为惩罚参数，一般由应用问题决定，$C$值大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。最优化目标函数包含两层含义：（1）使$\frac{1}{2}||w||^2$尽量小即间隔尽量大（2）同时使误分类点的个数尽量小，C是调和二者的系数。</p><p>线性不可分的线性支持向量机的学习问题如下</p><p>$$min_{w,b,\xi}\ \ \frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i$$</p><p>$$s.t.\ \ \ y_i(w\cdot x_i+b)\geq1-\xi_i,\ i=1,2,…,N$$</p><p>$$s.t.\ \ \xi_i\geq0,\ i=1,2,…,N$$</p><h2 id="凸二次规划求解"><a href="#凸二次规划求解" class="headerlink" title="凸二次规划求解"></a>凸二次规划求解</h2><p>上述优化问题可以用凸二次规划方法来求解，可以证明$w​$的解释唯一的，但$b​$的解不唯一，$b​$的解存在于一个区间。</p><h2 id="学习的对偶算法"><a href="#学习的对偶算法" class="headerlink" title="学习的对偶算法"></a>学习的对偶算法</h2><p>原始最优化问题的拉格朗日函数是</p><p>$$L(w,b,\xi,\alpha,\beta)=\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum_{i=1}^N\beta_i\xi_i$$</p><p>其中，$\alpha_i\geq0$，$\beta_i\geq0$</p><p>对偶问题是拉格朗日函数 的极大极小问题，即</p><p>$$max_{\alpha,\beta} min_{w,b,\xi}L(w,b,\xi,\alpha,\beta)$$</p><h3 id="求解对偶问题"><a href="#求解对偶问题" class="headerlink" title="求解对偶问题"></a>求解对偶问题</h3><h4 id="1-求-min-w-b-xi-L-w-b-xi-alpha-mu"><a href="#1-求-min-w-b-xi-L-w-b-xi-alpha-mu" class="headerlink" title="(1) 求$min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$"></a>(1) 求$min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$</h4><p>对$w,b,\xi$求偏导，令其为0</p><p>$$\triangledown_wL(w,b,\xi,\alpha,\beta)=w-\sum_{i=1}^N\alpha_iy_ix_i=0$$</p><p>$$\triangledown_bL(w,b,\xi,\alpha,\beta)=-\sum_{i=1}^N\alpha_iy_i=0$$</p><p>$$\triangledown_{\xi_i} L(w,b,\xi,\alpha,\beta)=C-\alpha_i-\beta_i=0$$</p><p>得</p><p>$$w = \sum_{i=1}^N\alpha_iy_ix_i$$</p><p>$$\sum_{i=1}^N\alpha_iy_i=0$$</p><p>$$C-\alpha_i-\beta_i=0$$</p><p>我们通过代数替换，将$\beta_i$换成$C-a_i$，并规定</p><p>$$C-a_i\geq 0 \rightarrow \alpha_i \leq C$$</p><p>将$\beta_i=C-\alpha_i$，$w = \sum_{i=1}^N\alpha_iy_ix_i$带入L中，得</p><p>$$\begin{aligned} L(w,b,\xi,\alpha,\beta) &amp;= \frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum_{i=1}^N(C-\alpha_i)\xi_i \\  &amp;= \frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\sum_{i=1}^N\alpha_i-\sum_{i=1}^N\alpha_i\xi_i-C\sum_{i=1}^N\xi_i+\sum_{i=1}^N\alpha_i\xi_i \\  &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) -\sum_{i=1}^N\alpha_i  \\ \end{aligned}$$</p><h4 id="2-求-min-w-b-xi-L-w-b-xi-alpha-mu-对-alpha-的极大"><a href="#2-求-min-w-b-xi-L-w-b-xi-alpha-mu-对-alpha-的极大" class="headerlink" title="(2) 求$min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$对$\alpha$的极大"></a>(2) 求$min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$对$\alpha$的极大</h4><p>$$max_\alpha\ -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i$$</p><p>$$s.t.\ \sum_{i=1}^N\alpha_iy_i=0$$</p><p>$$0\leq \alpha_i\leq C$$</p><p>对偶问题取得最优解需要满足如下的KKT条件</p><blockquote><p>KKT 条件</p><p>$$w^\ast -\sum_{i=1}^N\alpha_i^\ast y_ix_i=0$$</p><p>$$-\sum_{i=1}^N\alpha_i^\ast y_i=0$$</p><p>$$C-\alpha^\ast -\beta^\ast =0$$</p><p>$$\alpha_i^\ast (y_i(w^\ast \cdot x_i+b^\ast )-1+\xi^\ast _i)=0$$</p><p>$$\beta_i^\ast \xi_i^\ast =0$$</p><p>$$y_i(w^\ast \cdot x_i+b^\ast )-1+\xi_i^\ast \geq0$$</p><p>$$\xi_i^\ast \geq0$$</p><p>$$\alpha_i^\ast \geq0$$</p><p>$$\beta_i^\ast \geq0$$</p></blockquote><h3 id="模型理解"><a href="#模型理解" class="headerlink" title="模型理解"></a>模型理解</h3><p>设$\alpha^\ast =(\alpha_1^\ast ,\alpha_2^\ast ,…,\alpha_N^\ast )^T$是对偶问题的一个解，若存在$\alpha^\ast $的一个分量$\alpha_j^\ast $，$0&lt;\alpha_j^\ast &lt;C$，则原始问题的解$w^\ast ,b^\ast $可按下式求得</p><p>$$w^\ast =\sum_{i=1}^N\alpha_i^\ast y_ix_i$$</p><p>$$b^\ast =y_j-\sum_{i=1}^Ny_i\alpha_i^\ast (x_i\cdot x_j)$$</p><p>在线性不可分的情况下，将对偶问题的解$\alpha^\ast =(\alpha_1^\ast ,\alpha_2^\ast ,…,\alpha_N^\ast )^T$中对应于$\alpha_i^\ast &gt;0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量（软间隔的支持向量），这时的支持向量要比线性可分时的情况复杂一些</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-20-031530.jpg" alt=""></p><p>软间隔的支持向量$x_i$或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。</p><p>考虑两个KKT条件</p><blockquote><p>$$\alpha_i^\ast (y_i(w^\ast \cdot x_i+b^\ast )-1+\xi^\ast _i)=0$$</p><p>$$\beta_i^\ast \xi_i^\ast =0 \rightarrow (C-\alpha_i^\ast )\xi_i^\ast =0$$</p></blockquote><ul><li>若$\alpha_i = 0$，由第二个条件得，$\xi_i=0$，说明这些点没有违反边界，一般在边界之外；</li><li>若$0&lt;\alpha_i&lt; C$，由第二个条件得，$\xi_i=0$，由第一个条件得，$y_i(w^\ast \cdot x_i+b^\ast )-1+\xi^\ast _i=0$，综上，得$y_i(w^\ast \cdot x_i+b^\ast )-1=0$，则这些点在边界上。</li><li>若$\alpha_i = C$，样本点违反边界值，违反距离为$\xi_i=1-y_n(w^Tx_i+b)$<ul><li>$0&lt;\xi_i&lt;1$，则分类正确，$x_i$在间隔边界与分离超平面之间；</li><li>$\xi_i=1$，则$x_i$在分离超平面上；</li><li>$\xi_i&gt;1$，则$x_i$位于分离超平面误分一面；</li></ul></li></ul><h3 id="合页损失函数"><a href="#合页损失函数" class="headerlink" title="合页损失函数"></a>合页损失函数</h3><p>线性支持向量机还有另一种解释，就是最小化以下目标函数</p><p>$$min_{b,w}\ \ \sum_{i=1}^N[1-y_i(w\cdot x_i+b)]_++\lambda||w||^2$$</p><blockquote><p>直观理解</p><p>对于目标函数</p><p>$$\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i$$</p><p>我们考虑$\xi_i$</p><p>$$\xi_n=\begin{cases} 1-y_n(w^Tz_n+b), &amp; \text{$(x_n,y_n)$违反了边界} \\ 0, &amp; \text{$(x_n,y_n)$没有违反边界}\end{cases}$$</p><p>所以，我们可以把$\xi_n$改写如下</p><p>$$\begin{aligned} \xi_n &amp;= max(1-y_n(w^Tz_n+b),0) \\  &amp;= [1-y_i(w\cdot x_i+b)]_+  \end{aligned}$$</p></blockquote><p>严格证明</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-20-031533.jpg" alt="|center|"></p><p>所以，线性支持向量机学习等价于最小化二阶范数正则化的合页函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性支持向量机与软间隔最大化&quot;&gt;&lt;a href=&quot;#线性支持向量机与软间隔最大化&quot; class=&quot;headerlink&quot; title=&quot;线性支持向量机与软间隔最大化&quot;&gt;&lt;/a&gt;线性支持向量机与软间隔最大化&lt;/h1&gt;&lt;h2 id=&quot;线性支持向量机&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机 1-1：线性可分支持向量机</title>
    <link href="http://conghuai.me/2018/09/25/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%201-1%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://conghuai.me/2018/09/25/支持向量机 1-1：线性可分支持向量机/</id>
    <published>2018-09-25T03:20:12.000Z</published>
    <updated>2018-09-25T03:25:45.994Z</updated>
    
    <content type="html"><![CDATA[<p>支持向量机（support vector machines,SVM）是一种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器，支持向量机的学习策略就是间隔最大化，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法是求解凸二次规划的最小化算法。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>感知机模型告诉我们一个线性分类器就是要在$n$维的数据空间中找到一个超平面</p><p>$$w^Tx+b = 0$$</p><p>通过这个超平面可以把两类数据分隔开，比如，在超平面一边的数据点所对应的$y$全是-1；而另一边全是1。具体来说，我们令$f(x) = w^Tx+b$，显然，如果$f(x) = 0$，那么$x$是位于超平面上的点。我们不妨要求对于所有满足$f(x)&lt;0$的点，其对应的$y$等于-1，而$f(x)&gt;0$则对应$y=1$的样本点。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090113.jpg" alt="|center|"></p><p>从几何直观上来说，由于超平面是用于分隔两类数据的，越接近超平面的点越难分隔，因为如果超平面稍微转动一下，他们就有可能跑到另一边去。反之，如果是距离超平面很远的点，则很容分辨出其类别。所以，我们希望找到一个超平面，使得样本点到超平面的距离都尽量远，那么我们应该如何定义这个距离？</p><h2 id="函数距离"><a href="#函数距离" class="headerlink" title="函数距离"></a>函数距离</h2><p>一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度，在超平面$w\cdot x+b=0$确定的情况下，$|w\cdot x+b|$能够相对地表示点$x$距离超平面的远近。而$w\cdot x + b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以，可用量$y(w\cdot x+b)$来表示分类的正确性及确信度，这就是函数间隔的概念。</p><p>对于给定的训练数据集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_t,y_t)$的函数间隔为</p><p>$$\hat{\gamma}_i=y_i(w\cdot x_i+b) $$</p><p>定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，即</p><p>$$\hat{\gamma}=min_{i=1,..,N} \hat{\gamma}_i$$</p><p>函数间隔可以表示分类预测的正确性及确信度，但是选择分离超平面时，只有函数间隔不够，因为只要成比例地改变$w$和$b$，例如将它们改为$2w$和$2b$，超平面并没有改变，但函数间隔却成为原来的2倍。如果我们对函数间隔加某些约束，如规范化，$||w||=1$，使得间隔是确定的，这时函数间隔就成为几何间隔。</p><h2 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h2><p>对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为</p><p>$$\gamma_i = y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})$$</p><p>定义超平面$(w,b)$关于训练数据集$T$的几何间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即</p><p>$$\gamma=min_{i=1,…,N}\ \gamma_i$$</p><p>我们也可以通过几何关系，求解出集合间隔</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090102.jpg" alt="|center|"></p><h2 id="两种间隔的关系"><a href="#两种间隔的关系" class="headerlink" title="两种间隔的关系"></a>两种间隔的关系</h2><p>从函数间隔和几何间隔的定义可知，函数间隔和几何间隔有下面的关系：</p><p>$$\gamma_i = \frac{\hat{\gamma_i}}{||w||}$$</p><p>$$\gamma = \frac{\hat{\gamma}}{||w||}$$</p><p>如果$||w||=1$，那么函数间隔和几何间隔相等，如果超平面参数$w$和$b$成比例地改变，函数间隔也按此比例改变，而几何间隔不变。</p><h1 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h1><p>支持向量机学习的基本想法是求解能够正确划分训练数据集并且集合间隔最大的分离超平面。对线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机），但是几何间隔最大的分离超平面是唯一的。这里的间隔最大化又称为硬间隔最大化。间隔最大化的直观解释是：对训练数据集找到集合间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。</p><h2 id="最大间隔分离超平面"><a href="#最大间隔分离超平面" class="headerlink" title="最大间隔分离超平面"></a>最大间隔分离超平面</h2><p>$$max_{w,b}\ \ \ \frac{\hat{\gamma}}{||w||}$$</p><p>$$s.t.\ y_i(w\cdot x_i+b)\geq \hat{\gamma},i=1,2,…,N$$</p><p>函数间隔$\hat{\gamma}$的取值并不影响最优化问题的解。事实上，假设将$w$和$b$按比例改变为$\lambda w$和$\lambda b$，这时函数间隔成为$\lambda \hat{\gamma}$。函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，它产生一个等价的最优化问题。这样，就可以取$\hat{\gamma}=1$，带入上式</p><p>$$max_{w,b}\ \ \ \frac{1}{||w||}$$</p><p>$$s.t.\ y_i(w\cdot x_i+b)\geq 1,i=1,2,…,N$$</p><p>注意到最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$是等价的，于是就得到了下面的线性可分支持向量学习的最优化问题</p><p>$$min_{w,b}\ \ \ \frac{1}{2}||w||^2$$</p><p>$$s.t.\ y_i(w\cdot x_i+b) - 1\geq 0,i=1,2,…,N$$</p><h2 id="凸二次规划算法"><a href="#凸二次规划算法" class="headerlink" title="凸二次规划算法"></a>凸二次规划算法</h2><p>当凸优化问题的目标函数是二次函数且约束函数$g_i(w)$是仿射函数时，凸最优化问题就成为凸二次规划问题，所以上述最优化问题是凸二次规划问题，已经有现成的解法可以求解这类问题，我们按照模板把参数设置进去即可。</p><blockquote><p>Quadratic Programming</p><p>optimal $u \leftarrow QP(Q,p,A,c)$，其中：Q是二次项的系数，p是一次项的系数</p><p>​    $min_u$         $\ \ \frac{1}{2}u^TQu+p^Tu$</p><p>subject to     $\ \ a_m^Tu \geq c_m$, for m = 1,2,…,M</p></blockquote><p>上面就是凸二次规划问题求解的参数模板，对比线性可分支持向量机最优化问题</p><blockquote><p>optimal (b, w) = ?</p><p>​      $min_{b, w}$   $\frac{1}{2}w^Tw$</p><p>subject to   $y_n(w^Tx_n+b)\geq1$, for n = 1, 2, …, N</p></blockquote><p>参数设置如下</p><p>object function :       $u = \begin{bmatrix} b \\ w\end{bmatrix}$; $Q = \begin{bmatrix}0 &amp;0_d^T \\ 0_d &amp; I_d\end{bmatrix}$;$p=0_{d+1}$</p><p>constrains :             $a_n^T=y_n\begin{bmatrix} 1 &amp; x_n^T\end{bmatrix}; c_n=1;M=N$</p><h3 id="模型理解"><a href="#模型理解" class="headerlink" title="模型理解"></a>模型理解</h3><p>如果求出了约束最优化问题的解$w^<em>$，$b^</em>$，那么就可以得到最大间隔分离超平面$w^<em>\cdot x+b^</em> = 0$及分类决策函数$f(x)=sign(w^<em>\cdot x+b^</em>)$。</p><p>在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector），支持向量是使约束条件等号成立的点，即</p><p>$$y_i(w\cdot x_i+b)-1=0$$</p><p>对$y_i=+1$的正例点，支持向量在超平面$H_1$上</p><p>$$H_1:w\cdot x+b = 1$$</p><p>对$y_i = -1$的负例点，支持向量在超平面$H_2$上</p><p>$$H_2:w\cdot x+b = -1$$</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090110.jpg" alt="|center|"></p><p>注意到$H_1$和$H_2$平行，并且没有实例点落在它们中间。在$H_1$与$H_2$之间形成了一条长带，分离超平面与它们平行且位于它们中央。长带的宽度，即$H_1$与$H_2$之间的距离称为间隔。间隔依赖于分离超平面的法向量$w$，等于$\frac{2}{||w||}$，$H_1$和$H_2$称为间隔边界。</p><p>在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用，如果移动支持向量将改变所求的解；但是如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。支持向量的个数一般很少，所有支持向量机由很少的“重要的”训练样本确定。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090112.jpg" alt="|center|"></p><h2 id="学习的对偶算法"><a href="#学习的对偶算法" class="headerlink" title="学习的对偶算法"></a>学习的对偶算法</h2><p>为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。这样做的有点有：一、是对偶问题往往更容易求解；二、自然引入核函数，进而推广到非线性分类的问题。</p><h3 id="定义拉格朗日函数"><a href="#定义拉格朗日函数" class="headerlink" title="定义拉格朗日函数"></a>定义拉格朗日函数</h3><p>$$\begin{aligned} L(w,b,\alpha) &amp;= \frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1) \\  &amp;= \frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\sum_{i=1}^N\alpha_i \end{aligned}$$</p><p>其中，$\alpha=(\alpha_1,\alpha_2,…,\alpha_N)^T$为拉格朗日乘子向量。</p><p>这样，我们得到了拉格朗日形式</p><p>$$min_{w,b}max_\alpha\ L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\sum_{i=1}^N\alpha_i, \ \ \ \alpha_i\geq0,i=1,2,…,N $$</p><blockquote><ol><li>原始问题</li></ol><p>说明原始最优化问题和拉格朗日极小极大问题是等价：</p><p>考虑这个形式：$L(w,b,\alpha) = \frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1)$</p><p>其中，$y_i(w\cdot x_i+b)-1\geq 0$，则使得$max_\alpha\ L(w,b,\alpha)$最大的$\alpha$，必须是所有分量都为0（因为$\alpha_i\geq0$）。这样，拉格朗日形式就变为$min_{w,b}\ L(w,b,\alpha)=\frac{1}{2}||w||^2$。可以发现，我们定义的拉格朗日形式和原始问题表达式是一致的，我们把这两者表达方式都叫原始问题。</p><p>更形式化的证明</p><p>假设$f(x)$,$c_i(x)$,$h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题</p><p>$$min_{x\in R^n}\ f(x)$$</p><p>$$s.t.\ \ c_i(x)\leq0,\ i=1,2,…,k$$</p><p>$$h_j(x)=0,\ j=1,2,…,l$$</p><p>称此约束最优化问题为原始最优化问题或原始问题。</p><p>首先，引进广义拉格朗日函数</p><p>$$L(x,\alpha,\beta)=f(x) + \sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)$$</p><p>这里，$x=(x^{(1)},x^{(2)},…,x^{(n)})^T\in R$，$\alpha_i，\beta_j$是拉格朗日乘子，$\alpha_i\geq0$，考虑$x$的函数：</p><p>$$\theta_p(x)=max_{\alpha,\beta;\alpha_i\geq0}\ L(x,\alpha,\beta)$$</p><p>这里，下标P表示原始问题。</p><p>假设对于某个$x$，如果$x$违反原始问题的约束条件，即存在某个$i$使得$c_i(w)&gt;0$或则存在某个$j$使得$h_j(w)\neq0$，那么就有</p><p>$$\theta_P(x)= max_{\alpha,\beta;\alpha_i\geq0}[f(x) + \sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)]=+\infty $$</p><p>相反的，如果$x$满足约束条件，则$\theta_P(x)=f(x)$。因此</p><p>$$\begin{split} \theta_P(x)=\begin{cases} f(x), &amp; \text{x满足原始问题约束} \\ +\infty, &amp; \text{otherwise}\end{cases}\end{split}$$</p><p>所以如果考虑极小化问题</p><p>$$min_x\ \theta_P(x) = min_x max_{\alpha,\beta;\alpha_i\geq0}\ L(x, \alpha, \beta)$$</p><p>它是与原始最优化问题等价的，即他们有相同的解，这样一来，就把原始问题最优化问题表示为广义拉格朗日函数的极小极大问题。为了方便，定义原始问题的最优值</p><p>$$p^*=min_x\theta_P(x)$$</p><p>称为原始问题的值。</p></blockquote><p>通过以上分析，你可能会觉得，既然广义拉格朗日函数的极小极大问题和原始问题最优化问题本质上是一致的，那为什么要转化为拉格朗日表达式呢？原因在于，我们往往能够通过求解广义拉格朗日函数的极小极大问题的对偶问题来帮助我们求解原始问题。</p><h3 id="原始问题的对偶问题"><a href="#原始问题的对偶问题" class="headerlink" title="原始问题的对偶问题"></a>原始问题的对偶问题</h3><blockquote><ol><li>对偶问题</li></ol><p>定义</p><p>$$\theta_D(\alpha,\beta)=min_xL(x,\alpha,\beta)$$</p><p>再考虑极大化$\theta_D(\alpha,\beta)$，即</p><p>$$max_{\alpha,\beta;\alpha_i\geq0}\ \theta_D(\alpha,\beta)=max_{\alpha,\beta;\alpha_i\geq0}min_x\ L(x,\alpha,\beta)$$</p><p>问题$max_{\alpha,\beta;\alpha_i\geq0}min_x\ L(x,\alpha,\beta)$称为广义拉格朗日函数的极大极小问题。</p><p>可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：</p><p>$$max_{\alpha,\beta}\theta_D(\alpha,\beta)=max_{\alpha,\beta}min_x\ L(x, \alpha,\beta)$$</p><p>称为原始问题的对偶问题，定义对偶问题的最优值</p><p>$$d^*=max_{\alpha,\beta}\theta_D(\alpha,\beta)$$</p><p>称为对偶问题的值。</p></blockquote><p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：</p><p>$$max_\alpha min_{w,b}L(w,b,\alpha)$$</p><p>所以，为了得到对偶问题的解，需要先求$L(w,b,\alpha)$对$w,b$的极小，再求对$\alpha$的极大。</p><h4 id="1-求-min-w-b-L-w-b-alpha"><a href="#1-求-min-w-b-L-w-b-alpha" class="headerlink" title="(1) 求$min_{w,b}\ L(w, b, \alpha)$"></a>(1) 求$min_{w,b}\ L(w, b, \alpha)$</h4><p>$$\triangledown_wL(w,b,\alpha)=w-\sum_{i=1}^N\alpha_iy_ix_i=0\rightarrow w=\sum_{i=1}^N\alpha_iy_ix_i$$</p><p>$$\triangledown_bL(w,b,\alpha)=\sum_{i=1}^N\alpha_iy_i=0 \rightarrow\sum_{i=1}^N\alpha_iy_i=0$$</p><p>将上述结果带入到拉格朗日函数中，得</p><p>$$\begin{aligned} min_{w,b}L(w,b,\alpha) &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_iy_i((\sum_{j=1}^N\alpha_jy_jx_j)\cdot x_i+b)+\sum_{i=1}^N\alpha_i \\  &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-b\cdot \sum_{i=1}^N\alpha_iy_i + \sum_{i=1}^N\alpha_i\\  &amp;= -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i   \end{aligned}$$</p><h4 id="2-求-min-w-b-L-w-b-alpha-对-alpha-的极大"><a href="#2-求-min-w-b-L-w-b-alpha-对-alpha-的极大" class="headerlink" title="(2) 求$min_{w,b}L(w,b,\alpha)$对$\alpha$的极大"></a>(2) 求$min_{w,b}L(w,b,\alpha)$对$\alpha$的极大</h4><blockquote><ol><li>原始问题和对偶问题的关系</li></ol><p>C.1 若原始问题和对偶问题都有最优值，则</p><p>$$d^<em>（对偶问题） = max_{\alpha,\beta;\alpha_i\geq0}\ min_xL(x,\alpha,\beta)\leq min_x max_{\alpha,\beta;\alpha_i\geq0}L(x,\alpha,\beta)=p^</em>（原始问题）$$</p><p><strong>证明</strong></p><p>$$\theta_D(\alpha,\beta)=min_x\ L(x,\alpha,\beta)\leq L(x,\alpha,\beta)\leq max_{\alpha,\beta}\theta_D(\alpha,\beta)= \theta_P$$</p><p>即</p><p>$$\theta_D(\alpha,\beta)\leq \theta_P(x)$$</p><p>由于原始问题和对偶问题均有最优值，所以</p><p>$$max_{\alpha,\beta;\alpha_i\geq0}\ \theta_D(\alpha,\beta)\leq min_x\ \theta_p(x)$$</p><p>即</p><p>$$d^<em> = max_{\alpha,\beta;\alpha_i\geq0}min_x\ L(x,\alpha,\beta)\leq min_x max_{\alpha,\beta}\theta_D(\alpha,\beta)= p^</em>$$</p><p>在某些条件下，原始问题和对偶问题的最优值相等，$d^<em>=p^</em>$，这时可以用解对偶问题替代原始问题。</p><p>定理1 </p><p>假设函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是放射函数；并且假设不等式约束$c_i(x)$是严格可行的，即存在$x$，对所有$i$有$c_i(x)&lt;0$，则存在$x^<em>,\alpha^</em>,\beta^<em>$，使$x^</em>$是原始问题的解，$\alpha^<em>，\beta^</em>$是对偶问题的解，并且</p><p>$$p^<em>=d^</em>= L(x^<em>,\alpha^</em>,\beta^*)$$</p><p>定理2 KKT条件</p><p>$x^<em>$和$\alpha^</em>，\beta^<em>$分别是原始问题和对偶问题的解的充分必要条件是$x^</em>$，$\alpha^<em>$，$\beta^</em>$满足KKT条件</p><ol><li><p>primal feasible: $y_n(w^Tx_n+b)\geq1$</p></li><li><p>dual feasible: $\alpha_n\geq0$</p></li><li><p>Dual-inner optimal:  $\sum y_n\alpha_n=0$，$w=\sum \alpha_ny_nx_n$</p></li><li><p>Primal-inner optimal（在最优的情况下，所有的拉格朗日项将消失）:</p><p>$$\alpha_n(1-y_n(w^Tx_n+b)) = 0$$</p></li></ol></blockquote><p>由上面定理可知，我们可以通过求对偶问题来求解原始问题</p><p>$$max_{\alpha}\ -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i  $$</p><p>$$s.t.\ \sum_{i=1}^N\alpha_iy_i=0,\alpha_i\geq0,i=1,2,…,N$$</p><p>等价于</p><p>$$min_{\alpha}\ \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i  $$</p><p>$$s.t.\ \sum_{i=1}^N\alpha_iy_i=0,\alpha_i\geq0,i=1,2,…,N$$</p><p>求解上面式子，最后可得</p><p>$$w^<em> = \sum_{i=1}^N\alpha_i^</em>y_ix_i$$</p><p>$$b^<em>=y_j-\sum_{i=1}^N\alpha^</em>y_i(x_i\cdot x_j)$$</p><h3 id="模型理解-1"><a href="#模型理解-1" class="headerlink" title="模型理解"></a>模型理解</h3><p>有上面的推导，分离超平面可以写成</p><p>$$\sum_{i=1}^N\alpha_i^<em>y_i(x\cdot x_i)+b^</em>=0$$</p><p>分类决策函数可以写成</p><p>$$f(x) = sign(\sum_{i=1}^N\alpha_i^<em>y_i(x\cdot x_i)+b^</em>)$$</p><p>考虑原始最优化问题中，由KKT互补条件可知：</p><p>$$\alpha_i^<em> (y_i(w^</em> \cdot x_i+b^* )-1)=0，i=1,2,…,N$$</p><p>对应于$\alpha_i^*&gt;0$的实例有$x_i$,有</p><p>$$y_i(w^<em>\cdot x_i+b^</em>)-1=0$$</p><p>即$x_i$一定在间隔边界上，所以我们在用模型对未知样本进行分类的时候，只需要将样本与支持向量做运算即可。</p><h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090109.jpg" alt="|center|"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090107.jpg" alt="|center|"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;支持向量机（support vector machines,SVM）是一种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器，支持向量机的学习策略就是间隔最大化，这使它成为实质
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归 1-4：逻辑回归与多分类</title>
    <link href="http://conghuai.me/2018/09/24/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%201-4%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E5%A4%9A%E5%88%86%E7%B1%BB/"/>
    <id>http://conghuai.me/2018/09/24/逻辑回归 1-4：逻辑回归与多分类/</id>
    <published>2018-09-24T15:39:12.000Z</published>
    <updated>2018-09-24T15:20:46.444Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-逻辑回归与多分类"><a href="#1-逻辑回归与多分类" class="headerlink" title="1 逻辑回归与多分类"></a>1 逻辑回归与多分类</h1><p>我们已经知道，普通的logistic回归只能针对二分类(Binary Classification)问题，要想实现多个类别的分类，我们必须要改进logistic回归，让其适应多分类问题。<br>关于这种改进，有两种方式可以做到。</p><ol><li><p>第一种方式是直接根据每个类别，都建立一个二分类器，带有这个类别的样本标记为1，带有其他类别的样本标记为0。假如我们有<strong>k</strong>个类别，最后我们就得到了<strong>k</strong>个针对不同标记的普通的logistic二分类器。（本质上就是ovr的做法）</p></li><li><p>第二种方式是修改logistic回归的损失函数，让其适应多分类问题。这个损失函数不再笼统地只考虑二分类非1就0的损失，而是具体考虑每个样本标记的损失。这种方法叫做<strong>softmax</strong>回归，即logistic回归的多分类版本。</p></li></ol><h1 id="2-ovr"><a href="#2-ovr" class="headerlink" title="2 ovr"></a>2 ovr</h1><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-08-05-071841.png" alt="|center|500x300"></p><p>对于二分类问题，我们只需要一个分类器即可，但是对于多分类问题，我们需要多个分类器$h_1, h_2, …, h_k$，其中$h_c$表示一个二分类模型，其判断样本$x$属于第$c$类的概率值。<br>对于$h_c$的训练，我们挑选出带有标记为$c$的样本标记为1，将剩下的不带标记$c$的样本标记为0。针对每个分类器，都按上述步骤构造训练集进行训练。<br>针对每一个测试样本，我们需要找到这k个分类函数输出值最大的那一个，即为测试样本的标记</p><p>$$argmax_c h_c(x) \text{  $c=1,2,…,k$}$$</p><h1 id="3-softmax"><a href="#3-softmax" class="headerlink" title="3 softmax"></a>3 softmax</h1><p>该模型将逻辑回归推广到分类问题，其中类标签y可以采用两个以上的可能值。这对于诸如MNIST数字分类之类的问题将是有用的，其中目标是区分10个不同的数字。Softmax回归是一种监督学习算法，但我们稍后会将其与我们的深度学习/无监督特征学习方法结合使用。<br>在softmax回归设置中，我们对多类分类感兴趣（而不是仅对二元分类），所以<code>y</code>可以取<code>k</code>个不同的取值。因此，在我们的训练集$\{(x^{(1)}, y^{(1)}), …,(x^{(m)}, y^{(m)})\}$，其中$y^{(i)}\in \{1,2,…,k\}$。<br>给定测试输入x，我们希望我们的模型估计每个类别的概率。因此，我们的模型将输出k维向量（其元素总和为1），给出我们的k个类别的估计概率。具体地说，我们的假设$h_\theta(x)$采用以下形式：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-08-05-072523.jpg" alt="|center|"></p><p>其中，$\theta_1, \theta_2,…,\theta_k \in R^{n+1}$是模型的参数，而$\frac{1}{\sum_{j=1}^ke^{\theta_j^Tx^{(i)}}}$是归一化项。<br>为方便起见，我们还会向量法来表示模型的所有参数。当你实现softmax回归时，将θ表示为通过堆叠$\theta_1, \theta_2,…,\theta_k$成行获得的k-by（n + 1）矩阵通常很方便，这样</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-08-05-072827.jpg" alt="|center|"></p><h2 id="3-1-损失函数"><a href="#3-1-损失函数" class="headerlink" title="3.1 损失函数"></a>3.1 损失函数</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-08-05-072908.jpg" alt="|center|"></p><p>求导后，可得</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-08-05-073000.jpg" alt="|center|"></p><p>更新参数</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-08-05-073043.jpg" alt="|center|"></p><h1 id="4-ovr-vs-softmax"><a href="#4-ovr-vs-softmax" class="headerlink" title="4 ovr vs. softmax"></a>4 ovr vs. softmax</h1><p>假设您正在处理音乐分类应用程序，并且您正在尝试识别k种类型的音乐。您应该使用softmax分类器，还是应该使用逻辑回归构建k个单独的二元分类器？这取决于这四个类是否<strong>相互排斥</strong>。例如，如果您的四个类是经典，乡村，摇滚和爵士乐，那么假设您的每个训练样例都标有这四个类别标签中的一个，那么您应该构建一个k = 4的softmax分类器。（如果有’还有一些不属于上述四个类的例子，那么你可以在softmax回归中设置k = 5，并且还有第五个，“以上都不是”类。）但是，如果你的类别是has_vocals，舞蹈，配乐，流行音乐，那么这些课程<strong>并不相互排斥</strong>;例如，可以有一段来自音轨的流行音乐，另外还有人声。在这种情况下，构建4个二元逻辑回归分类器更合适。这样，对于每个新的音乐作品，<strong>您的算法可以单独决定它是否属于四个类别中的每一个</strong>。现在，考虑一个计算机视觉示例，您尝试将图像分为三个不同的类。（i）假设您的课程是indoor_scene，outdoor_urban_scene和outdoor_wilderness_scene。你会使用sofmax回归还是三个逻辑回归分类器？（ii）现在假设你的课程是indoor_scene，black_and_white_image和image_has_people。您会使用softmax回归或多重逻辑回归分类器吗？在第一种情况下，类是互斥的，因此softmax回归分类器是合适的。在第二种情况下，构建三个单独的逻辑回归分类器更为合适。<br>总结就是，如果类别之间是互斥的，那么用<strong>softmax</strong>会比较合适，如果类别之间不是互斥的，用<strong>ovr</strong>比较合适。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-逻辑回归与多分类&quot;&gt;&lt;a href=&quot;#1-逻辑回归与多分类&quot; class=&quot;headerlink&quot; title=&quot;1 逻辑回归与多分类&quot;&gt;&lt;/a&gt;1 逻辑回归与多分类&lt;/h1&gt;&lt;p&gt;我们已经知道，普通的logistic回归只能针对二分类(Binary Cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归 1-3：逻辑回归模型</title>
    <link href="http://conghuai.me/2018/09/24/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%201-3%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>http://conghuai.me/2018/09/24/逻辑回归 1-3：逻辑回归模型/</id>
    <published>2018-09-24T08:39:12.000Z</published>
    <updated>2018-09-24T15:20:40.341Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-逻辑斯蒂回归模型"><a href="#1-逻辑斯蒂回归模型" class="headerlink" title="1 逻辑斯蒂回归模型"></a>1 逻辑斯蒂回归模型</h1><p>二项逻辑斯蒂回归模型是一种分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的逻辑斯蒂分布，这里随机变量$X$取值为实数，随机变量$Y$取值为1或-1。</p><p>$$P(Y=1|x) = \frac{e^{w\cdot x+b}}{1+e^{w\cdot x+b}}$$</p><p>$$P(Y=-1|x) = \frac{1}{1+e^{w\cdot x+b}}$$</p><p>对于给定的输入实例$x$，按照上面式可以求得$P(Y=1|x)$和$P(Y=-1|x)$。逻辑斯蒂回归比较两个条件概率值的大小，将实例$x$分到概率值较大的那一类。</p><p>有时为了方便，将权值向量和输入向量进行扩充，仍记作$w,x$，即$w=(w^{(1)},w^{(2)},…,w^{(n)},b)^T$，$x=(x^{(1)},x^{(2)},…,x^{(n)},1)^T$，这时候，逻辑斯蒂回归模型如下：</p><p>$$P(Y=1|x) = \frac{e^{w\cdot x}}{1+e^{w\cdot x}}$$</p><p>$$P(Y=-1|x) = \frac{1}{1+e^{w\cdot x}}$$</p><h1 id="2-模型参数估计"><a href="#2-模型参数估计" class="headerlink" title="2 模型参数估计"></a>2 模型参数估计</h1><h2 id="2-1-损失函数"><a href="#2-1-损失函数" class="headerlink" title="2.1 损失函数"></a>2.1 损失函数</h2><p>逻辑斯蒂回归模型学习时，对于给定的训练数据集$T=\{(x_1, 1),(x_2,-1),…,(x_N,-1)\}$，其中，$x_i\in R^n \text{,}y_i\in\{-1,1\}$，可以应用极大似然估计法估计模型参数</p><p>$$\begin{aligned} L(h) &amp;= P(x_1)\cdot h(1|x_1)\cdot P(x_2)\cdot h(-1|x_2)…P(x_N)\cdot h(-1|x_N) \\  &amp;= P(x_1)\cdot h(1|x_1)\cdot P(x_2)\cdot [1-h(1|x_2)]…P(x_N)\cdot [1-h(1|x_N)] \end{aligned}$$</p><p>对于$h$我们用sigmoid函数带入，用对称性$1-\sigma(x) = \sigma(-x)$得</p><p>$$\begin{aligned} L(h) &amp;= P(x_1)\cdot \sigma(w_1^Tx_1)\cdot P(x_2)\cdot [1-\sigma(w_2^Tx_2)]…P(x_N)\cdot [1-\sigma(w_N^Tx_N)] \\ &amp;= P(x_1)\cdot \sigma(w_1^Tx_1)\cdot P(x_2)\cdot \sigma(-w_2^Tx_2)…P(x_N)\cdot \sigma(-w_N^Tx_N)\end{aligned} $$</p><p>对于不同的模型，$P(x_1),P(x_2)…P(x_N)$是一样的，乘上这些值对于我们选择最优的模型没有帮助，故略去。</p><p>$$L(\sigma)\propto \sigma(w_1^Tx_1)\cdot \sigma(-w_2^Tx_2) … \sigma(-w_N^Tx_N)$$<br>简化上述，可得两种表示方式</p><ol><li><strong>形式一：$y\in \{-1, 1\}$</strong> </li></ol><p>$$L(\sigma)\propto\prod_{n=1}^N\sigma(y_nw^Tx_n)$$</p><ol><li><strong>形式二：$y\in \{0, 1\}$</strong></li></ol><p>$$L(\sigma)\propto\prod_{n=1}^N[\sigma(w^Tx_n)]^{y_n}\cdot [1- \sigma(w^Tx_n)]^{1-y_n}$$</p><p>下面，通过<strong>极大似然估计</strong>得到的函数取负数，得到等价的需要最小化的损失函数</p><ol><li><strong>形式一：$y\in \{-1, 1\}$</strong></li></ol><p>$$\begin{aligned} g &amp;= argmax_\sigma \prod_{n=1}^N\sigma(y_nw^Tx_n) \\  &amp;= argmax_{\sigma}ln\sum_{n=1}^N\sigma(y_nw^Tx_n) \\  &amp;= argmin_w\frac{1}{N}\sum_{n=1}^N-ln\sigma(y_nw^Tx_n)  \\ &amp;= argmin_w\frac{1}{N} \sum_{n=1}^Nln(1+e^{-y_nw^Tx_n})\\ \end{aligned}$$</p><ol><li><strong>形式二：$y\in \{0, 1\}$</strong></li></ol><p>$$\begin{aligned} g &amp;= argmax_\sigma \prod_{n=1}^N[\sigma(w^Tx_n)]^{y_n}\cdot [1- \sigma(w^Tx_n)]^{1-y_n}\\  &amp;= argmax_{\sigma}\prod_{n=1}^N[\sigma(w^Tx_n)]^{y_n}\cdot [1- \sigma(w^Tx_n)]^{1-y_n} \\  &amp;= argmin_w\frac{1}{N}\sum_{n=1}^N-ln[\sigma(w^Tx_n)]^{y_n}\cdot [1- \sigma(w^Tx_n)]^{1-y_n}  \\ &amp;= argmin_w\frac{1}{N} \sum_{n=1}^N-[y_n\cdot ln\sigma(w^Tx_n)+(1-y_n)\cdot ln(1- \sigma(w^Tx_n))]\\ \end{aligned}$$</p><p>所以，损失函数为</p><ol><li><strong>形式一：$y\in \{-1, 1\}$</strong></li></ol><p>$$E_{in}(w)=\frac{1}{N}\sum_{n=1}^Nln(1+e^{-y_nw^Tx_n})$$</p><ol><li><strong>形式二：$y\in \{0, 1\}$</strong></li></ol><p>$$E_{in}(w)=\frac{1}{N} \sum_{n=1}^N-[y_n\cdot ln\sigma(w^Tx_n)+(1-y_n)\cdot ln(1- \sigma(w^Tx_n))]$$</p><h2 id="2-2-证明损失函数为交叉熵"><a href="#2-2-证明损失函数为交叉熵" class="headerlink" title="2.2 证明损失函数为交叉熵"></a>2.2 证明损失函数为交叉熵</h2><p>如果熟悉交叉熵损失函数，那么可以发现对于形式二，其定义就是交叉熵损失函数。我们现在试着从交叉熵的公式推导到形式一，并证明形式一和形式二等价。</p><h3 id="形式二与形式一等价推导"><a href="#形式二与形式一等价推导" class="headerlink" title="形式二与形式一等价推导"></a>形式二与形式一等价推导</h3><p>如果不太熟悉交叉熵损失函数，可先回顾一下：<a href="http://conghuai.me/2018/04/08/Cross%20Entropy%20Loss%20Function/">Cross Entropy Loss Function</a></p><p>对于二分类损失问题，交叉熵损失函数定义为<br>$$E =\sum_{n=1}^N -[y\cdot log(p)+(1-y)\cdot log(1-p)]$$</p><p>其中</p><ul><li>$y\in \{0,1\}$，一般规定正类为1，负类为0；</li><li>$p$为取正类的概率；</li><li>对于每个样本上式，其中一项为0；</li></ul><p>对于逻辑回归来说，其交叉熵损失函数中，$p$用逻辑函数带入，得</p><p>$$\begin{aligned} E &amp;=\sum_{i=1}^N -(y\cdot log(p)+(1-y)\cdot log(1-p)) \\  &amp;= \sum_{i=1}^N -(y\cdot log(\frac{1}{1+e^{-w\cdot x}})+(1-y)\cdot log(1-\frac{1}{1+e^{-w\cdot x}})) \\  &amp;= \sum_{i=1}^N -(-y\cdot log(1+e^{-w\cdot x})+(1-y)\cdot log(\frac{e^{-w\cdot x}}{1+e^{-w\cdot x}}))  \\ &amp;= \sum_{i=1}^N -(-y\cdot log(1+e^{-w\cdot x})+(1-y)\cdot log(\frac{1}{1+e^{w\cdot x}}))  \\ &amp;=  \sum_{i=1}^N -(-y\cdot log(1+e^{-w\cdot x})-(1-y)\cdot log(1+e^{w\cdot x})) \\ &amp;= \sum_{i=1}^N y\cdot log(1+e^{-w\cdot x})+(1-y)\cdot log(1+e^{w\cdot x})\end{aligned}$$</p><p>我们发现</p><p>$$E=\begin{cases} \sum_{i=1}^Nlog(1+e^{-w\cdot x}), &amp; \text{$y=+1$} \\ \sum_{i=1}^Nlog(1+e^{w\cdot x}), &amp; \text{$y=0$}\end{cases}$$</p><p>我们可以简化上式，得</p><p>$$E= \sum_{i=1}^Nln(1+e^{-y_nw^Tx_n}),y_n\in \{1,-1\}$$</p><p>所以，我们发现，<strong>对逻辑斯蒂回归模型求极大似然估计得到的损失函数，本质上就是交叉熵损失函数</strong>。</p><h1 id="3-学习算法"><a href="#3-学习算法" class="headerlink" title="3 学习算法"></a>3 学习算法</h1><p>对于逻辑回归的损失函数，由于是凸函数，所以我们可以使用梯度下降法来求解。现在，我们推导一下求导过程</p><h2 id="3-1-形式一：-y-in-1-1"><a href="#3-1-形式一：-y-in-1-1" class="headerlink" title="3.1 形式一：$y\in \{-1, 1\}$"></a>3.1 形式一：$y\in \{-1, 1\}$</h2><p>对于形式一，其损失函数如下</p><p>$$E_{in}(w)=\frac{1}{N}\sum_{i=1}^Nln(1+e^{-y_nw^Tx_n})$$</p><p>对$w_i$求偏导，令$A = 1+e^{-y_nw^Tx_n}$，$B = -y_nw^Tx_n$</p><p>$$\frac{\partial E_{in}(w)}{\partial w_i}= \frac{1}{N} \sum_{i=1}^{N} \frac{\partial lnA}{\partial A}\cdot \frac{\partial (1+e^B)}{\partial B}\cdot \frac{\partial -y_nw^Tx_n}{\partial w_i}$$</p><h4 id="第一项-frac-partial-lnA-partial-A"><a href="#第一项-frac-partial-lnA-partial-A" class="headerlink" title="第一项 $\frac{\partial lnA}{\partial A}$"></a>第一项 $\frac{\partial lnA}{\partial A}$</h4><p>$$\frac{\partial lnA}{\partial A} = \frac{1}{A}$$</p><h4 id="第二项-frac-partial-1-e-B-partial-B"><a href="#第二项-frac-partial-1-e-B-partial-B" class="headerlink" title="第二项 $\frac{\partial (1+e^B)}{\partial B}$"></a>第二项 $\frac{\partial (1+e^B)}{\partial B}$</h4><p>$$\frac{\partial (1+e^B)}{\partial B}= e^B$$</p><h4 id="第三项-frac-partial-y-nw-Tx-n-partial-w-i"><a href="#第三项-frac-partial-y-nw-Tx-n-partial-w-i" class="headerlink" title="第三项 $\frac{\partial -y_nw^Tx_n}{\partial w_i}$"></a>第三项 $\frac{\partial -y_nw^Tx_n}{\partial w_i}$</h4><p>$$\frac{\partial -y_nw^Tx_n}{\partial w_i} = -y_nx_{n,i}$$</p><p>最后得</p><p>$$\begin{aligned} \frac{\partial E_{in}(w)}{\partial w_i} &amp;= \frac{1}{N} \sum_{n=1}^{N} \frac{\partial lnA}{\partial A}\cdot \frac{\partial (1+e^B)}{\partial B}\cdot \frac{\partial -y_nw^Tx_n}{\partial w_i} \\  &amp;= \frac{1}{N}\sum_{n=1}^N\frac{1}{A}\cdot e^B \cdot (-y_nx_{n,i}) \\ &amp;= \frac{1}{N}\sum_{n=1}^N\frac{e^{-y_nw^Tx_n}}{1+e^{-y_nw^Tx_n}}\cdot  (-y_nx_{n,i}) \\ &amp;= \frac{1}{N}\sum_{n=1}^N\sigma(-y_nw^Tx_n)\cdot (-y_nx_{n,i})\end{aligned}$$</p><p>通过梯度更新参数：</p><p>$$w_{t+1} \leftarrow w_t-\eta \triangledown E_{in}(w_t)$$</p><h2 id="3-2-形式二：-y-in-0-1"><a href="#3-2-形式二：-y-in-0-1" class="headerlink" title="3.2 形式二：$y\in \{0, 1\}$"></a>3.2 形式二：$y\in \{0, 1\}$</h2><p>对于形式二，其损失函数如下<br>$$E_{in}(w)=\frac{1}{N}\sum_{n=1}^N-[y\cdot log \frac{e^{w\cdot x}}{1+e^{w\cdot x}}+(1-y)\cdot  log\frac{1}{1+e^{w\cdot x}}]$$<br>参考交叉熵求导，可得<br>$$\frac{\partial E}{\partial w_i} = \frac{1}{N}\sum_{n=1}^N[\sigma(w^Tx_n)-y_n]\cdot x_{n,i}$$</p><h2 id="3-3-证明形式一与形式二等价"><a href="#3-3-证明形式一与形式二等价" class="headerlink" title="3.3 证明形式一与形式二等价"></a>3.3 证明形式一与形式二等价</h2><p>对于形式一，我们得到求导的结果为<br>$$E = \frac{1}{N}\sum_{n=1}^N\sigma(-y_nw^Tx_n)\cdot (-y_nx_{n,i})$$<br>当$y_n = 1$时<br>$$\begin{aligned} E_n &amp;= \sigma(-y_nw^Tx_n)\cdot (-y_nx_{n,i}) \\  &amp;= \sigma(-w^Tx_n)\cdot (-x_{n,i}) \\  &amp;= [1 - \sigma(w^Tx_n)]\cdot (-x_{n,i})  \\ &amp;= [\sigma(w^Tx_n) - 1]\cdot x_{n,i}  \\  \\ \end{aligned}$$<br>上述的第三步，主要用到了sigmoid函数的对称性，最后得到的结果与形式二求导的结果一致。</p><p>当$y_n = -1$ 时<br>$$\begin{aligned} E_n &amp;= \sigma(w^Tx_n)\cdot x_{n,i} \\  &amp;= [\sigma(w^Tx_n) - 0 ]\cdot x_{n,i} \\ \end{aligned}$$<br>注意到，形式一和形式二对<code>y</code>取负类的取值不同。所以，上述推导的结果和形式二也是一样的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-逻辑斯蒂回归模型&quot;&gt;&lt;a href=&quot;#1-逻辑斯蒂回归模型&quot; class=&quot;headerlink&quot; title=&quot;1 逻辑斯蒂回归模型&quot;&gt;&lt;/a&gt;1 逻辑斯蒂回归模型&lt;/h1&gt;&lt;p&gt;二项逻辑斯蒂回归模型是一种分类模型，由条件概率分布$P(Y|X)$表示，形式
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归 1-2：逻辑回归由来</title>
    <link href="http://conghuai.me/2018/09/24/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%201-2%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%94%B1%E6%9D%A5/"/>
    <id>http://conghuai.me/2018/09/24/逻辑回归 1-2：逻辑回归由来/</id>
    <published>2018-09-24T08:10:12.000Z</published>
    <updated>2018-09-24T15:20:34.225Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-高斯分布"><a href="#1-高斯分布" class="headerlink" title="1 高斯分布"></a>1 高斯分布</h1><p>首先，LR的假设只有一个，就是两个类别的特征服从<strong>均值不等，方差相等</strong>的高斯分布，也就是</p><p>$$p(x|y=0)\sim N(\mu_0,\sigma)$$</p><p>$$p(x|y=1)\sim N(\mu_1,\sigma)$$</p><p>为什么会假设它们服从高斯分布？一方面是因为高斯分布是比较容易处理的分布，另一方面，从信息论的角度上看，当均值和方差已知时（尽管你并不知道确切的均值和方差，但是根据概率论，当样本量足够大时，样本均值和方差以概率1趋向于均值和方差），高斯分布是熵最大的分布，为什么要熵最大？因为最大熵的分布可以平摊你的风险，这就好比不要把鸡蛋放到同一个篮子里，想想二分查找中，为什么每次都是选取中间点作为查找点？就是为了平摊风险。为什么假设方差相等？为了后面处理起来方便….不相等的话没法消去。。。</p><p>接下来就是贝叶斯决策的东西了，首先，我们定义风险</p><p>$$R(y=0 | x) = \lambda_{00} P(y=0|x) +   \lambda_{01} P(y=1|x)$$</p><p>$$R(y=1 | x) = \lambda_{10} P(y=0|x) +  \lambda_{11} P(y=1|x)$$</p><p>其中，$R(y=0|x)$是把样本预测为0时的风险，$R(y=1|x)$是把样本预测为1时的风险，$λ_{ij}$是样本实际标签为$j$时，却把它预测为$j$是所带来的风险。</p><p>在LR里，我们认为预测正确并不会带来风险，因此$λ<em>{00}$和$λ</em>{11}$都为0，此外，我们认为当标签为0而预测为1 和 当标签为1而预测为0，这两者所带来的风险是相等的，因此$λ<em>{10}$和$λ</em>{01}$相等，方便起见，我们记为$λ$。这里你可能认为我说的是废话，但在一些领域里，比如医学、风控等，这些λ在大多数情况下是不相等的，有时候我们会选择“宁可杀错一百也不能放过一个”<br>所以，上面定义的风险就可以简化为</p><p>$$R(y=0 | x) =   \lambda P(y=1|x)$$</p><p>$$R(y=1 | x) =   \lambda P(y=0|x)$$</p><p>现在问题来了，我拿到一个样本，我应该把它预测为0还是预测为1好？按照风险最小化的原则，我们应该选择风险最小的，也就是，当</p><p>$$R(y=0 | x)  &lt; R(y=1 | x)$$</p><p>时，预测为0的风险要小于预测为1的风险，即</p><p>$$P(y=1|x) &lt; P(y=0|x)$$</p><p>时，应该把样本预测为0，从而也就是书上提到的：比较两个条件概率，并把样本分配到概率最大的那个类上。</p><p>我们两边除一下，就会得到</p><p>$$\frac{P(y=1|x)}{ P(y=0|x) } &lt; 1$$</p><p>我们对不等式左边的部分取一下对数，（为什么取对数？因为之前我们提过，两个类别的特征服从均值不等，方差相等的高斯分布，<br>取对数方便处理高斯分布里的指数），再利用贝叶斯公式进行展开，归一化常数扔掉，我们将得到</p><p>$$\log \frac{P(y=1|x)}{ P(y=0|x) } = \log  \frac{P(x, y=1)}{ P(x, y=0) }$$</p><p>$$= \log  \frac{P(x | y=1) P(y=1)}{ P(x | y=0) P(y=0) }$$</p><p>$$=  \log  \frac{P(x | y=1) }{ P(x | y=0) }  +  \log  \frac{ P(y=1)}{ P(y=0) }$$</p><p>为了方便起见，我们假设x是一维的，当然也很容易推广到多维的情况，我们套入高斯分布的公式，此外，由于$P(y=1)$和<br>$P(y=0)$都是常数，第二项我们简记为常数$C_1$继续展开，将得到</p><p>$$= -\frac{(x - \mu_1)^2}{2\sigma^2} +  \frac{(x - \mu_0)^2}{2\sigma^2}   + C_1$$</p><p>打开括号，化简，就会得到</p><p>$$= \frac{\mu_1}{\sigma^2} x  - \frac{\mu_0}{\sigma^2} x  +  C_2$$</p><p>整理整理，就会得到</p><p>$$= \theta x$$</p><p>也就是</p><p>$$\log \frac{P(y=1|x)}{ P(y=0|x) } = \theta x$$</p><p>两面取指数，并且利用上$P(y=1|x)+P(y=0|x)=1$这个概率公理，移一下，你就会看到熟悉的logistic公式</p><p>$$P(y=1|x) = \frac{1}{1 +  e^{-\theta x}}$$</p><h1 id="2-对数几率"><a href="#2-对数几率" class="headerlink" title="2 对数几率"></a>2 对数几率</h1><p>现在考察逻辑斯蒂回归模型的特点，一个事件的几率是指该事件发生的概率与该事件不发生的概率的比值，如果事件发生的概率是$p$，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率或logit函数是</p><p>$$logit(p)= log\frac{p}{1-p}$$</p><p>对于逻辑斯蒂回归而言，得</p><p>$$\begin{aligned} log\frac{P(Y=1|x)}{1-P(Y=1|x)} &amp;= log\frac{\frac{e^{w\cdot x}}{1+e^{w\cdot x}}}{1- \frac{e^{w\cdot x}}{1+e^{w\cdot x}}} \\  &amp;= log\frac{e^{w\cdot x}}{1+e^{w\cdot x}-e^{w\cdot x}} \\  &amp;= loge^{w\cdot x}  \\&amp;= w\cdot x  \end{aligned}$$</p><p>这就是说，在逻辑斯蒂回归模型中，输出$Y=1$的对数几率是输入$x$的线性函数，或者说，输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯蒂回归模型。</p><p>换一个角度看，考虑对输入$x$进行分类的线性函数$w\cdot x$，其值域为实数域，通过逻辑斯蒂回归模型可以将线性函数$w\cdot x$转化为概率：</p><p>$$P(Y=1|x) = \frac{e^{w\cdot x}}{1+e^{w\cdot x}}$$</p><p>这时，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0。</p><h1 id="3-最大熵原理"><a href="#3-最大熵原理" class="headerlink" title="3 最大熵原理"></a>3 最大熵原理</h1><p>我们现在尝试把最大熵模型推导成logistic回归模型：</p><p>最大熵模型定义了在给定输入变量$x$时，输出变量$y$的条件分布：</p><p>$$P(y|x,w)=\frac{e^{w\cdot f(x,y)}}{\sum_{y\in Dom(y)} e^{w\cdot f(x,y)}}$$</p><p>如果我们我们限定$y$为二元变量，即$Dom(y) = \{y_0,y_1\}$，那么就可以把最大熵模型转换为logistic回归模型，我们还需要定义特征函数为：</p><p>$$f(x,y)=\begin{cases} g(x), &amp; \text{$y=y_1$} \\ 0, &amp; \text{$y=y_0$}\end{cases}$$</p><p>即仅在$y=y_1$时抽取$x$的特征，在$y=y_0$时不抽任何特征（直接返回全为0的特征向量。）</p><p>将这个特征函数带回最大熵模型，我们得到$y=y_1$时：</p><p>$$\begin{aligned} P(y_1|x) &amp;= \frac{e^{w\cdot f(x,y_1)}}{e^{w\cdot f(x,y_0)}+e^{w\cdot f(x,y_1)}} \\  &amp;= \frac{e^{w\cdot g(x)}}{e^{w\cdot 0}+e^{w\cdot g(x)}} \\  &amp;= \frac{e^{w\cdot g(x)}}{1+e^{w\cdot g(x)}}  \\ &amp;= \sigma(w\cdot g(x)) \\ \end{aligned}$$</p><p>当$y=y_0$时，得</p><p>$$\begin{aligned} P(y_0|x) &amp;= \frac{e^{w\cdot f(x,y_0)}}{e^{w\cdot f(x,y_0)}+e^{w\cdot f(x,y_1)}} \\  &amp;= \frac{e^{w\cdot 0}}{e^{w\cdot 0}+e^{w\cdot g(x)}} \\  &amp;= \frac{1}{1+e^{w\cdot g(x)}}  \\ &amp;= \sigma(-w\cdot g(x)) \\&amp;=1-\sigma(w\cdot g(x)) \\ \end{aligned}$$</p><p>我们发现，当类别标签只有两个时，最大熵模型就是logistic回归模型，表面上看，logistic回归模型里面的特征函数的确只考虑$x$不考虑$y$，然而通过上面的推导，我们发现其实$g$抽取的特征仅仅在$y=y_1$时被用到。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-高斯分布&quot;&gt;&lt;a href=&quot;#1-高斯分布&quot; class=&quot;headerlink&quot; title=&quot;1 高斯分布&quot;&gt;&lt;/a&gt;1 高斯分布&lt;/h1&gt;&lt;p&gt;首先，LR的假设只有一个，就是两个类别的特征服从&lt;strong&gt;均值不等，方差相等&lt;/strong&gt;的高斯分
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归 1-1：逻辑函数</title>
    <link href="http://conghuai.me/2018/09/24/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%201-1%EF%BC%9A%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0/"/>
    <id>http://conghuai.me/2018/09/24/逻辑回归 1-1：逻辑函数/</id>
    <published>2018-09-24T07:20:12.000Z</published>
    <updated>2018-09-24T15:20:24.351Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-表达式"><a href="#1-表达式" class="headerlink" title="1 表达式"></a>1 表达式</h1><p>逻辑函数（逻辑曲线、sigmoid函数）表达式如下：</p><p>$$\sigma(x)=\frac{L}{1+e^{-k(x-x_0)}}$$</p><p>其中：</p><ul><li>$e$是自然对数；</li><li>$x_0$是sigmoid曲线的中点；</li><li>$L$是曲线的最大值；</li><li>$k$是曲线的坡度</li></ul><p>其标准形式为：$L=1,k=1,x_0=0$</p><p>标准的逻辑函数表达式如下：</p><p>$$\begin{aligned} \sigma(x) &amp;= \frac{1}{1+e^{-x}} \\  &amp;= \frac{e^x}{1+e^x} \\  &amp;= \frac{1}{2}+\frac{1}{2}\cdot tanh(\frac{x}{2}) \end{aligned}$$</p><h1 id="2-函数性质"><a href="#2-函数性质" class="headerlink" title="2 函数性质"></a>2 函数性质</h1><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-17-040033.jpg" alt="logistics function函数图像"></p><ul><li>对称性：$1-\sigma(x) = \sigma(-x)$</li><li>输出值不是关于0对称的</li></ul><h1 id="3-导函数"><a href="#3-导函数" class="headerlink" title="3 导函数"></a>3 导函数</h1><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-09-23-114422.png" alt=""></p><p>逻辑函数的一个优点在于其容易求导：</p><p>$$\begin{aligned} \frac{d\sigma(x)}{dx} &amp;= \frac{e^x(1+e^x)-e^x\cdot e^x}{(1+e^{-x})^2} \\  &amp;= \frac{e^x}{(1+e^x)^2} \\  &amp;= \sigma(x)\cdot (1-\sigma(x)) \end{aligned}$$</p><p>但是其缺点在于导函数的值域在(0,1/4)，当多个sigmoid的导数相乘的时候，会使得值越来越小，这也是梯度消失的原因。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-表达式&quot;&gt;&lt;a href=&quot;#1-表达式&quot; class=&quot;headerlink&quot; title=&quot;1 表达式&quot;&gt;&lt;/a&gt;1 表达式&lt;/h1&gt;&lt;p&gt;逻辑函数（逻辑曲线、sigmoid函数）表达式如下：&lt;/p&gt;
&lt;p&gt;$$\sigma(x)=\frac{L}{1+e
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>集成学习 1-1：Bagging &amp; Blendding</title>
    <link href="http://conghuai.me/2018/05/24/Bagging%20&amp;%20Blendding/"/>
    <id>http://conghuai.me/2018/05/24/Bagging &amp; Blendding/</id>
    <published>2018-05-24T03:10:15.000Z</published>
    <updated>2018-09-25T03:27:19.120Z</updated>
    
    <content type="html"><![CDATA[<p>模型融合策略是指我们对多个机器学习算法模型进行组合，以构造出一个更强大的分类器，组合多个模型后得到的结果往往比单个模型的效果要好。这里涉及两个概念，1. 如何得到多个不同的基模型？2. 有了多个基模型后，如何组合？针对此，Bagging强调产生模型的方式，即如何获得不同的基模型，而Blending是假设模型都有了，采用哪种组合策略。</p><h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p>我们要得到不同的模型，我们通常有两种思路，一种就是用同一套模型，但是训练数据不一样，另外一种就是用不一样的模型。后者一般比较直接，只需要在同一份数据集上训练不同的模型即可，这里不再赘述。对于前者，我们需要考虑如何用已有的训练数据，产生出不一样的样本呢？一般都是采用<code>Bootstrap</code>的方式：先抓一个，记录下来，放回去，摇一摇，在抓一个…（注意这种抓取方式可能使得同一笔资料被抓取多次）。</p><p>通过根据训练子集获取方式的不同，可以分为以下几类：</p><ul><li>随机取样——Pasting</li><li><strong>有放回取样（Bootstrap）——Bagging</strong> 这种采样方式，有$N!/N^N$种几率从N个样本抽取出来的样本和原来样本一样。</li><li>特征子集——Random Subspaces</li><li>样本和特征都是子集——Random Patches</li></ul><p>所有选取样本子集的时候有两个方面要考虑，是否选取特征，是否为有放回选取。</p><p>在取样的时候，我们有时候拿袋外的样本当做测试样本。</p><p>那么这个袋外的资料有多少呢：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-24-032158.jpg" alt="3"></p><p><strong>是否可以用来做验证集？</strong></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-24-032134.jpg" alt="2"></p><p>是的，但是要记住，在模型融合中，我们要的不是单个模型的最优，而是所有模型融合后的最优，所以我们希望用这些袋外数据来对G做验证集，具体如下:</p><p>对于$(x_i, y_i)$然后来交叉验证$G_{N}^-(x) = average(g_i, g_j, …, g_t)$，其中，这些g是单个模型，而且$(x_i, y_i)$对于这些模型是袋外。最后求总的验证误差 ： $E_{oob}(G) = \frac{1}{N}\sum_{n=1}^Nerr(y_n, G^-_n(x_n))$</p><h1 id="Blending"><a href="#Blending" class="headerlink" title="Blending"></a>Blending</h1><p>Blending强调的是组合弱分类器的策略，而不强调产生不同模型的方式。组合模型的策略有线性组合，此时每个模型都有一个投票权重，当权重都一样的时候，即都是1时，称为Uniform Blending，如果权重不一样呢，我们一般称为Linear Blending；当然也有非线性组合，我们一般称为Stacking，决策树模型实际上就是这种方式。</p><h2 id="Uniform-Blending-Voting"><a href="#Uniform-Blending-Voting" class="headerlink" title="Uniform Blending(Voting)"></a>Uniform Blending(Voting)</h2><p>这种方式有称为投票的方式，我们给每个模型的权重是一样的，都是1票：</p><ul><li>对于二分类问题 $G(x) = sign(\sum_{t=1}^Tg_t(x))$，对于多分类问题 $G(x) = argmax_{1&lt;=k&lt;=K}\sum_{t=1}^T[g_t(x) = k]$</li><li>对于回归问题，我们对所有结果相加然后求平均：$G(x) = \frac{1}{T} \sum_{t=1}^T g_t(x)$</li></ul><h3 id="数学推导证明有效性"><a href="#数学推导证明有效性" class="headerlink" title="数学推导证明有效性"></a>数学推导证明有效性</h3><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-24-031151.jpg" alt="1"></p><p>我们可以认为右边的第一项为variance，第二项为bias，我们希望通过降低variance的方式来是的模型的误差变小。</p><h2 id="Linear-Blending"><a href="#Linear-Blending" class="headerlink" title="Linear Blending"></a>Linear Blending</h2><p>上面讨论的是均匀组合，但是，有时候我们需要给不同的模型以不同的权值。</p><p>即，$G(x) = sign(\sum_{t=1}^T \alpha_t g_t(x)) with \alpha_t &gt;= 0 $</p><p>这时候我们需要通过计算，求得是的$E_{in}$最小的$\alpha$，如果选择损失函数为均方损失函数，我们就可以得到如下的表达式：$min_{\alpha_t &gt;= 0} \frac{1}{N}\sum_{n=1}^N(y_n - \sum_{t=1}^T \alpha_t g_t(x_x))^2$</p><p>实践中，我们的做法都是，先在训练数据上选出最佳的单个模型，然后在验证集上进行权值的选择。</p><h2 id="Any-Blending（Stacking）"><a href="#Any-Blending（Stacking）" class="headerlink" title="Any Blending（Stacking）"></a>Any Blending（Stacking）</h2><p>我们可以不用线性组合的方式，而是进行其他的组合方式，这样的方式的另一个名称叫做Stacking</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-24-042805.jpg" alt="workflow"></p><ul><li>初始训练数据$X$包含$m$个观察值和$n$个特征（所以它是$m\times n$。</li><li>有$M$个不同的模型在$X$（通过某种训练方法，如交叉验证）进行训练。</li><li>每个模型提供对结果$y$的预测，然后将其转换为$m\times M$的第二级训练数据$X^{(l_2)}$。即，$M$个预测成为该第二级数据的特征。然后可以对这些数据进行二级模型（或多个模型）的训练，以产生将用于预测的最终结果。</li></ul><p>现在研究用Stacking方式构建二级训练数据的，这里的关键词是out-of-sample，因为如果我们使用适合所有训练数据的$M$模型的预测，那么第二级模型将偏向$M$模型中的最佳模型，这将是没用的。</p><p>这个想法是将训练集分成几部分，就像你在k-fold交叉验证中所做的一样。对于每折数据，其余的数据在模型1 … M获得预测，并在该折上做预测。解释这个最好的方法是通过下图：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-24-042807.jpg" alt="stacking"></p><p>最后这个预测矩阵将用于二级训练数据。</p><h1 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h1><p>在天池、Kaggle等机器学习比赛中，要想提高得分，一个重要的技巧就是模型融合，在实际使用中，我们一般会采用哪些做法呢？</p><h2 id="对submmison文件进行融合"><a href="#对submmison文件进行融合" class="headerlink" title="对submmison文件进行融合"></a>对submmison文件进行融合</h2><p>模型融合的最基本和最方便的方式是融合Kaggle提交CSV文件。您只需要对这些方法的测试集进行预测 - 无需重新训练模型。这使得它可以快速集成已经存在的模型预测，是组队时的理想选择。</p><h2 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h2><p>对预测文件进行平均并不困难，但它并不是顶级Kagglers使用的唯一方法。更好的收益始于Stacking和Blending。</p><p>原理在上面已经描述过，以2折为例子，说明一下：</p><ul><li>Split the train set in 2 parts: train_a and train_b</li><li>Fit a first-stage model on train_a and create predictions for train_b</li><li>Fit the same model on train_b and create predictions for train_a</li><li>Finally fit the model on the entire train set and create predictions for the test set.</li><li>Now train a second-stage stacker model on the probabilities from the first-stage model(s).</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航 统计学习方法</p><p>[2]. <a href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="noopener">Kaggle Ensembling Guide | MLWave</a></p><p>[3]. <a href="https://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html" target="_blank" rel="noopener">Stacking Models for Improved Predictions</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;模型融合策略是指我们对多个机器学习算法模型进行组合，以构造出一个更强大的分类器，组合多个模型后得到的结果往往比单个模型的效果要好。这里涉及两个概念，1. 如何得到多个不同的基模型？2. 有了多个基模型后，如何组合？针对此，Bagging强调产生模型的方式，即如何获得不同的基
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="blending" scheme="http://conghuai.me/tags/blending/"/>
    
      <category term="stacking" scheme="http://conghuai.me/tags/stacking/"/>
    
  </entry>
  
  <entry>
    <title>Random Forest</title>
    <link href="http://conghuai.me/2018/05/23/Random-Forest/"/>
    <id>http://conghuai.me/2018/05/23/Random-Forest/</id>
    <published>2018-05-23T14:45:06.000Z</published>
    <updated>2018-09-24T15:29:28.074Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h2><p>在模型融合里，blending方式通过组合不同模型投票的结果，来使得融合后的模型variance变小。而决策树，通常variance比较大，我们是否能够结合这两种思路呢？答案是可以，这就是随机森林。</p><p>Random Forest(RF) = bagging + fully-grown C&amp;RT decision tree.</p><p>基本做法：</p><ol><li>通过Bootstrap的方式取得不同数据和特征，输入到决策树中训练得到不同的决策树。</li><li>组合不同的决策树，构成最终的分类器。</li></ol><p>随机性的体现：</p><ol><li>Bootstrap采用</li><li>特征投影<ol><li>选择不同的特征</li><li>随机的方向</li></ol></li></ol><h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>假设我们资料里面特征具有依赖性，比如生日和年龄（用生日可以推出年龄），或者说有一些特征对于最后的分类结果没有什么作用。</p><p>如果模型是线性的，那么我们通过w分量大小就可以判断出哪个模型比较重要，因为$score = wx$，但是如果是非线性的模型，那么这个问题就比较复杂，但是Random Forest由于其模型的特性，也可以用来做特征选择。</p><h5 id="permutation-test进行特征重要性衡量"><a href="#permutation-test进行特征重要性衡量" class="headerlink" title="permutation test进行特征重要性衡量:"></a>permutation test进行特征重要性衡量:</h5><ul><li>对于第i个特征的所有样本，我们进行重排列（即甲的身高值给乙的身高值，乙的身高值给甲的身高值）。</li><li>计算重排序后的<strong>performance</strong>：<ul><li>$importance(i) = performance(D) - performance(D^p)$ with $D^p$ is D with ${x_n, i}$ replaced by permuted ${x_n, i}_{n=1}^N$</li><li>正常来说，performance应该怎么得到呢？我们应该对资料为D的模型进行训练，然后获得交叉验证的结果，作为performance(D)。并且对资料为$D^p$的模型进行训练，然后获得交叉验证的结果，作为performance($D^p$)。<strong>注意</strong>，由于训练样本的获取是采样bootstrap，所以可以利用out-of-bag样本做验证集。即，上面的式子就变成了 importance(i) = $E_{oob}(G) - E_{oob}(G^p)$</li><li>但是，原始的随机森林的作者， 提出了在验证的过程中做permutation而不是训练的过程中，这样的话，我们只需要做一次训练，两次验证即可。这样的话，结果就变成了importance(i) = $E_{oob}(G) - E_{oob}^p(G)$</li></ul></li></ul><h1 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h1><ul><li>由于每次选择不是在全部的特征上选择最佳切分点，所有bias会有一些上升。</li><li>由于平均化，所有variance会有所下降。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Random-Forest&quot;&gt;&lt;a href=&quot;#Random-Forest&quot; class=&quot;headerlink&quot; title=&quot;Random Forest&quot;&gt;&lt;/a&gt;Random Forest&lt;/h2&gt;&lt;p&gt;在模型融合里，blending方式通过组合不同模型投
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="tree" scheme="http://conghuai.me/tags/tree/"/>
    
  </entry>
  
  <entry>
    <title>词向量总结</title>
    <link href="http://conghuai.me/2018/05/23/%E8%AF%8D%E5%90%91%E9%87%8F%E6%80%BB%E7%BB%93/"/>
    <id>http://conghuai.me/2018/05/23/词向量总结/</id>
    <published>2018-05-23T03:26:25.000Z</published>
    <updated>2018-05-23T07:24:02.684Z</updated>
    
    <content type="html"><![CDATA[<p>自然语言处理任务中，词通常作为最小的基本单位，如何提取词的特征也成为非常重要的话题。常见的提取特征的方式有：one-hot表示、tf-idf表示、word class和word embedding方式。</p><h1 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h1><h2 id="One-Hot"><a href="#One-Hot" class="headerlink" title="One-Hot"></a>One-Hot</h2><p>这种方式对词的表征方式非常的简单直接，首先收集样本中的词，构建词表，假设总共收集了10,000个单词，那么每个单词的就是10,000维的向量。该向量上只有一个是1，其他位置是0：</p><p><img src="../../../../../../../Desktop/Screen Shot 2018-01-25 at 22.42.41.png" alt="Screen Shot 2018-01-25 at 22.42.41"></p><p>这种处理方式虽然简单，但是往往会造成词维度太大，而且很难捕捉两个近义词之间的关系，所以one-hot表示方法效果往往不是特别好。</p><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><p>字词的重要性随着它在该文档中出现的次数成正比例增加，但同时会随着它在预料库中出现的频率成反比下降。TF-IDF提取词特征方式能够捕捉这一特征，进而赋予重要词语较高的权值。</p><ol><li><p>词频（term frequence TF）: 词语在文档中出现的次数。</p><p>这个数字是对词数的归一化，以防止它偏向长的文件，其表达式如下：</p><p>$$tf_{i,j}=\frac{n_{i,j}}{\sum_kn_{k,j}} \text{，其中i表示第i个词，j表示第j篇文档}$$</p></li><li><p>逆向文件频率（inverse document frequency IDF）：总文件数目除以包含该词语的文件数目。</p><p>这一指标是词语普遍重要性的度量，其表达式如下：</p><p>$$idf_i=log\frac{|D|}{|{j:w_i\in d_j}|}$$</p><ul><li>$|D|$：语料库中总的文件数；</li><li>$|{j:w_i\in d_j}|$：包含该词语的文件数目，如果词语不在预料库中，就会导致除数为0，因此做平滑处理。</li></ul></li><li><p>$tfidf_{i,j}=tf_{i,j}\cdot idf_i$</p></li></ol><ul><li>$tf_{i,j}$指标与具体的文档有关，是局部指标；</li><li>$idf_i$与具体文档无关，只与全体文档和目标词有关，是一个全局指标。</li></ul><h1 id="分布式表示"><a href="#分布式表示" class="headerlink" title="分布式表示"></a>分布式表示</h1><p>分布式表示方法的基本想法就是将词嵌入到一个数值空间中， 表示成向量形式。</p><p><img src="../../../../../../../Desktop/Screen Shot 2018-01-25 at 22.44.53.png" alt="Screen Shot 2018-01-25 at 22.44.53"></p><h2 id="共现矩阵"><a href="#共现矩阵" class="headerlink" title="共现矩阵"></a>共现矩阵</h2><p><img src="../../../../../../../Desktop/Screen Shot 2018-03-16 at 20.35.39.png" alt="Screen Shot 2018-03-16 at 20.35.39"></p><ul><li>将共现矩阵行（列）作为 词向量；</li><li>向量维数随着词典大小线性增大，可以考虑用SVD对共现矩阵进行降维；</li><li>存储整个词典的空间，消耗非常大；</li><li>一些模型，如文本分类模型，会面临稀疏性问题；</li><li>模型欠稳点。</li></ul><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><h3 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h3><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 台湾大学 李宏毅 机器学习课程</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;自然语言处理任务中，词通常作为最小的基本单位，如何提取词的特征也成为非常重要的话题。常见的提取特征的方式有：one-hot表示、tf-idf表示、word class和word embedding方式。&lt;/p&gt;
&lt;h1 id=&quot;词袋模型&quot;&gt;&lt;a href=&quot;#词袋模型&quot; c
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://conghuai.me/categories/Deep-Learning/"/>
    
      <category term="Natural Language Processing" scheme="http://conghuai.me/categories/Natural-Language-Processing/"/>
    
      <category term="Feature Extraction" scheme="http://conghuai.me/categories/Natural-Language-Processing/Feature-Extraction/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="nlp" scheme="http://conghuai.me/tags/nlp/"/>
    
      <category term="word2vec" scheme="http://conghuai.me/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>Recurrent Neural Network Foundation</title>
    <link href="http://conghuai.me/2018/05/20/Recurrent-Neural-Network-Foundation/"/>
    <id>http://conghuai.me/2018/05/20/Recurrent-Neural-Network-Foundation/</id>
    <published>2018-05-20T08:57:33.000Z</published>
    <updated>2018-09-24T15:29:36.227Z</updated>
    
    <content type="html"><![CDATA[<p>序列模型是深度学习中最令人兴奋的领域之一。像递归神经网络或RNN这样的模型应用在语音识别，自然语言处理和其他领域。那么不能用传统的神经网络来解决自然语言领域的问题呢？主要有两个原因：1. 自然语言问题中不同实例数据的输入和输出长度往往都是不同的，例如在情感分类问题中，输入可以使评论数据，而不同评论数据的长度是不一样的。对于机器翻译任务，针对不同的输入，翻译后的结果长度也往往不同；2. 如果对于不同长度的输入数据，按最大数据数据的长度进行填充，能解决上述的问题，但是，采用这种方式在传统的神经网络模型中进行训练，通常都不能学习到词语之间的序列（位置）信息。</p><p>基于上述原因，序列模型随之诞生。</p><h1 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h1><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-21-64345.jpg" alt="Screen Shot 2018-05-21 at 14.23.12"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-21-064346.jpg" alt="3"></p><p>根据输入数据和输出数据的长度，我们可以将序列模型分为如下几种类型：</p><ol><li><strong>One-to-One</strong>：输入是单个，输出也是单个，这其实是一种最特殊的序列模型；</li><li><strong>One-to-Many</strong>：当个输入，得到多个输出，类似的任务如文本生成或者音乐生成；</li><li><strong>Many-to-one</strong>：多个输入，得到单个输出，如情感分析，我们的输入是评论数据，对于每个评分输出一个类别变量，而这个评论对应对个多个词，也即多个输入序列；</li><li><strong>Many-to-Many</strong>： 机器翻译。</li></ol><h1 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h1><p>假设我们的输入为：$\text{x : Harry Potter and Hermione Granger invented a new spell.}$</p><p>我们的任务是识别x中每个词是否是实体，现在我们建立一个序列模型来对这个问题进行建立，如下图所示</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-21-064351.jpg" alt="Screen Shot 2018-05-20 at 17.34.45"></p><ul><li>$x$：输入数据集；</li><li>$x^{(i)}$：第$i$个输入数据；</li><li>$x^{&lt; t &gt;}$：第$t$时刻的输入数据；</li><li>$x^{(i)&lt; t &gt;}$：第$i$个样本在第$t$时刻的输出；</li><li>$y$：输出数据集；</li><li>$y^{(i)}$：第$i$个样本的输出数据；</li><li>$y^{&lt; t &gt;}$：第t时刻的输出数据；</li><li>$y^{(i)&lt; t &gt;}$：第$i$个样本在第$t$时刻的输出。</li></ul><p>对于上述的问题，我们可以试着使用标准的神经网络来求解，其结构图如下所示：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-21-064343.jpg" alt="Screen Shot 2018-05-20 at 17.40.54"></p><p>但是，正如我们前面叙述的一样，这样做的效果并不好，并且其中存在两个主要的问题。第一个问题是，输入和输出在不同的实例中，长度可能会不同。所以，并不是每个实例都有相同的输入长度$T_x$或相同的输出长度$T_y$。另外，如果每个句子都有最大长度，可能我们需要扩充或零扩充每个输入项，使其达到最大长度，但是这仍然不是高质量的表示方法。另外，第二个问题可能更严重，像这样的朴素神经网络结构，不会将那些从不同文本位置学到的特征，进行共享。</p><p>下面，我们引进最基本的序列模型RNN，其网络结构如下：</p><p><img src="http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg" alt=""></p><p>RNN模型的计算步骤如下：</p><ul><li>$x_t$是时间$t$的输入。</li><li>$s_t$是隐层在时间t的状态值。该值也称为“<strong>记忆</strong>”，它的值取决于$s_{t-1}$和$x_t$，即$s_t=f(Ux_t+Ws_{t-1})$，这个激活函数通常选择<strong>tanh</strong>和<strong>ReLU</strong>。$s_{-1}$通常初始化为0。</li><li>$o_t$是我们在时间t的输出，它通常是在词汇表上的softmax：$o_t = softmax(Vs_t)$。</li></ul><p>RNN模型的特性如下：</p><ul><li>网络结构有$U,V,W$三个参数，这三个参数在每个时刻共享；</li><li>没有用到后面的信息，只用到了前面的信息；</li><li>Elman Network 上一个时间点的隐层输出到下一个时间点；</li><li>Jordan Network 上一个时间点的输出层接到下一个时间点。</li></ul><h2 id="RNN模型学习"><a href="#RNN模型学习" class="headerlink" title="RNN模型学习"></a>RNN模型学习</h2><p>训练RNN和训练传统的神经网络模型类似，我们也可以使用反向传播算法，但稍加改动。由于参数在网络中的所有时刻共享，因此，每个输出的梯度不仅取决于当前时刻的计算，还取决于之前时刻的计算值。例如，为了计算梯度，我们需要反向传播3个步骤并总结梯度，这被称为BPTT。但是，需要注意的是，用BPTT训练RNN容易出现所谓的梯度消失/梯度爆炸问题，这使得模型难以学习到样本之间长期的相关性（即两个相距较长的时刻之间的相关性）。</p><h2 id="Backpropagation-Through-Time（BPTT）"><a href="#Backpropagation-Through-Time（BPTT）" class="headerlink" title="Backpropagation Through Time（BPTT）"></a>Backpropagation Through Time（BPTT）</h2><p>我们先回顾一下RNN的基本计算公式：</p><p>$$a_t = tanh(Ux_t + Wa_{t-1})$$</p><p>$$\hat{y_t}=softmax(Vs_t)$$</p><p>我们定义交叉熵损失函数为：</p><p>$$L^{&lt; t &gt;}(\hat{y}^{&lt; t &gt;}, y^{&lt; t &gt;})=-y^{&lt; t &gt;}log\hat{y}^{&lt; t &gt;}-(1-y^{&lt; t &gt;})log(1-\hat{y}^{&lt; t &gt;})$$</p><p>$$L(\hat{y}, y) = \sum_{t=1}^{T_x}L^{}(\hat{y}^{&lt; t &gt;}, y^{&lt; t &gt;})$$</p><p>这里，$y_t​$是时刻$t​$正确的输出值，而$\hat{y}_t​$是我们的预测值。我们通常把整个序列（句子）当做一个训练样本，所以总误差可看做是每个时刻（单词）误差的总和。<img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-21-064352.jpg" alt="Screen Shot 2018-05-21 at 10.50.07"></p><p>我们的目标是根据参数计算误差的梯度，然后使用随机梯度下降来学习好的参数，由于我们将每个时刻的误差累加，我们将求导展开到每个时刻：$\frac{\partial L}{\partial W}=\sum_t\frac{\partial L^{&lt; t &gt;}}{\partial W}$</p><p>下面，以计算上图的$L^{<3>}$来说明BPTT的计算过程：</3></p><h3 id="计算-frac-partial-L-partial-V"><a href="#计算-frac-partial-L-partial-V" class="headerlink" title="计算$\frac{\partial L^{}}{\partial V}$"></a>计算$\frac{\partial L^{<3>}}{\partial V}$</3></h3><p>$$\begin{align} \frac{\partial L^{<3>}}{\partial V} &amp;= \frac{\partial L^{<3>}}{\partial \hat{y}^{<3>}}\cdot \frac{\partial \hat{y}^{<3>}}{\partial V} \  &amp;= \frac{\partial L^{<3>}}{\partial \hat{y}^{<3>}}\cdot \frac{\partial \hat{y}^{<3>}}{\partial z^{<3>}}\cdot \frac{\partial z^{<3>}}{\partial V} \  &amp;= (\hat{y}^{<3>}-y^{<3>})\otimes a^{<3>}  \end{align}$$</3></3></3></3></3></3></3></3></3></3></3></3></p><ul><li>其中，$z^{<3>}=V\cdot a^{<3>}$；</3></3></li><li>从上面可以发现，对于该参数的求导只依赖于当前时刻的值：$\hat{y}^{<3>},y^{<3>},a^{<3>}$。</3></3></3></li></ul><h3 id="计算-frac-partial-L-partial-W"><a href="#计算-frac-partial-L-partial-W" class="headerlink" title="计算$\frac{\partial L^{}}{\partial W}$"></a>计算$\frac{\partial L^{<3>}}{\partial W}$</3></h3><p>$$\frac{\partial L^{<3>}}{\partial W}=\sum_{t=0}^3\frac{\partial L^{<3>}}{\partial \hat{y}^{<3>}}\cdot \frac{\partial \hat{y}^{<3>}}{\partial a^{<3>}}\cdot \frac{\partial a^{<3>}}{\partial a^{&lt; k &gt;}}\cdot \frac{\partial a^{&lt; k &gt;}}{\partial W}$$</3></3></3></3></3></3></p><p>我们将每个时刻对于梯度的贡献值都累加起来，我们从上面的式子或者下面示意图中可以发现，$W$在每个时刻的损失函数计算中都被使用到了，所以我们使用梯度下降法对其进行更新的时候，我们需要从$t=3$到$t=0$，都将梯度进行反向传播。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-21-064347.jpg" alt="Screen Shot 2018-05-21 at 11.30.09"></p><p>注意到，上面的计算过程与深度前馈神经网络中使用的标准反向传播算法是完全相同的，关键的区别是我们将每个时刻的梯度进行累加。在传统的神经网络中，我们不会跨层共享参数，所以我们不需要累加任何内容。</p><h2 id="梯度消失、梯度爆炸"><a href="#梯度消失、梯度爆炸" class="headerlink" title="梯度消失、梯度爆炸"></a>梯度消失、梯度爆炸</h2><p>RNN在学习长距离依赖性方面存在困难，在自然语言问题中，这个依赖就是指相隔几个词距离的单词之间的相互作用关系，比如这样一句话：“The man who wore a wig on his head went inside”，当我们看到went inside的时候，我们需要知道主语是”The man”而不是”wig”，这需要我们捕捉住长依赖的信息。</p><p>我们通过分析数学表达式来看一下为什么RNN在捕捉该信息上存在困难，我们有如下求导公式</p><p>$$\frac{\partial L^{<3>}}{\partial W}=\sum_{t=0}^3\frac{\partial L^{<3>}}{\partial \hat{y}^{<3>}}\cdot \frac{\partial \hat{y}^{<3>}}{\partial a^{<3>}}\cdot \frac{\partial a^{<3>}}{\partial a^{&lt; k &gt;}}\cdot \frac{\partial a^{&lt; k &gt;}}{\partial W}$$</3></3></3></3></3></3></p><p>其中，$\frac{\partial a^{<3>}}{\partial a^{<k>}}$又可以用链式法则展开求导，例如$\frac{\partial a^{<3>}}{\partial a^{<2>}}\cdot \frac{\partial a^{<2>}}{\partial a^{<1>}}$。注意到，因为我们正在考虑向量函数相对于向量的导数，所以其结果是一个矩阵（称为Jacobian matrix），其元素是所有的逐点导数，我们用链式法则将上述重写为：</1></2></2></3></k></3></p><p>$$\frac{\partial L^{<3>}}{\partial W}=\sum_{t=0}^3\frac{\partial L^{<3>}}{\partial \hat{y}^{<3>}}\cdot \frac{\partial \hat{y}^{<3>}}{\partial a^{<3>}}\cdot (\prod_{j=k+1}^3 \frac{\partial a^{&lt; j &gt;}}{\partial a^{&lt; j-1 &gt;}} )\cdot \frac{\partial a^{&lt; k &gt;}}{\partial W}$$</3></3></3></3></3></p><p>事实证明，上述雅可比矩阵的2范数，你可以认为它是一个绝对值，其上限为1。这是非常直观的，因为tanh（或S形）激活函数将所有值映射到-1和1之间的范围内，并且导数也限制在1（在sigmoid情况下为1/4）：<img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-21-064344.jpg" alt="tanh"></p><p>你可以看到，sigmoid函数在两端都有0的导数。他们接近一条平坦的路线。当发生这种情况时，我们说相应的神经元是饱和的。它们具有零梯度并且将前一层中的其他梯度驱动为0。因此，在矩阵和多个矩阵乘法（特别是）的值较小的情况下，梯度值以指数级快速收缩，最终在几个时间步后完全消失。“遥远”步骤的梯度贡献变为零，并且这些步骤的状态不会影响您正在学习的内容：您最终不会学习远程依赖关系。消失梯度不是RNN专有的。它们也发生在深度前馈神经网络中。只是RNN往往非常深刻（与我们的案例中的句子长度一样深），这使问题更加普遍。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. deeplearning.ai Sequence Model</p><p>[2]. <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="noopener">Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;序列模型是深度学习中最令人兴奋的领域之一。像递归神经网络或RNN这样的模型应用在语音识别，自然语言处理和其他领域。那么不能用传统的神经网络来解决自然语言领域的问题呢？主要有两个原因：1. 自然语言问题中不同实例数据的输入和输出长度往往都是不同的，例如在情感分类问题中，输入可
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="rnn" scheme="http://conghuai.me/tags/rnn/"/>
    
  </entry>
  
  <entry>
    <title>决策树 1-1：概述</title>
    <link href="http://conghuai.me/2018/05/17/%E5%86%B3%E7%AD%96%E6%A0%91%201-1%EF%BC%9A%E6%A6%82%E8%BF%B0/"/>
    <id>http://conghuai.me/2018/05/17/决策树 1-1：概述/</id>
    <published>2018-05-17T15:29:57.000Z</published>
    <updated>2018-09-24T15:24:53.604Z</updated>
    
    <content type="html"><![CDATA[<p>决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。</p><h1 id="决策树介绍"><a href="#决策树介绍" class="headerlink" title="决策树介绍"></a>决策树介绍</h1><p>通俗来说，决策树分类的思想类似于找对象，先想象一个女孩的母亲要给这个女孩介绍男朋友，于是有了下面的对话:</p><p>女儿： 多大年纪了</p><p>母亲：26</p><p>女儿：长得帅不帅</p><p>母亲：挺帅的</p><p>女儿：收入高不高</p><p>母亲：不算太高，中等情况。</p><p>女儿：是公务员不？</p><p>母亲：是，在税务局上班呢。</p><p>女儿：那好，我去见见。</p><p>这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员将男人分为两个类别：见或不见。我们可以将上面的决策过程抽象成一颗决策树:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-22-160930.jpg" alt="1_3"></p><p>上图完整表达了这个女孩决定是否见一个约会对象的策略， 其中绿色节点表示判断条件，橙色节点表示决策过程，箭头表示在一个判断条件在不同情况下的决策路径，途中红色表示了上面例子中女孩的决策过程。有了上面的直观的认识，我们可以正式定义决策树：</p><p>决策树是一个树结构。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。</p><p>不同于贝叶斯算法，决策树的构造过程不依赖领域知识，它使用属性选择度量来选择将元组最好的划分成不同的属性。所谓决策树的构造就是进行属性选择度量确定各个特征属性之间的拓扑结构。</p><p>构造决策树的关键步骤是分裂属性。所谓分类属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能的“纯”。也就是尽量让一个分裂子集中待分裂项属于同一类别。分裂属性分为三种不同情况:</p><ol><li>属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。</li><li>属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。</li><li>属性是连续值，此时确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支。</li></ol><p>构造决策树的关键性内容是进行属性选择度量，属性选择度量是一种选择分裂准则， 是将给定的类标记的训练集合的数据划分进行最好的划分成个体类的启发式方法，它决定了拓扑结构及分裂点split_point的选择。</p><p>属性选择度量算法有很多，一般使用自顶向下递归分治法，并采用不回溯的贪心策略。经典算法有ID3和C4.5。</p><h1 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h1><blockquote><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。</p></blockquote><p>假设，我们现在有如下的数据，该数据通过表格来展示:</p><table><thead><tr><th>日期</th><th>天气</th><th>湿度</th><th>风级</th><th>打球</th></tr></thead><tbody><tr><td>1</td><td>晴</td><td>高</td><td>弱</td><td>否</td></tr><tr><td>2</td><td>晴</td><td>高</td><td>强</td><td>否</td></tr><tr><td>3</td><td>阴</td><td>高</td><td>弱</td><td>是</td></tr><tr><td>4</td><td>雨</td><td>高</td><td>弱</td><td>是</td></tr><tr><td>5</td><td>雨</td><td>正常</td><td>弱</td><td>是</td></tr><tr><td>6</td><td>雨</td><td>正常</td><td>弱</td><td>是</td></tr><tr><td>7</td><td>阴</td><td>正常</td><td>强</td><td>是</td></tr><tr><td>8</td><td>晴</td><td>高</td><td>弱</td><td>否</td></tr><tr><td>9</td><td>晴</td><td>正常</td><td>弱</td><td>是</td></tr><tr><td>10</td><td>雨</td><td>正常</td><td>弱</td><td>是</td></tr><tr><td>11</td><td>晴</td><td>正常</td><td>强</td><td>是</td></tr><tr><td>12</td><td>阴</td><td>高</td><td>强</td><td>是</td></tr><tr><td>13</td><td>阴</td><td>正常</td><td>弱</td><td>是</td></tr><tr><td>14</td><td>雨</td><td>高</td><td>强</td><td>否</td></tr></tbody></table><p>决策树是采用分治思想来做的，把数据集按照各个属性进行分割，属性的不同取值可以得到不同的子数据集，做完分割后，判断数据点被完美区分开了没，如果区分开了，说明该次划分具有比较好的划分性。算法应该考虑用什么样的指标来判断这次划分的好坏。以及如何在多个属性中选择最好的划分属性。</p><h2 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h2><p>将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上的内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备。</p><h2 id="决策树与条件概率分布"><a href="#决策树与条件概率分布" class="headerlink" title="决策树与条件概率分布"></a>决策树与条件概率分布</h2><p>决策树还表示给定特征条件下类的概率分布，这一条件概率分布定义在特征空间的一个划分上，将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$，X取值于给定划分下单元的集合，Y取值于类的集合。各叶节点上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。</p><h2 id="决策树的学习"><a href="#决策树的学习" class="headerlink" title="决策树的学习"></a>决策树的学习</h2><p>决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这一个过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根节点，选择一个最优特征，按照这一特征训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到对应的叶结点中取；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点，如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。</p><h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。给出例子为如下</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-22-160935.jpg" alt="Screen Shot 2018-05-19 at 15.33.07"></p><h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>在信息论与概率统计中，熵是表示随机变量不确定性的度量，设$X$是一个取有限个值的离散随机变量，其概率分布为：</p><p>$$P(X = x_i)=p_i,i=1,2,…,n$$</p><p>则随机变量$X$的熵定义为</p><p>$$H(X) = - \sum_{i=1}^np_ilogp_i$$</p><p>熵越大，随机变量的不确定性就越大，从定义可验证</p><p>$$0\leq H(p)\leq logn$$</p><h2 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h2><p>设随机变量$(X,Y)$，其联合概率分布为</p><p>$$P(X=x_i,Y=y_j)=p_{ij}\text{ , i=1,2,…,n; j=1,2,…,m}$$</p><p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为$X$给定条件下$Y$的条件概率分布的上对$X$的数学期望</p><p>$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$$</p><p>这里，$p_i=P(X=x_i)\text{ i=1,2,..,n}$</p><h2 id="信息增益值"><a href="#信息增益值" class="headerlink" title="信息增益值"></a>信息增益值</h2><p>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即</p><p>$$g(D,A)=H(D)-H(D|A)$$</p><p>一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类的特征的互信息。</p><p>计算步骤：</p><ol><li><p>计算数据集D的经验熵$H(D)$</p><p>$$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}log_2\frac{|C_k|}{D}$$</p><p>套用上面的公式得：$H(D)=-\frac{9}{15}log_2\frac{9}{15}-\frac{6}{15}log_2\frac{6}{15}=0.971$</p></li><li><p>计算特征A对数据集D的经验条件熵$H(D|A)$</p><p>$$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}$$</p><p>特征有$A_1,A_2,A_3,A_4$分别表示年龄、有工作、有自己的房子和信贷情况，则</p><p>$$\begin{align} g(D,A_1) &amp;= H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)] \\  &amp;= 0.971-[\frac{5}{15}(-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5})+\frac{5}{15}(-\frac{3}{5}log_2\frac{3}{5}-\frac{2}{5}log_2\frac{2}{5})+\frac{5}{15}(-\frac{4}{5}log_2\frac{4}{5}-\frac{1}{5}log_2\frac{1}{5})] \\&amp;= 0.971-0.888 \\&amp;=0.083   \\ \end{align}$$</p><p>$$\begin{align} g(D,A_2) &amp;= H(D)-[\frac{5}{15}H(D_1)+\frac{10}{15}H(D_2)] \\  &amp;= 0.971-[\frac{5}{15}*0+\frac{10}{15}(-\frac{4}{10}log_2\frac{4}{10}-\frac{6}{10}log_2\frac{6}{10})]  \\&amp;=0.324  \\ \end{align}$$</p><p>$$g(D,A_3) =0.420$$</p><p>$$g(D,A_4) =0.363$$</p></li><li><p>最后，比较个特征的信息增益值，选择信息增益值最大的特征作为最优特征。</p></li></ol><h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><p>对于取值非常多的特征，其划分后，得到的信息增益值比较大，换句话说，信息增益这一个度量指标偏向于多值特征。采用信息增益比可以对这一问题进行校正：</p><p>$$g_R(D,A)=\frac{g(D,A)}{H(D)}$$</p><h2 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h2><p>假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为</p><p>$$Gini(D)=1-\sum_{k=1}^{|y|}p_k^2$$</p><p>在特征a的条件下，集合D的基尼指数定义为</p><p>$$Gini_index(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)$$</p><h2 id="均方差（回归）"><a href="#均方差（回归）" class="headerlink" title="均方差（回归）"></a>均方差（回归）</h2><h1 id="终止条件"><a href="#终止条件" class="headerlink" title="终止条件"></a>终止条件</h1><p>递归的选择属性分裂子树，那么什么时候递归终止呢？其终止条件如下：</p><ol><li># of leaves.</li><li>tree depth.</li><li># of instances in current node.</li></ol><h1 id="常见算法"><a href="#常见算法" class="headerlink" title="常见算法"></a>常见算法</h1><p>经典的决策树算法诞生于以下经典论文</p><ol><li>J. Quinlan, 1986, ID3（Classification）</li><li>J. Quinlan, 1993, C4.5（Classification）</li><li>L. Breiman and J. Friedman, 1984, CART（Classification &amp; Regression）</li></ol><h2 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h2><ul><li>以信息增益度量属性选择，选择分裂后信息增益最大的属性进行划分；</li><li>属性只能取离散值(为了使决策树能应用于连续属性值情况，可以使用ID3的一个扩展算法C4.5算法)；</li><li>偏向多值属性。</li></ul><h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>这次我们每次进行选取特征属性的时候，不再使用ID3算法的信息增益，而是使用了信息增益率这个概念。</p><ul><li>使用信息增益率；</li><li>偏向少值属性；</li><li>不直接选择增益率最大的候选划分属性，候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。</li></ul><h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>分类与回归树（classification and regression tree, CART）模型既可以用于分类也可以用于回归。</p><ul><li>假设决策树是二叉树；</li><li>对回归树用平方误差最小化准则；对分类树用基尼指数最小化准则，进行特征选择，生成二叉树；</li></ul><h3 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h3><blockquote><p>一个回归树对应着输入空间的一个划分以及在划分的单元上的输出值，假设已将输入空间划分为M个单元$R_1,R_2,…,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为：</p><p>$$f(x)=\sum_{m=1}^Mc_mI(x\in R_m)$$</p><p>当输入空间的划分确定时，可以用平方误差最小的准则求解每个单元上的最优输出值。易知，单元$R_m$上的$c_m$的最优值$\hat{c_m}$是$R_m$上的所有输入实例$x_i$对应的输出$y_i$的均值，即</p><p>$$\hat{c_m}=ave(y_i|x_i\in R_m)$$</p></blockquote><p>问题是怎样对输入空间进行划分呢？</p><p>这里采用启发式的方法，选择第j个变量$x^{(j)}$和它取的值$s$，作为切分变量和切分点，并定义两个区域：</p><p>$$R_1(j,s)=\{x|x^{(j)}\leq s\}\text{ 和 } R_2(j,s)=\{x|x^{(j)}&gt;s\}$$</p><p>然后寻找最优切分变量$j$和最优切分点$s$，具体的，求解：</p><p>$$min_{j,s}[min_{c_1}\sum_{x_1\in R_1(j,s)}(y_i-c_1)^2+min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$</p><p>对固定输入变量$j$可以找到最优切分点$s$：</p><p>$$\hat{c_1}=ave(y_i|x_i\in R(j,s))\text{ 和 } \hat{c_2}=ave(y_i|x_i\in R_2(j,s))$$</p><p>遍历所有的输入变量，找到最优的切分变量$j$，构成一个对(j,s)，依次将输入空间划分为两个区域。<img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-22-160936.jpg" alt="回归树"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-22-160934.jpg" alt="生成"></p><h3 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h3><p>用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-22-160938.jpg" alt="分类树"></p><h2 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h2><p>决策树为什么要剪枝？原因是避免决策树过拟合样本。前面的算法生产的决策树非常详细并且庞大，每个属性都被详细地加以考虑，决策树的树叶节点所覆盖的训练样本都是纯的，因此用这个决策树来对训练样本进行分类的话，你会发现对于训练样本而言，这个树表现完好，误差率极地且能够正确的对训练样本集中的样本进行分类。但是，对于测试数据的表现就没有想象的那么好，或者极差，这就是所谓的过拟合问题。</p><p>现在的问题在于，如何在原生的过拟合决策树的基础上，生成简化版的决策树？可以通过剪枝的方法来简化过拟合的决策树。</p><p>剪枝可以分为两种：预剪枝和后剪枝。</p><ul><li><p>预剪枝</p><p>及早的停止树增长。</p></li><li><p>后剪枝</p><p>在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。</p></li></ul><p>剪枝的准则是如何确定决策树的规模，可以参考的剪枝思路有以下几个:</p><ol><li>使用训练集合和验证集合，来评估剪枝方法在修剪节点上的效用；</li><li>使用所有的训练集合进行训练，但是用统计测试来估计修剪特定节点是否会改善训练集合外的数据的评估性能，如使用Chi-Square测试来进一步扩展节点是否能改善整个分类数据的性能，还是仅仅改善了当前训练集合数据上的性能。</li></ol><p>后剪枝的算法有很多，这里列举出常用的三种算法:</p><h2 id="Reduced-Error-Pruning-REP-错误率降低剪枝"><a href="#Reduced-Error-Pruning-REP-错误率降低剪枝" class="headerlink" title="Reduced-Error Pruning(REP, 错误率降低剪枝)"></a>Reduced-Error Pruning(REP, 错误率降低剪枝)</h2><p>该剪枝方法考虑将树上的每个节点作为修剪的候选对象，决定是否修剪这个节点有如下步骤组成：</p><ul><li>删除以该节点为根的子树；</li><li>使其成为叶子结点；</li><li>赋予该结点关联的训练数据最常见分类；</li><li>当修剪后的树对于验证集合的性能不会比原来的树差时，才真正删除该结点。</li><li>因为训练集合的过拟合，使得验证集合数据能够对其进行修正，反复进行上面的操作，从底向上的处理结点，删除那些能够最大限度的提高验证集合的精度的节点，直到进一步修剪有害为止。</li></ul><p>REP是最简单的后剪枝方法之一，不过在数据量比较少的情况下，REP方法趋于过拟合而较少使用。这是因为训练数据集合中的特性在剪枝过程中被忽略，所以在验证数据集合比训练数据集合小的多时，要注意这个问题。尽管REP有这个缺点，不过REP仍然作为一种基准来评价其他剪枝算法的性能。它对于两阶段决策树学习方法的优点和缺点提供了一种很好的学习思路。由于验证集合没有参与决策树的创建，所以用REP剪枝后的决策树对于测试样例的偏差要好很多，能够解决一定程度的过拟合问题。</p><h2 id="Pessimistic-Error-Pruning-PEP-悲观剪枝"><a href="#Pessimistic-Error-Pruning-PEP-悲观剪枝" class="headerlink" title="Pessimistic Error Pruning(PEP, 悲观剪枝)"></a>Pessimistic Error Pruning(PEP, 悲观剪枝)</h2><p>PEP剪枝算法是在C4.5决策树算法中提出的，把一颗子树(具有多个叶子节点)用一个叶子节点来替代的话，比起REP剪枝法，它不需要一个单独的测试数据集。对于这一部分，仍然需要更进一步的学习，后续如果有时间在进行相关的研究。</p><h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>前面对决策树的概念，分类进行了介绍，并扩展了集成学习的一些思想和算法。这里我们来学习一下sklearn这个机器学习类库中的关于决策树的一些说明和工具。后面会用一些例子在实际项目中使用决策树这个算法，以及算法融合的思想。之前介绍的决策树算法有ID3，C4.5和CART，sklearn中采用的是最佳的CART算法。</p><h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><p>优点：</p><ul><li>需要较少的数据预处理；</li><li>支持类别特征；</li><li>容易理解和实现。</li></ul><p>缺点：</p><ul><li>容易过拟合；</li><li>模型拟合能力不强。</li></ul><h1 id="实战sklearn"><a href="#实战sklearn" class="headerlink" title="实战sklearn"></a>实战sklearn</h1><p>决策树是一个非参数的监督式学习方法，主要用于分类和回归。算法的目标是通过推断数据特征，学习决策规则从而创建一个预测目标变量的模型。如下所示，决策树通过一系列if-then-else决策规则，近似估计一个正弦曲线。</p><p><img src="http://scikit-learn.org/stable/_images/sphx_glr_plot_tree_regression_0011.png" alt=""></p><p>决策树的优势:</p><ul><li>简单易懂，原理清晰，决策树可以实现可视化。</li><li>数据准备简单。其他的方法需要实现数据归一化，创建虚拟变量，删除空白变量。(注意：这个模块不支持缺失值)</li><li>使用决策树的代价是数据点的对数级别。</li><li>能够处理数值和分类数据</li><li>能够处理多路输出问题</li><li>使用白盒子模型(内部结构可以直接观测的模型)。一个给定的情况是可以观测的，那么就可以用布尔逻辑解释这个结果。相反，如果在一个黑盒模型(ANN)，结果可能很难解释</li><li>可以通过统计学检验验证模型。这也使得模型的可靠性计算变得可能</li><li>即使模型假设违反产生数据的真实模型，表现性能依旧很好。</li></ul><p>决策树劣势:</p><ul><li>可能会建立过于复杂的规则，即过拟合。为避免这个问题，剪枝、设置叶节点的最小样本数量、设置决策树的最大深度有时候是必要的。</li><li>决策树有时候是不稳定的，因为数据微小的变动，可能生成完全不同的决策树。 可以通过总体平均(ensemble)减缓这个问题。应该指的是多次实验。</li><li>学习最优决策树是一个NP完全问题。所以，实际决策树学习算法是基于试探性算法，例如在每个节点实现局部最优值的贪心算法。这样的算法是无法保证返回一个全局最优的决策树。可以通过随机选择特征和样本训练多个决策树来缓解这个问题。</li><li>有些问题学习起来非常难，因为决策树很难表达。如：异或问题、奇偶校验或多路复用器问题</li><li>如果有些因素占据支配地位，决策树是有偏的。因此建议在拟合决策树之前先平衡数据的影响因子。</li></ul><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>我们先介绍一下DecisionTreeClassifier这个类，它用于多分类问题。</p><h3 id="类签名"><a href="#类签名" class="headerlink" title="类签名"></a>类签名</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">tree</span>.<span class="title">DecisionTreeClassifier</span><span class="params">(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=None, class_weight=None, presort=False)</span></span></span><br></pre></td></tr></table></figure><h3 id="构造器参数说明"><a href="#构造器参数说明" class="headerlink" title="构造器参数说明"></a>构造器参数说明</h3><ul><li><p><code>criterion</code>: string, optional(default=’gini’)</p><p>度量划分属性质量:”gini”指基尼不纯度;”entropy”指信息增益。</p></li><li><p><code>splitter</code>: string, optional(default=”best”)</p><p>每个节点选择划分属性的策略: “best”选择最好的划分，”random”选择最好的随机划分</p></li><li><p><code>max_depth</code> : int or None, optional(default=None)</p><p>树的最大深度</p></li><li><p><code>min_samples_split</code> : int, float, optional(default=2)</p><p>划分一个内部节点所需的最少样本数。</p></li><li><p><code>min_samples_leaf</code> ：int, float, optional(default=1)</p><p>划分一个叶子节点所需的最少样本数。</p></li><li><p><code>min_weight_fraction_leaf</code> : float, optional(default=0)</p><p>限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。</p></li><li><p><code>max_features</code>: int, float, string or None, optional(default=None)</p><p>选择划分时所用的最大特征数目</p></li><li><p><code>random_state</code>: int, RandomState instances o None, optional(default=None)</p><p>随机种子</p></li><li><p><code>max_leaf_nodes</code>: int or None, optional(default=None)</p><p>最大的叶子数量。</p></li><li><p><code>min_impurity_decrease</code> : float, optional(default=0)</p><p>如果划分后的纯度减少量大于这个值，那么这个节点就会被划分。</p></li><li><p><code>min_impurity_split</code>: float</p><p>阈值，用于控制节点是否进一步划分，如果大于这个值则需要进一步划分，否则就停止划分。已经deprecated掉了，用上面那个参数。</p></li><li><p><code>class_weight</code> :dict, list of dicts, “balanced” or None, default=None</p><p>与预测类别相关的权重dict, 以{class_label: weight}这种方式传入，当然了，如果有多个类别输出，那就传入这种dict的list。如果传入的是balance，那么将用根据样本数来决定类别的权重。</p><p>n_samples / (n_classes * np.bincount(y))</p></li><li><p><code>presort</code>: bool, optional(default=None)</p><p>是否预划分。如果预划分，那么在小数据集上可能加快速度，在大数据集上可能会变慢。</p></li></ul><h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><ul><li><code>classes_</code><ul><li>类标</li></ul></li><li><code>feature_importances_</code><ul><li>特征的重要性。越高，特征越重要。</li></ul></li><li><code>max_featrues_</code><ul><li>推断的价值  不知道是啥</li></ul></li><li><code>n_classes_</code><ul><li>类的数量</li></ul></li><li><code>n_features_</code><ul><li>特征的数量</li></ul></li><li><code>n_outputs_</code><ul><li>输出的数量</li></ul></li><li><code>tree_</code><ul><li>底层树对象</li></ul></li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul><li><code>apply(X[, check_input])</code><ul><li>返回样本被预测为叶子的索引</li></ul></li><li><code>decision_path(X[, check_input])</code><ul><li>返回样本的决策路径</li></ul></li><li><code>fit(X, y[, sample_weight, check_input, …])</code><ul><li>从(X,y)训练数据中创建一颗决策树</li></ul></li><li><code>get_params([deep])</code><ul><li>获得参数</li></ul></li><li><code>predict(X[, check_input])</code><ul><li>预测样本类别或连续值</li></ul></li><li><code>predict_log_proba(X)</code><ul><li>预测输入数据的log概率</li></ul></li><li><code>predict_proba(X,[, check_input])</code><ul><li>预测输入数据的概率</li></ul></li><li>score(X,y[, sample_weight])<ul><li>返回平均正确率</li></ul></li><li>set_params(**params)<ul><li>设置参数</li></ul></li></ul><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><h3 id="类签名-1"><a href="#类签名-1" class="headerlink" title="类签名"></a>类签名</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">tree</span>.<span class="title">DecisionTreeRegressor</span><span class="params">(criterion=’mse’, splitter=’best’, max_depth=None, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=None, presort=False)</span>[<span class="title">source</span>]</span></span><br></pre></td></tr></table></figure><p>这些参数，属性和方法与上面介绍的类似。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” <em>清华大学出版社, 北京</em> (2012).</p><p>[2]. <a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="noopener">算法杂货铺——分类算法之决策树(Decision tree)</a></p><p>[3]. <a href="https://zhuanlan.zhihu.com/p/26760551" target="_blank" rel="noopener">知乎专栏</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。决策树学习通
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="tree" scheme="http://conghuai.me/tags/tree/"/>
    
  </entry>
  
  <entry>
    <title>Classic Convolutional Network</title>
    <link href="http://conghuai.me/2018/05/16/Classic-Convolutional-Network/"/>
    <id>http://conghuai.me/2018/05/16/Classic-Convolutional-Network/</id>
    <published>2018-05-16T02:01:32.000Z</published>
    <updated>2018-09-24T15:23:34.195Z</updated>
    
    <content type="html"><![CDATA[<p>卷积神经网络中有很多经典的网络模型，通过学习这些网络模型对于我们有很大的启发作用，经典的模型如下：</p><ul><li>LeNet - 5</li><li>AlexNet</li></ul><h1 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h1><p>卷积神经网络是一种特殊的多层神经网络。像几乎所有其他的神经网络一样，它们都使用反向传播算法的版本进行训练。卷积神经网络被设计为以最小的预处理直接从像素图像识别视觉图案。他们可以识别极端变化的模式（如手写字符），以及对失真和简单几何变换的鲁棒性。LeNet-5是最早的的用于手写和机器打印字符识别的卷积网络。这是一个LeNet-5的例子。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-16-084213.jpg" alt="asamples"></p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>LeNet-5网络模型结构如下：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-16-084211.jpg" alt="Screen Shot 2018-05-16 at 10.20.57"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-16-084212.jpg" alt="The-LeNet-5-Architecture-a-convolutional-neural-network"></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LeNet</span><span class="params">(x)</span>:</span>    </span><br><span class="line">    <span class="comment"># Hyperparameters</span></span><br><span class="line">    mu = <span class="number">0</span></span><br><span class="line">    sigma = <span class="number">0.1</span></span><br><span class="line">    layer_depth = &#123;</span><br><span class="line">        <span class="string">'layer_1'</span> : <span class="number">6</span>,</span><br><span class="line">        <span class="string">'layer_2'</span> : <span class="number">16</span>,</span><br><span class="line">        <span class="string">'layer_3'</span> : <span class="number">120</span>,</span><br><span class="line">        <span class="string">'layer_f1'</span> : <span class="number">84</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.</span></span><br><span class="line">    conv1_w = tf.Variable(tf.truncated_normal(shape = [<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">6</span>],mean = mu, stddev = sigma))</span><br><span class="line">    conv1_b = tf.Variable(tf.zeros(<span class="number">6</span>))</span><br><span class="line">    conv1 = tf.nn.conv2d(x,conv1_w, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'VALID'</span>) + conv1_b </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Activation.</span></span><br><span class="line">    conv1 = tf.nn.relu(conv1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Pooling. Input = 28x28x6. Output = 14x14x6.</span></span><br><span class="line">    pool_1 = tf.nn.max_pool(conv1,ksize = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding = <span class="string">'VALID'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Layer 2: Convolutional. Output = 10x10x16.</span></span><br><span class="line">    conv2_w = tf.Variable(tf.truncated_normal(shape = [<span class="number">5</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">16</span>], mean = mu, stddev = sigma))</span><br><span class="line">    conv2_b = tf.Variable(tf.zeros(<span class="number">16</span>))</span><br><span class="line">    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'VALID'</span>) + conv2_b</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Activation.</span></span><br><span class="line">    conv2 = tf.nn.relu(conv2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Pooling. Input = 10x10x16. Output = 5x5x16.</span></span><br><span class="line">    pool_2 = tf.nn.max_pool(conv2, ksize = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding = <span class="string">'VALID'</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Flatten. Input = 5x5x16. Output = 400.</span></span><br><span class="line">    fc1 = flatten(pool_2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Layer 3: Fully Connected. Input = 400. Output = 120.</span></span><br><span class="line">    fc1_w = tf.Variable(tf.truncated_normal(shape = (<span class="number">400</span>,<span class="number">120</span>), mean = mu, stddev = sigma))</span><br><span class="line">    fc1_b = tf.Variable(tf.zeros(<span class="number">120</span>))</span><br><span class="line">    fc1 = tf.matmul(fc1,fc1_w) + fc1_b</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Activation.</span></span><br><span class="line">    fc1 = tf.nn.relu(fc1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Layer 4: Fully Connected. Input = 120. Output = 84.</span></span><br><span class="line">    fc2_w = tf.Variable(tf.truncated_normal(shape = (<span class="number">120</span>,<span class="number">84</span>), mean = mu, stddev = sigma))</span><br><span class="line">    fc2_b = tf.Variable(tf.zeros(<span class="number">84</span>))</span><br><span class="line">    fc2 = tf.matmul(fc1,fc2_w) + fc2_b</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Activation.</span></span><br><span class="line">    fc2 = tf.nn.relu(fc2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Layer 5: Fully Connected. Input = 84. Output = 10.</span></span><br><span class="line">    fc3_w = tf.Variable(tf.truncated_normal(shape = (<span class="number">84</span>,<span class="number">10</span>), mean = mu , stddev = sigma))</span><br><span class="line">    fc3_b = tf.Variable(tf.zeros(<span class="number">10</span>))</span><br><span class="line">    logits = tf.matmul(fc2, fc3_w) + fc3_b</span><br><span class="line">    <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>AlexNet以大幅度赢得2012年ImageNet LSVRC-2012大赛（15.3％VS 26.2％（第二名）错误率）而闻名。</p><p>AlexNet网络有如下的几个特点：</p><ol><li>使用<code>ReLU</code>而不是<code>Tanh</code>激活函数，在相同精度下，训练速度提高了6倍；</li><li>使用<code>dropout</code>而不是<code>regularization</code>来防止过拟合。但是，这使得是训练时间增加了1倍，dropout率为0.5；</li><li>将卷积层复制到不同的GPU中，将完全连接的图层分配到不同的GPU中；</li><li>将数据分批、平行的送入到每个GPU的卷积层；</li></ol><h2 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-16-084214.jpg" alt="Screen Shot 2018-05-16 at 15.54.29"></p><h2 id="实战-1"><a href="#实战-1" class="headerlink" title="实战"></a>实战</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Implementation of the AlexNet."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, keep_prob, num_classes, skip_layer,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weights_path=<span class="string">'DEFAULT'</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Create the graph of the AlexNet model.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: Placeholder for the input tensor.</span></span><br><span class="line"><span class="string">            keep_prob: Dropout probability.</span></span><br><span class="line"><span class="string">            num_classes: Number of classes in the dataset.</span></span><br><span class="line"><span class="string">            skip_layer: List of names of the layer, that get trained from</span></span><br><span class="line"><span class="string">                scratch</span></span><br><span class="line"><span class="string">            weights_path: Complete path to the pretrained weight file, if it</span></span><br><span class="line"><span class="string">                isn't in the same folder as this code</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># Parse input arguments into class variables</span></span><br><span class="line">        self.X = x</span><br><span class="line">        self.NUM_CLASSES = num_classes</span><br><span class="line">        self.KEEP_PROB = keep_prob</span><br><span class="line">        self.SKIP_LAYER = skip_layer</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> weights_path == <span class="string">'DEFAULT'</span>:</span><br><span class="line">            self.WEIGHTS_PATH = <span class="string">'bvlc_alexnet.npy'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.WEIGHTS_PATH = weights_path</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Call the create function to build the computational graph of AlexNet</span></span><br><span class="line">        self.create()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Create the network graph."""</span></span><br><span class="line">        <span class="comment"># 1st Layer: Conv (w ReLu) -&gt; Lrn -&gt; Pool</span></span><br><span class="line">        conv1 = conv(self.X, <span class="number">11</span>, <span class="number">11</span>, <span class="number">96</span>, <span class="number">4</span>, <span class="number">4</span>, padding=<span class="string">'VALID'</span>, name=<span class="string">'conv1'</span>)</span><br><span class="line">        norm1 = lrn(conv1, <span class="number">2</span>, <span class="number">1e-04</span>, <span class="number">0.75</span>, name=<span class="string">'norm1'</span>)</span><br><span class="line">        pool1 = max_pool(norm1, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, padding=<span class="string">'VALID'</span>, name=<span class="string">'pool1'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2nd Layer: Conv (w ReLu)  -&gt; Lrn -&gt; Pool with 2 groups</span></span><br><span class="line">        conv2 = conv(pool1, <span class="number">5</span>, <span class="number">5</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>, groups=<span class="number">2</span>, name=<span class="string">'conv2'</span>)</span><br><span class="line">        norm2 = lrn(conv2, <span class="number">2</span>, <span class="number">1e-04</span>, <span class="number">0.75</span>, name=<span class="string">'norm2'</span>)</span><br><span class="line">        pool2 = max_pool(norm2, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, padding=<span class="string">'VALID'</span>, name=<span class="string">'pool2'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3rd Layer: Conv (w ReLu)</span></span><br><span class="line">        conv3 = conv(pool2, <span class="number">3</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">1</span>, <span class="number">1</span>, name=<span class="string">'conv3'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4th Layer: Conv (w ReLu) splitted into two groups</span></span><br><span class="line">        conv4 = conv(conv3, <span class="number">3</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">1</span>, <span class="number">1</span>, groups=<span class="number">2</span>, name=<span class="string">'conv4'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5th Layer: Conv (w ReLu) -&gt; Pool splitted into two groups</span></span><br><span class="line">        conv5 = conv(conv4, <span class="number">3</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>, groups=<span class="number">2</span>, name=<span class="string">'conv5'</span>)</span><br><span class="line">        pool5 = max_pool(conv5, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, padding=<span class="string">'VALID'</span>, name=<span class="string">'pool5'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 6th Layer: Flatten -&gt; FC (w ReLu) -&gt; Dropout</span></span><br><span class="line">        flattened = tf.reshape(pool5, [<span class="number">-1</span>, <span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>])</span><br><span class="line">        fc6 = fc(flattened, <span class="number">6</span>*<span class="number">6</span>*<span class="number">256</span>, <span class="number">4096</span>, name=<span class="string">'fc6'</span>)</span><br><span class="line">        dropout6 = dropout(fc6, self.KEEP_PROB)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 7th Layer: FC (w ReLu) -&gt; Dropout</span></span><br><span class="line">        fc7 = fc(dropout6, <span class="number">4096</span>, <span class="number">4096</span>, name=<span class="string">'fc7'</span>)</span><br><span class="line">        dropout7 = dropout(fc7, self.KEEP_PROB)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 8th Layer: FC and return unscaled activations</span></span><br><span class="line">        self.fc8 = fc(dropout7, <span class="number">4096</span>, self.NUM_CLASSES, relu=<span class="keyword">False</span>, name=<span class="string">'fc8'</span>)</span><br></pre></td></tr></table></figure><h1 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h1><h2 id="网络架构-1"><a href="#网络架构-1" class="headerlink" title="网络架构"></a>网络架构</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032553.jpg" alt="Screen Shot 2018-05-17 at 10.18.21"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032554.jpg" alt="uLXrKxe"></p><h2 id="实战-2"><a href="#实战-2" class="headerlink" title="实战"></a>实战</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vgg16</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vgg16_npy_path=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> vgg16_npy_path <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            path = inspect.getfile(Vgg16)</span><br><span class="line">            path = os.path.abspath(os.path.join(path, os.pardir))</span><br><span class="line">            path = os.path.join(path, <span class="string">"vgg16.npy"</span>)</span><br><span class="line">            vgg16_npy_path = path</span><br><span class="line">            print(path)</span><br><span class="line"></span><br><span class="line">        self.data_dict = np.load(vgg16_npy_path, encoding=<span class="string">'latin1'</span>).item()</span><br><span class="line">        print(<span class="string">"npy file loaded"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, rgb)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        load variable from npy to build the VGG</span></span><br><span class="line"><span class="string">        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        print(<span class="string">"build model started"</span>)</span><br><span class="line">        rgb_scaled = rgb * <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert RGB to BGR</span></span><br><span class="line">        red, green, blue = tf.split(axis=<span class="number">3</span>, num_or_size_splits=<span class="number">3</span>, value=rgb_scaled)</span><br><span class="line">        <span class="keyword">assert</span> red.get_shape().as_list()[<span class="number">1</span>:] == [<span class="number">224</span>, <span class="number">224</span>, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">assert</span> green.get_shape().as_list()[<span class="number">1</span>:] == [<span class="number">224</span>, <span class="number">224</span>, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">assert</span> blue.get_shape().as_list()[<span class="number">1</span>:] == [<span class="number">224</span>, <span class="number">224</span>, <span class="number">1</span>]</span><br><span class="line">        bgr = tf.concat(axis=<span class="number">3</span>, values=[</span><br><span class="line">            blue - VGG_MEAN[<span class="number">0</span>],</span><br><span class="line">            green - VGG_MEAN[<span class="number">1</span>],</span><br><span class="line">            red - VGG_MEAN[<span class="number">2</span>],</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">assert</span> bgr.get_shape().as_list()[<span class="number">1</span>:] == [<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        self.conv1_1 = self.conv_layer(bgr, <span class="string">"conv1_1"</span>)</span><br><span class="line">        self.conv1_2 = self.conv_layer(self.conv1_1, <span class="string">"conv1_2"</span>)</span><br><span class="line">        self.pool1 = self.max_pool(self.conv1_2, <span class="string">'pool1'</span>)</span><br><span class="line"></span><br><span class="line">        self.conv2_1 = self.conv_layer(self.pool1, <span class="string">"conv2_1"</span>)</span><br><span class="line">        self.conv2_2 = self.conv_layer(self.conv2_1, <span class="string">"conv2_2"</span>)</span><br><span class="line">        self.pool2 = self.max_pool(self.conv2_2, <span class="string">'pool2'</span>)</span><br><span class="line"></span><br><span class="line">        self.conv3_1 = self.conv_layer(self.pool2, <span class="string">"conv3_1"</span>)</span><br><span class="line">        self.conv3_2 = self.conv_layer(self.conv3_1, <span class="string">"conv3_2"</span>)</span><br><span class="line">        self.conv3_3 = self.conv_layer(self.conv3_2, <span class="string">"conv3_3"</span>)</span><br><span class="line">        self.pool3 = self.max_pool(self.conv3_3, <span class="string">'pool3'</span>)</span><br><span class="line"></span><br><span class="line">        self.conv4_1 = self.conv_layer(self.pool3, <span class="string">"conv4_1"</span>)</span><br><span class="line">        self.conv4_2 = self.conv_layer(self.conv4_1, <span class="string">"conv4_2"</span>)</span><br><span class="line">        self.conv4_3 = self.conv_layer(self.conv4_2, <span class="string">"conv4_3"</span>)</span><br><span class="line">        self.pool4 = self.max_pool(self.conv4_3, <span class="string">'pool4'</span>)</span><br><span class="line"></span><br><span class="line">        self.conv5_1 = self.conv_layer(self.pool4, <span class="string">"conv5_1"</span>)</span><br><span class="line">        self.conv5_2 = self.conv_layer(self.conv5_1, <span class="string">"conv5_2"</span>)</span><br><span class="line">        self.conv5_3 = self.conv_layer(self.conv5_2, <span class="string">"conv5_3"</span>)</span><br><span class="line">        self.pool5 = self.max_pool(self.conv5_3, <span class="string">'pool5'</span>)</span><br><span class="line"></span><br><span class="line">        self.fc6 = self.fc_layer(self.pool5, <span class="string">"fc6"</span>)</span><br><span class="line">        <span class="keyword">assert</span> self.fc6.get_shape().as_list()[<span class="number">1</span>:] == [<span class="number">4096</span>]</span><br><span class="line">        self.relu6 = tf.nn.relu(self.fc6)</span><br><span class="line"></span><br><span class="line">        self.fc7 = self.fc_layer(self.relu6, <span class="string">"fc7"</span>)</span><br><span class="line">        self.relu7 = tf.nn.relu(self.fc7)</span><br><span class="line"></span><br><span class="line">        self.fc8 = self.fc_layer(self.relu7, <span class="string">"fc8"</span>)</span><br><span class="line"></span><br><span class="line">        self.prob = tf.nn.softmax(self.fc8, name=<span class="string">"prob"</span>)</span><br><span class="line"></span><br><span class="line">        self.data_dict = <span class="keyword">None</span></span><br><span class="line">        print((<span class="string">"build model finished: %ds"</span> % (time.time() - start_time)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">avg_pool</span><span class="params">(self, bottom, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.avg_pool(bottom, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_pool</span><span class="params">(self, bottom, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.max_pool(bottom, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">conv_layer</span><span class="params">(self, bottom, name)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line">            filt = self.get_conv_filter(name)</span><br><span class="line"></span><br><span class="line">            conv = tf.nn.conv2d(bottom, filt, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">            conv_biases = self.get_bias(name)</span><br><span class="line">            bias = tf.nn.bias_add(conv, conv_biases)</span><br><span class="line"></span><br><span class="line">            relu = tf.nn.relu(bias)</span><br><span class="line">            <span class="keyword">return</span> relu</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fc_layer</span><span class="params">(self, bottom, name)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line">            shape = bottom.get_shape().as_list()</span><br><span class="line">            dim = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> d <span class="keyword">in</span> shape[<span class="number">1</span>:]:</span><br><span class="line">                dim *= d</span><br><span class="line">            x = tf.reshape(bottom, [<span class="number">-1</span>, dim])</span><br><span class="line"></span><br><span class="line">            weights = self.get_fc_weight(name)</span><br><span class="line">            biases = self.get_bias(name)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Fully connected layer. Note that the '+' operation automatically</span></span><br><span class="line">            <span class="comment"># broadcasts the biases.</span></span><br><span class="line">            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> fc</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_conv_filter</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(self.data_dict[name][<span class="number">0</span>], name=<span class="string">"filter"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(self.data_dict[name][<span class="number">1</span>], name=<span class="string">"biases"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_fc_weight</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tf.constant(self.data_dict[name][<span class="number">0</span>], name=<span class="string">"weights"</span>)</span><br></pre></td></tr></table></figure><h1 id="ResNets"><a href="#ResNets" class="headerlink" title="ResNets"></a>ResNets</h1><p>在LSVRC2012分级竞赛中AlexNet的胜利之后，深度残余网络可以说是过去几年计算机视觉/深度学习领域最具开创性的工作。ResNet可以训练数百甚至数千层，并且仍然可以获得令人信服的性能。利用ResNets强大的表现能力，除了图像分类以外，许多计算机视觉应用的性能得到了提升，如对象检测和人脸识别。</p><h2 id="分析ResNets"><a href="#分析ResNets" class="headerlink" title="分析ResNets"></a>分析ResNets</h2><p>根据universal approximation theorem，给定足够的容量，我们知道具有单层的前馈网络足以表示任何函数。但是，该层可能很大，网络容易过度配合数据。因此，研究界有一个共同的趋势，即我们的网络架构需要更深入。</p><p>但是，增加网络深度不能通过将层叠在一起来实现。深度网络难以训练，因为臭名昭着的渐变梯度问题 - 随着梯度向后传播到较早的层次，重复的乘法可能会使梯度变得无限小。因此，随着网络进一步深入，其性能会变得饱和，甚至开始迅速退化。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032547.jpg" alt="1_McwAbGJjA1lV_xBdg1w5XA"></p><p>ResNet的核心思想是引入一个跳过一个或多个层的所谓“identity shortcut connection”，如下图所示：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032559.jpg" alt="Screen Shot 2018-05-17 at 10.48.01"></p><p>作者认为，堆叠层不应该降低网络性能，因为我们可以简单地在当前网络上堆叠identity mappings（不做任何事的层），并且所得到的架构将执行相同的操作。这表明较深的模型不应产生比较浅的模型更高的训练误差。他们假设让堆叠图层适合残差映射比让它们直接适合所需的下层映射容易。上面的残余块明确允许它做到这一点。</p><h2 id="网络架构-2"><a href="#网络架构-2" class="headerlink" title="网络架构"></a>网络架构</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032558.jpg" alt="1_2ns4ota94je5gSVjrpFq3A"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032556.jpg" alt="Screen Shot 2018-05-17 at 11.06.53"></p><h1 id="Inception-Network"><a href="#Inception-Network" class="headerlink" title="Inception Network"></a>Inception Network</h1><h2 id="1-x-1-convolution"><a href="#1-x-1-convolution" class="headerlink" title="1 x 1 convolution"></a>1 x 1 convolution</h2><p>我们可以使用大小为1的卷积核做卷积操作，这样做的好处可以在卷积后不改变输入的尺寸。但是，通过设置卷积核的通道数和数量，来减少卷积后的通道数：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032552.jpg" alt="Screen Shot 2018-05-17 at 11.20.24"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032549.jpg" alt="Screen Shot 2018-05-17 at 11.20.58"></p><h2 id="Inception-module"><a href="#Inception-module" class="headerlink" title="Inception module"></a>Inception module</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032542.jpg" alt="Screen Shot 2018-05-17 at 11.23.05"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-17-032548.jpg" alt="Screen Shot 2018-05-17 at 11.23.21"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] . <a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="noopener">Yann Lecun LeNet-5</a></p><p>[2]. <a href="https://medium.com/@siddharthdas_32104/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5" target="_blank" rel="noopener">CNNs Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and more …</a></p><p>[3]. <a href="https://www.kaggle.com/malekbadreddine/tensorflow-convnet-lenet-5/code" target="_blank" rel="noopener">Tensorflow convNet (leNet-5)</a></p><p>[4]. <a href="https://github.com/sujaybabruwad/LeNet-in-Tensorflow" target="_blank" rel="noopener"><strong>LeNet-in-Tensorflow</strong></a></p><p>[5]. <a href="https://medium.com/@smallfishbigsea/a-walk-through-of-alexnet-6cbd137a5637" target="_blank" rel="noopener">A Walk-through of AlexNet</a></p><p>[6]. <a href="https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035" target="_blank" rel="noopener">An Overview of ResNet and its Variants</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;卷积神经网络中有很多经典的网络模型，通过学习这些网络模型对于我们有很大的启发作用，经典的模型如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LeNet - 5&lt;/li&gt;
&lt;li&gt;AlexNet&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;LeNet-5&quot;&gt;&lt;a href=&quot;#LeNet-5&quot; 
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="cnn" scheme="http://conghuai.me/tags/cnn/"/>
    
  </entry>
  
  <entry>
    <title>Validation</title>
    <link href="http://conghuai.me/2018/05/14/Validation/"/>
    <id>http://conghuai.me/2018/05/14/Validation/</id>
    <published>2018-05-14T07:20:12.000Z</published>
    <updated>2018-05-14T08:22:03.937Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习模型训练过程中，我们将面临有关使用预测变量的选择，使用何种类型的模型，提供这些模型的参数等。我们通过测量各种替代方案的模型质量，以数据驱动的方式做出这些选择。通常有如下四种验证模型性能的方法：</p><ul><li>Hold Out Method</li><li>K-Fold Cross Validation</li><li>Bootstrap</li></ul><h1 id="Hold-Out-Method"><a href="#Hold-Out-Method" class="headerlink" title="Hold Out Method"></a>Hold Out Method</h1><p>这种方法应该是最简单的验证方法，我们将训练样本随机分配数据点到两个集合<code>train</code>和<code>test</code>，通常分别称为训练集和测试集。每个集合的大小是任意的，尽管通常测试集合比训练集合小。然后我们在<code>train</code>上训练并在<code>test</code>上测试，所以该方法也称为<code>train_test_split</code>。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-073059.jpg" alt="Screen Shot 2018-05-14 at 15.25.04"></p><p>但是，这种方法是有缺点的，缺点如下：</p><p>想象一下你有一个包含5000行的数据集。train_test_split函数有一个test_size参数，您可以使用它来确定有多少行到达训练集以及有多少行进入测试集。测试集越大，您的模型质量测量值就越可靠。在极端情况下，您可以想象测试集中只有一行数据。如果您比较替代模型，哪一个模型能够在单个数据点上进行最佳预测，那么结果主要是运气问题。您通常会保留约20％作为测试数据集。但是即使在测试集中有1000行，在确定模型评分时也有一些随机的机会。一个模型可能在一组1000行上表现不错，即使它在不同的1000行上不准确。测试集越大，我们测量模型质量的随机性（又称“噪声”）就越少。但是我们只能通过从我们的训练数据中删除数据来获得大型测试集，而较小的训练数据集意味着更差的模型。事实上，对小数据集的理想建模决策通常不是大数据集上的最佳建模决策。</p><h1 id="K-Fold-Cross-Validation"><a href="#K-Fold-Cross-Validation" class="headerlink" title="K-Fold Cross Validation"></a>K-Fold Cross Validation</h1><p>由于没有足够的数据来训练模型，因此删除其中的一部分以进行验证会带来训练数据不足的问题。通过减少训练数据，我们有可能失去数据集中的重要模式/趋势，从而增加偏差导致的误差。所以，我们需要的是一种为训练模型提供充足数据的方法，同时也为验证集留下了充足的数据，K折交叉验证就是这样一种方法。</p><p>在交叉验证中，我们在不同的数据子集上运行我们的建模过程，以获得多个模型质量度量。例如，我们可以进行5次折叠或实验。我们将数据分成5个部分，每个部分占整个数据集的20％。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-075003.jpg" alt="1fXzJ"></p><p>我们运行实验一，其中第一折数据子集保留作为验证集，其他的数据子集作为训练数据。然后我们进行第二次实验，在第二次实验中我们保留第二折数据子集作为验证集，其他的数据子集作为训练数据。我们重复这个过程，使用每一折数据作为一次验证集。这样能保证模型在所有的数据上都训练过。最后，模型的误差为上面5个实验的误差的平均。</p><p>交叉验证提供了更准确的模型质量度量，如果我们正在做出大量建模决策，这一点尤其重要。但是，它可能需要更多时间才能运行，因为它每次估计一次模型。</p><h1 id="Bootstrap"><a href="#Bootstrap" class="headerlink" title="Bootstrap"></a>Bootstrap</h1><ul><li>随机有放回的从训练集中采样；</li><li>每个样本的大小与训练集数据一样；</li><li>用采样后的数据集训练模型；</li><li>评估模型的误差。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.kaggle.com/dansbecker/cross-validation/code" target="_blank" rel="noopener">Kaggle Cross-Validation</a></li><li><a href="https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f" target="_blank" rel="noopener">Cross-Validation in Machine Learning</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在机器学习模型训练过程中，我们将面临有关使用预测变量的选择，使用何种类型的模型，提供这些模型的参数等。我们通过测量各种替代方案的模型质量，以数据驱动的方式做出这些选择。通常有如下四种验证模型性能的方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hold Out Method&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Performance" scheme="http://conghuai.me/categories/Machine-Learning/Performance/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>Convolutional Network</title>
    <link href="http://conghuai.me/2018/05/14/Convolutional-Network/"/>
    <id>http://conghuai.me/2018/05/14/Convolutional-Network/</id>
    <published>2018-05-14T03:09:23.000Z</published>
    <updated>2018-09-24T15:23:44.832Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Convolutional-Layer"><a href="#Convolutional-Layer" class="headerlink" title="Convolutional Layer"></a>Convolutional Layer</h1><p>我们将卷积、padding、stride和filter的概念都整合起来，构建卷积神经网络。对于卷积后输出的结果，还需要经过激活函数来得到最终卷积层的输出：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-073109.jpg" alt="Screen Shot 2018-05-14 at 11.19.00"></p><p>现在考虑卷积层中需要的参数：</p><p>假设，卷积操作为：$f\cdot f\cdot n_c\cdot n’_c$，则总共需要的参数为：$f\cdot f\cdot n_c \cdot n’_c+1(bias)$个</p><p>因为，在卷积神经网络中，会涉及不同的卷积层，池化层或者全连接，为了区分每层的参数及超参数，我们做如下的定义：</p><ul><li>Input：$n_H^{[l-1]}\cdot n_W^{[l-1]}\cdot n_c^{[l-1]}$</li></ul><ul><li>$f^{[l]} = filter\ size$</li><li>$p^{[l]}=padding$</li><li>$s^{[l]}=stride$</li><li>$n_c^{[l]}=number\ of \ filters$</li><li>Each filter is : $f^{[l]}\cdot f^{[l]}\cdot n_c^{[l-1]}$</li><li>Output：$n_H^{[l]}\cdot n_w^{[l]}\cdot n_c^{[l]}$</li><li>Activations：$a^{[l]}\rightarrow n_H^{[l]}\cdot n_W^{[l]}\cdot n_c^{[l]}$</li><li>Weights：$f^{[l]}\cdot f^{[l]}\cdot n_c^{[l-1]}\cdot n_c^{[l]}$</li><li>bias：$n_c^{[l]}$</li></ul><h1 id="Pooling-Layers"><a href="#Pooling-Layers" class="headerlink" title="Pooling Layers"></a>Pooling Layers</h1><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-073111.jpg" alt="Screen Shot 2018-05-14 at 11.35.30"></p><p>常见的Pooling操作有：</p><ul><li>Average pooling.</li><li>Max pooling.</li></ul><h1 id="CNN-Example"><a href="#CNN-Example" class="headerlink" title="CNN Example"></a>CNN Example</h1><p>将卷积层、池化层和全连接层组合拼接成一个比较深的网络结构，就得到了CNN网络：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-073110.jpg" alt="Screen Shot 2018-05-14 at 11.38.55"></p><h1 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h1><p>我们通过实现一个小型的CNN来深入理解网络的细节，其模型结构如下所示：<img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-73113.jpg" alt="model"></p><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>卷积层涉及到卷积、padding、stride、filter、activcation等概念，卷积层将输入转换成不同维度的输出：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-073112.jpg" alt="conv_nn"></p><h3 id="Zero-Padding"><a href="#Zero-Padding" class="headerlink" title="Zero-Padding"></a>Zero-Padding</h3><p>padding操作对图片四周填充默认值，这个值通常是0：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-073113.jpg" alt="PAD"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: zero_pad</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span><span class="params">(X, pad)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, </span></span><br><span class="line"><span class="string">    as illustrated in Figure 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images</span></span><br><span class="line"><span class="string">    pad -- integer, amount of padding around each image on vertical and horizontal dimensions</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line)</span></span><br><span class="line">    X_pad = np.pad(X, ((<span class="number">0</span>,<span class="number">0</span>), (pad, pad), (pad, pad), (<span class="number">0</span>,<span class="number">0</span>)), <span class="string">'constant'</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_pad</span><br></pre></td></tr></table></figure><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-073114.jpg" alt="download"></p><h3 id="单步卷积操作"><a href="#单步卷积操作" class="headerlink" title="单步卷积操作"></a>单步卷积操作</h3><p>实现单步卷积操作，将卷积核与输入进行卷积操作，包含如下步骤：</p><ul><li>输入数据；</li><li>将卷积核作用于输入数据对应的每个位置上；</li><li>对其他通道采用同样的操作。</li></ul><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-73110.jpg" alt="Convolution_schematic"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: conv_single_step</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_single_step</span><span class="params">(a_slice_prev, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation </span></span><br><span class="line"><span class="string">    of the previous layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    <span class="comment"># Element-wise product between a_slice and W. Do not add the bias yet.</span></span><br><span class="line">    s = a_slice_prev * W</span><br><span class="line">    <span class="comment"># Sum over all entries of the volume s.</span></span><br><span class="line">    Z = np.sum(s)</span><br><span class="line">    <span class="comment"># Add bias b to Z. Cast b to a float() so that Z results in a scalar value.</span></span><br><span class="line">    Z = Z + np.squeeze(b)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><h3 id="卷积神经网络——前向运算"><a href="#卷积神经网络——前向运算" class="headerlink" title="卷积神经网络——前向运算"></a>卷积神经网络——前向运算</h3><ul><li>选择区域；</li></ul><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-073106.jpg" alt="vert_horiz_kiank"></p><ul><li>定义输出的维度；</li></ul><p>$$n_H = \lfloor \frac{n_{H_{prev}} - f + 2 \times pad}{stride} \rfloor +1$$</p><p>$$n_W = \lfloor \frac{n_{W_{prev}} - f + 2 \times pad}{stride} \rfloor +1$$</p><p>$$n_C = \text{number of filters used in the convolution}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: conv_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(A_prev, W, b, hparameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    b -- Biases, numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "stride" and "pad"</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward() function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape (≈1 line)  </span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape (≈1 line)</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters" (≈2 lines)</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)</span></span><br><span class="line">    n_H = int((n_H_prev - f + <span class="number">2</span>*pad)/stride) + <span class="number">1</span></span><br><span class="line">    n_W = int((n_W_prev - f + <span class="number">2</span>*pad)/stride) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the output volume Z with zeros. (≈1 line)</span></span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create A_prev_pad by padding A_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over the batch of training examples</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i,:,:,:]               <span class="comment"># Select ith training example's padded activation</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                           <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                       <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):                   <span class="comment"># loop over channels (= #filters) of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice" (≈4 lines)</span></span><br><span class="line">                    vert_start = h*stride</span><br><span class="line">                    vert_end = vert_start+f</span><br><span class="line">                    horiz_start = w*stride</span><br><span class="line">                    horiz_end = horiz_start+f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)</span></span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)</span></span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])</span><br><span class="line">                                        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save information in "cache" for the backprop</span></span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化（POOL）层减少输入的高度和宽度。它有助于减少计算量，并有助于使特征检测器的输入位置更加稳定。这两种池化层是：</p><ul><li>最大池化层：在输入上滑动（f，f）窗口，并将窗口的最大值存储在输出中。</li><li>平均池图层：在输入上滑动（f，f）窗口，并将窗口的平均值存储在输出中。</li></ul><table><br><td><br><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-73107.jpg" style="width:500px;height:300px;"><br></td><td><br><br></td><td><br><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-14-073108.jpg" style="width:500px;height:300px;"><br></td><td><br></td></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: pool_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span><span class="params">(A_prev, hparameters, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward pass of the pooling layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "f" and "stride"</span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from the input shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve hyperparameters from "hparameters"</span></span><br><span class="line">    f = hparameters[<span class="string">"f"</span>]</span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the dimensions of the output</span></span><br><span class="line">    n_H = int(<span class="number">1</span> + (n_H_prev - f) / stride)</span><br><span class="line">    n_W = int(<span class="number">1</span> + (n_W_prev - f) / stride)</span><br><span class="line">    n_C = n_C_prev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize output matrix A</span></span><br><span class="line">    A = np.zeros((m, n_H, n_W, n_C))              </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                         <span class="comment"># loop over the training examples</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                     <span class="comment"># loop on the vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                 <span class="comment"># loop on the horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range (n_C):            <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice" (≈4 lines)</span></span><br><span class="line">                    vert_start = h*stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w*stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)</span></span><br><span class="line">                    a_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.max(a_prev_slice)</span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.mean(a_prev_slice)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Store the input and hparameters in "cache" for pool_backward()</span></span><br><span class="line">    cache = (A_prev, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Convolutional-Layer&quot;&gt;&lt;a href=&quot;#Convolutional-Layer&quot; class=&quot;headerlink&quot; title=&quot;Convolutional Layer&quot;&gt;&lt;/a&gt;Convolutional Layer&lt;/h1&gt;&lt;p&gt;我们
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="cnn" scheme="http://conghuai.me/tags/cnn/"/>
    
  </entry>
  
  <entry>
    <title>Convolutional Neural Networks Operations</title>
    <link href="http://conghuai.me/2018/05/14/Convolutional%20Neural%20Networks%20Operations/"/>
    <id>http://conghuai.me/2018/05/14/Convolutional Neural Networks Operations/</id>
    <published>2018-05-14T01:51:03.000Z</published>
    <updated>2018-09-24T15:23:41.147Z</updated>
    
    <content type="html"><![CDATA[<p>计算机视觉是深度学习应用的比较成功的一个领域。卷积神经网络这种神经网络结构在计算机视觉很多方面的做得非常出色，比如图片分类、目标检测和图片风格转换。</p><h1 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h1><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fraoxk9dgaj309304at92.jpg" alt="download"></p><p>上图比较清晰的反映出图片中的一个区域与一个卷积核进行卷积操作后得到的结果，我们将区域对应位置的值与卷积核对应位置的值进行相乘，最后将所有相乘后的结果累加，得到最终的结果。</p><p>那么卷积操作有什么意义呢？在吴恩达的deeplearning.ai课程上，通过一个例子来直观理解卷积操作的在边缘检测上的意义：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fraoxkrheej30pk0edwh3.jpg" alt="Screen Shot 2018-05-14 at 10.06.30"></p><p>如果我们要检测不同的边缘，我们可以采用不同的卷积核来进行卷积操作：</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fraoxl6lrdj30gy09m75m.jpg" alt="Screen Shot 2018-05-14 at 10.09.04"></p><p>卷积操作后得到的图片大小：</p><p>$$n’=\frac{n+2p-f}{s}+1$$</p><ul><li>$n$ : 原始输入大小；</li><li>$p$ : 填充的大小；</li><li>$f$：卷积核（filter）的大小；</li><li>$s$：步长。</li></ul><h1 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h1><p>Padding这种操作是指我们对输入图片按照某种策略进行填充，使之成为更大的图片，这样做的一个目的在于使得卷积后的图片的大小和原始图片大小是一样的， 这样就不会使得图片在逐次的卷积操作中越变越小。</p><p>一般采用Padding的策略都是<code>constant</code>填充，即用一个固定值来填充输入图片的四周。</p><p>根据填充后图片的大小，有两种比较特别的填充方式：</p><ul><li>“Valid” ：表示没有进行padding操作；</li><li>“Same”：表示进行填充，使得卷积后的图片大小和原始图片大小是一样的。<ul><li>$$n’=\frac{n+2p-f}{s}+1 = n =&gt; p=\frac{f-1}{2}$$</li></ul></li></ul><p>使用Padding操作的好处：</p><ul><li>它允许您使用CONV层，而不必缩小图片的高度和宽度。这对于建立更深的网络非常重要，否则当你走向更深层时，高度/宽度会缩小。一个重要的特例是“Same”卷积，其中高度/宽度在一层之后被完全保留。</li><li><strong>它可以帮助我们将更多的信息保存在图像的边界。如果没有填充，下一层的极少数值将受到像素边缘的影响。</strong></li></ul><h1 id="Strided-Convolutions"><a href="#Strided-Convolutions" class="headerlink" title="Strided Convolutions"></a>Strided Convolutions</h1><p>步长是指，我们每次移动卷积核的步数，比如1、2、5等。</p><h1 id="Convolutions-Over-Volume"><a href="#Convolutions-Over-Volume" class="headerlink" title="Convolutions Over Volume"></a>Convolutions Over Volume</h1><p>上面的例子中，卷积操作都是只在一个通道上进行的。但是，图片可以使多通道的，如RGB三个通道。所以，卷积操作也可以是多通道的卷积操作。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fraoxj7dwaj31gi0tq44q.jpg" alt="Screen Shot 2018-05-14 at 10.40.36"></p><p>对于多个通道得到的多个结果，将多个结果进行线性相加，得到一个结果。</p><h1 id="Multiple-filters"><a href="#Multiple-filters" class="headerlink" title="Multiple filters"></a>Multiple filters</h1><p>除了输入可能有多个通道外，进行卷积操作涉及的卷积核也可能有多个，这样得到的结果就会有多个，不同的卷积核通常会抽取不同的特征。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fraoxjtpyxj30iy0aq403.jpg" alt="Screen Shot 2018-05-14 at 10.51.12"></p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>$$n\cdot n \cdot n_c\ *\ f\cdot f\cdot n_c\cdot n’_c\rightarrow \left \lfloor \frac{n+2p-f}{s}+1 \right \rfloor\cdot \left \lfloor \frac{n+2p-f}{s}+1 \right \rfloor\cdot n’_c $$</p><ul><li>$n$：输入数据大小；</li><li>$n_c$：输入数据的通道数；</li><li>$f$：卷积核大小；</li><li>$n’_c$：卷积核数量；</li><li>$p$：padding的大小；</li><li>$s$：步长。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;计算机视觉是深度学习应用的比较成功的一个领域。卷积神经网络这种神经网络结构在计算机视觉很多方面的做得非常出色，比如图片分类、目标检测和图片风格转换。&lt;/p&gt;
&lt;h1 id=&quot;Convolution&quot;&gt;&lt;a href=&quot;#Convolution&quot; class=&quot;headerl
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积神经网络" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="cnn" scheme="http://conghuai.me/tags/cnn/"/>
    
  </entry>
  
  <entry>
    <title>Dropout</title>
    <link href="http://conghuai.me/2018/05/01/Dropout/"/>
    <id>http://conghuai.me/2018/05/01/Dropout/</id>
    <published>2018-05-01T03:26:03.000Z</published>
    <updated>2018-09-24T15:25:12.295Z</updated>
    
    <content type="html"><![CDATA[<p>过拟合是机器学习和深度学习的一个比较重要的问题，模型一旦过拟合，就会出现在训练样本上表现很好，但是在测试样本上表现很差的情况。所以，过拟合导致模型变得不可用。</p><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqvphars2ej30h209475u.jpg" alt="dropout"></p><blockquote><p>In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing. Therefore, units may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit, Dropout prevents co-adaptation by making the presence of other hidden units unreliable. Therefore, a hidden unit cannot rely on other specific units to correct its mistakes.</p></blockquote><p>Dropout方法在每次迭代中都会随机关闭一部分的神经元，这就相当于修改了模型，所以在每次迭代中，实际上训练的就是不同的模型，每个模型都只用到一部分的神经元。这样做的好处在于，使得某个神经元不会过于依赖另一个神经元，避免了互相适应的效果，所以有效的防止了过拟合。</p><p>被关闭的神经元对于前向传播和反向传播都不起作用。</p><h2 id="Directly-Dropout"><a href="#Directly-Dropout" class="headerlink" title="Directly Dropout"></a>Directly Dropout</h2><p>因为在训练阶段，某个神经元被保留的概率为$p$，在测试阶段，为了模拟组合不同神经网络的结果，我们需要对激活函数值乘以概率$p$。</p><ul><li>Train phase : $O_i = X_i a(\sum_{k=1}^{d_i}w_kx_k+b)$</li><li>Test phase : $O_i = pa(\sum_{k=1}^{d_i}w_kx_k+b)$</li></ul><h2 id="Inverted-Dropout"><a href="#Inverted-Dropout" class="headerlink" title="Inverted Dropout"></a>Inverted Dropout</h2><p>与上面方式不同是，该方法在训练过程就考虑的缩放因子，所以在测试阶段就不需要做任何处理。</p><ul><li>Train phase : $O_i = \frac{1}{p}X_i a(\sum_{k=1}^{d_i}w_kx_k+b)$</li><li>Test phase：$O_i = a(\sum_{k=1}^{d_i}w_kx_k+b)$</li></ul><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="前向运算"><a href="#前向运算" class="headerlink" title="前向运算"></a>前向运算</h2><ol><li>对于每层神经元$a^{[l]}$，我们创建一个维度一样的变量$d^{[l]}$，其中$d^{[l]}_i\in(0,1)$。因为每一层的输入有多个，我们采用向量化方式来处理，将$d^{[l]}$进行扩充得，$D^{[l]}=[d^{[l]1},d^{[l]2},…,d^{[l]m}]$，该维度与$A^{[l]}$的维度一致；</li><li>将$D^{[l]}$中的每个元素根据与keep_prob相比，置位0或1；</li><li>$A^{[l]} = A^{[l]}*D^{[l]}$；</li><li>$A^{[l]} /= keep\_prob$</li></ol><h2 id="后向运算"><a href="#后向运算" class="headerlink" title="后向运算"></a>后向运算</h2><ol><li>在前向运算中，我们利用$D^{[l]}$关闭了某些神经元的前向传播，在后向运算的时候，我们同样关闭该神经元的后向传播：$dA^{[l]} = dA^{[l]}*D^{[l]}$;</li><li>$dA^{[l]} /= keep\_prob$</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>Dropout是一种防止过拟合的技术，通常用于神经网络中；</li><li>在训练阶段使用Dropout，而不要在测试阶段使用它；</li><li>在前向运算和后向运算都使用Dropout；</li><li>Dropout存在两种版本：direct 和 inverted。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] . <a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">Analysis of Dropout</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;过拟合是机器学习和深度学习的一个比较重要的问题，模型一旦过拟合，就会出现在训练样本上表现很好，但是在测试样本上表现很差的情况。所以，过拟合导致模型变得不可用。&lt;/p&gt;
&lt;h1 id=&quot;Dropout&quot;&gt;&lt;a href=&quot;#Dropout&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型优化" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="overfiting" scheme="http://conghuai.me/tags/overfiting/"/>
    
  </entry>
  
  <entry>
    <title>Parameters Initialization</title>
    <link href="http://conghuai.me/2018/04/30/Parameters-Initialization/"/>
    <id>http://conghuai.me/2018/04/30/Parameters-Initialization/</id>
    <published>2018-04-30T07:16:45.000Z</published>
    <updated>2018-09-24T15:29:04.573Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络通常有大量的参数需要训练，在模型一开始训练时，需要初始化模型的参数，我们该采用何种策略来初始化参数呢？不同的初始化方法将会带来如下的效果：</p><ul><li>加速梯度下降的收敛速度；</li><li>可能会使得模型在训练集上的错误率更低。</li></ul><h1 id="Zero-initialization"><a href="#Zero-initialization" class="headerlink" title="Zero initialization"></a>Zero initialization</h1><p>如果我们将权重设置为零，那么所有层的所有神经元都执行相同的计算，给出相同的输出，则整个深层网络的复杂度将与单个神经元的复杂度相同，并且预测不会比随机更好。这样的初始化方式带来了隐藏层神经元<strong>对称问题</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l], layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">    parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fquqmny4l2j30dj0hktbc.jpg" alt="Screen Shot 2018-04-30 at 15.35.56"></p><h1 id="Random-initialization"><a href="#Random-initialization" class="headerlink" title="Random initialization"></a>Random initialization</h1><p>为了解决对称问题，我们可以随机初始化$W$参数，这样可以使得每个神经元得到不同的输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class="number">-1</span>]) * <span class="number">10</span></span><br><span class="line">    parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fquqmmnj2qj30ey0hk413.jpg" alt="Screen Shot 2018-04-30 at 15.38.06"></p><p>随机初始化参数可能会带来两个问题：（1）梯度消失；（2）梯度爆炸。</p><h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><blockquote><p>The weight update is minor and results in slower convergence. This makes the optimization of the loss function slow. In the worst case, this may completely stop the neural network from training further.</p></blockquote><p>在神经网络中，对于任何激活函数，当我们对损失做反向传播的时候，$dW$将会越来越小。所以，越靠近输入层的神经元其改变量将越小。</p><h2 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h2><blockquote><p>This may result in oscillating around the minima or even overshooting the optimum again and again and the model will never learn!</p></blockquote><p>该问题和上面的相反，当我们参数值很大时，经过多层神经元累积后，得到的值将会非常大。</p><h2 id="梯度消失于梯度爆炸分析"><a href="#梯度消失于梯度爆炸分析" class="headerlink" title="梯度消失于梯度爆炸分析"></a>梯度消失于梯度爆炸分析</h2><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqvomchpu7j30ga08hjtd.jpg" alt="v2-82873a89ff3c14c1d3b42d1862917f35_hd"></p><p>如图含有3个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的hidden layer3等的权值更新相对正常，单前面的hidden layer1的权值更新会变得很慢，导致前面的层权值几乎不变， 仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做一个同一映射，这时此深层网络的学习就等价于只有后几层的浅层网络的学习了。</p><p>而这种问题为何会产生呢？以下图的反向传播为例（假设每一层只有一个神经元且对于每一层，$y_i=\sigma(z_i)=\sigma(w_ix_i+b_i)$，其中$\sigma$为sigmoid函数）</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqvomc3qyqj30cm01qglh.jpg" alt="v2-b9e0d6871fbcae05d602bab65620a3ca_hd"></p><p>可以推导出</p><p>$$\begin{align} \frac{\partial C}{\partial b_1} &amp;= \frac{\partial C}{\partial y_4}\cdot \frac{\partial y_4}{\partial z_4}\cdot \frac{\partial z_4}{\partial x_4}\cdot \frac{\partial x_4}{\partial z_3}\cdot \frac{\partial z_3}{\partial x_3}\cdot \frac{\partial x_3}{\partial z_2}\cdot \frac{\partial z_2}{\partial x_2}\cdot \frac{\partial x_2}{\partial z_1}\cdot \frac{\partial z_1}{\partial b_1} \\  &amp;= \frac{\partial C}{\partial y_4}\cdot \sigma’(z_4)\cdot w_4\cdot \sigma’(z_3)\cdot w_3\cdot \sigma’(z_2)\cdot w_2 \sigma’(z_1)  \\   \end{align}$$</p><p>而sigmoid的导数$\sigma’(x)$如下图所示</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqvomcwj7zj30ai06kt8p.jpg" alt="v2-da5606a2eebd4d9b6ac4095b398dacf5_hd"></p><p>可见，$\sigma’(x)$的最大值为$\frac{1}{4}$，而我们初始化的网络权值$|w|$通常都小于1，因此$|\sigma’(z)w|\leq\frac{1}{4}$，因此对于上面的链式求导，层数越多，求导结果$\frac{\partial C}{\partial b_1}$越小，因而导致梯度消失的情况出现。</p><p>这样，梯度爆炸问题的出现原因就显而易见了，即$|\sigma’(z)w|&gt;1$，也就是$w$比较大的情况下。</p><h1 id="Best-Practices"><a href="#Best-Practices" class="headerlink" title="Best Practices"></a>Best Practices</h1><p>在神经网络模型中，我们通常会使用<code>ReLU</code>或者<code>leaky ReLU</code>作为激活函数，该激活函数对于梯度消失和梯度爆炸问题具有较好的鲁棒性。我们可以根据不同的激活函数，采用启发式方式来初始化参数。</p><h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>$$\sqrt {\frac{2}{size^{[l-1]}}}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(size_l, size_l<span class="number">-1</span>) * np.sqrt(<span class="number">2</span>/size_l<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h2><p>$$\sqrt {\frac{1}{size^{[l-1]}}}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(size_l, size_l<span class="number">-1</span>) * np.sqrt(<span class="number">1</span>/size_l<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks" target="_blank" rel="noopener">coursera deeplearning.ai</a></p><p>[2]. <a href="https://towardsdatascience.com/deep-learning-best-practices-1-weight-initialization-14e5c0295b94" target="_blank" rel="noopener">Deep Learning Best Practices (1) — Weight Initialization</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;神经网络通常有大量的参数需要训练，在模型一开始训练时，需要初始化模型的参数，我们该采用何种策略来初始化参数呢？不同的初始化方法将会带来如下的效果：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加速梯度下降的收敛速度；&lt;/li&gt;
&lt;li&gt;可能会使得模型在训练集上的错误率更低。&lt;/li&gt;
&lt;/u
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型优化" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="regularization" scheme="http://conghuai.me/tags/regularization/"/>
    
      <category term="hyperparameter tuning" scheme="http://conghuai.me/tags/hyperparameter-tuning/"/>
    
  </entry>
  
  <entry>
    <title>Building blocks of deep neural networks</title>
    <link href="http://conghuai.me/2018/04/27/Building-blocks-of-deep-neural-networks/"/>
    <id>http://conghuai.me/2018/04/27/Building-blocks-of-deep-neural-networks/</id>
    <published>2018-04-27T12:16:45.000Z</published>
    <updated>2018-09-24T15:23:26.065Z</updated>
    
    <content type="html"><![CDATA[<p>深度学习中通常都有很多个隐藏层， 每个隐藏层会有很多个神经元，这些神经元的输出连接到下一层，然后经过激活函数产生新的输出。数据输入到神经元，经过多个隐藏层，最后产生最终的输出结果。我们对预测结果和真实结果计算损失函数，然后利用该损失对各层的参数求偏导，进而通过偏导值更新参数。这个过程会迭代很多次。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqtkpzf23hj30g30793zh.jpg" alt="Screen Shot 2018-04-29 at 13.47.34"></p><h1 id="构建运算块"><a href="#构建运算块" class="headerlink" title="构建运算块"></a>构建运算块</h1><p>神经网络运算从大的方向上主要分为两个，一个是前向运算，另一个是后向运算。两个运算的输入、输出已经所需参数都是不一样的。</p><h2 id="前向运算"><a href="#前向运算" class="headerlink" title="前向运算"></a>前向运算</h2><p>前向运算指训练数据输入到神经网络中，经过多个隐藏层，最后得到损失函数值。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqtkpzvds9j30qp0eeq3z.jpg" alt="Screen Shot 2018-04-29 at 14.26.21"></p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqtkq0sj3uj30zf0drdgx.jpg" alt="Screen Shot 2018-04-29 at 14.37.57"></p><p>对于<code>Layer L</code></p><ul><li>输入<ul><li>$a^{[l-1]}$</li></ul></li><li>输出<ul><li>$a^{[l]}$</li></ul></li><li>参数<ul><li>$W^{[l]}$,$b^{[l]}$</li></ul></li><li>运算</li></ul><p>$$Z^{[l]}=W^{[l]}A^{[l-1]}+b$$</p><p>$$A^{[l]}=g^{[l]}(Z^{[l]})$$</p><ul><li>缓存<ul><li>$Z^{[l]}$</li></ul></li></ul><h2 id="后向运算"><a href="#后向运算" class="headerlink" title="后向运算"></a>后向运算</h2><p>后向运算指损失函数值对各个隐藏层参数求偏导的过程。</p><ul><li>输入<ul><li>$da^{[l]}$</li></ul></li><li>输出<ul><li>$da^{[l-1]}$,$dW^{[l]}$,$db^{[l]}$</li></ul></li><li>运算</li></ul><p>$$dZ^{[l]}=dA^{[l]}\cdot g^{[l]}‘(Z^{[l]})$$</p><p>$$dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}$$</p><p>$$db^{[l]}=\frac{1}{m}\cdot np.sum(dZ^{[l]}, axis=1, keepdims=True)$$</p><p>$$dA^{[l-1]}=W^{[l]T}dZ^{[l]}$$    </p><ul><li>计算分析如下</li></ul><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqtkq1t8qkj30sp0cu0ug.jpg" alt="Screen Shot 2018-04-29 at 15.22.48"></p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqtkq17h0fj30w50d7myv.jpg" alt="Screen Shot 2018-04-29 at 15.32.43"></p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqtkq0b17bj30t60d7wfx.jpg" alt="Screen Shot 2018-04-29 at 15.44.07"></p><h2 id="运算块"><a href="#运算块" class="headerlink" title="运算块"></a>运算块</h2><p>我们可以通过运算块接连图的方式来直观的理解前向运算和后向运算的过程。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqtkq28olzj30yz0ijwi9.jpg" alt="Screen Shot 2018-04-29 at 15.46.20"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. Coursera deeplearning.ai</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;深度学习中通常都有很多个隐藏层， 每个隐藏层会有很多个神经元，这些神经元的输出连接到下一层，然后经过激活函数产生新的输出。数据输入到神经元，经过多个隐藏层，最后产生最终的输出结果。我们对预测结果和真实结果计算损失函数，然后利用该损失对各层的参数求偏导，进而通过偏导值更新参数
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深层网络" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>激活函数 1-1：概述</title>
    <link href="http://conghuai.me/2018/04/25/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%201-1%EF%BC%9A%E6%A6%82%E8%BF%B0/"/>
    <id>http://conghuai.me/2018/04/25/激活函数 1-1：概述/</id>
    <published>2018-04-25T07:38:44.000Z</published>
    <updated>2018-09-24T15:21:57.900Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqp0idk6d0j315o0p0tc4.jpg" alt="nural-network_3"></p><p>激活函数是神经网络中一个比较重要的概念，激活函数首先计算输入的线性值，然后决定是否“激活”该值。</p><p>$$Y=Activation(\sum(weight\ *\ input ) + bias)$$</p><p>上面的线性组合的值的范围为$[-\infty, +\infty]$，激活函数根据上一层输出的线性组合值来决定是否将该值传到下一层，并决定以何种方式传到下一层。</p><p>正如绝大多数神经网络借助某种形式的梯度下降进行优化，激活函数需要是可微分的。此外，复杂的激活函数也许产生一些梯度消失或梯度爆炸的问题。常见的激活函数有很多，我们该如何选择合适的激活函数呢？我们先来分析一下每种激活函数的特点和性质。</p><h1 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h1><p>激活函数显然增加了神经网络的复杂度，那么 ，我们是否可以不同激活函数呢？答案是不可行的，如果不用激活函数，那么神经网络模型最终将会只是一个线性模型。而线性模型由于其模型的复杂度不够，在解决很多复杂问题上都显得力不从心。</p><h1 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h1><h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><h3 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h3><p>$$g(z) = \frac{1}{1+e^{-z}}$$</p><h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><p>$$g’(z) = g(z)(1-g(z))$$</p><h3 id="函数性质"><a href="#函数性质" class="headerlink" title="函数性质"></a>函数性质</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqp0icy6xwj311z0moacm.jpg" alt="5561420171010093434"></p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x, derivative=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> (derivative == <span class="keyword">True</span>):</span><br><span class="line">        <span class="keyword">return</span> x * (<span class="number">1</span> - x)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>梯度消失问题</li></ol><p>在导函数的两端，函数值非常的小，这意味着在做梯度下降时，参数的改变量将非常小。这就可能带来梯度消失的问题。</p><ol><li>饱和问题。</li></ol><p>饱和问题是指当初始参数过大时，会导致sigmoid导数值很小，造成学习速率非常慢。</p><h2 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h2><h3 id="表达式-1"><a href="#表达式-1" class="headerlink" title="表达式"></a>表达式</h3><p>$$g(z) = tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$$</p><h3 id="导数-1"><a href="#导数-1" class="headerlink" title="导数"></a>导数</h3><p>$$g’(z)= 1-g(z)^2$$</p><h3 id="函数性质-1"><a href="#函数性质-1" class="headerlink" title="函数性质"></a>函数性质</h3><p>在分类任务中，双曲正切函数（Tanh）逐渐取代Sigmoid函数作为标准的激活函数。<img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqp0ig26d5j311m0jy0vf.jpg" alt="8940520171010093544"></p><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x, derivative=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> (derivative == <span class="keyword">True</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="number">1</span> - (x ** <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br></pre></td></tr></table></figure><h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>修正线性单元（Rectified linear unit，ReLU）是神经网络中最常用的激活函数。它保留了 step 函数的生物学启发（只有输入超出阈值时神经元才激活），不过当输入为正的时候，导数不为零，从而允许基于梯度的学习（尽管在 x=0 的时候，导数是未定义的）。使用这个函数能使计算变得很快，因为无论是函数还是其导数都不包含复杂的数学运算。然而，当输入为负值的时候，ReLU 的学习速度可能会变得很慢，甚至使神经元直接无效，因为此时输入小于零而梯度为零，从而其权重无法得到更新，在剩下的训练过程中会一直保持静默。</p><h3 id="表达式-2"><a href="#表达式-2" class="headerlink" title="表达式"></a>表达式</h3><p>$$\begin{split} f(x)=\begin{cases} x, &amp; \text{$x\geq 0$} \\ 0, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><h3 id="导数-2"><a href="#导数-2" class="headerlink" title="导数"></a>导数</h3><p>$$\begin{split} f’(x)=\begin{cases} 1, &amp; \text{$x\geq 0$} \\ 0, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><h3 id="函数性质-2"><a href="#函数性质-2" class="headerlink" title="函数性质"></a>函数性质</h3><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqp0if00ohj31240mujtv.jpg" alt="4217520171010093357"></p><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ol><li>静默神经元。</li></ol><p>由于该激活函数在$x&lt;0$的时候，其导数值为0，就会使得一些神经元一直得不到更新。</p><ol><li>只能用于隐藏层。</li></ol><h2 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2><p>经典（以及广泛使用的）ReLU 激活函数的变体，带泄露修正线性单元（Leaky ReLU）的输出对负值输入有很小的坡度。由于导数总是不为零，这能减少静默神经元的出现，允许基于梯度的学习（虽然会很慢）。</p><h3 id="表达式-3"><a href="#表达式-3" class="headerlink" title="表达式"></a>表达式</h3><p>$$\begin{split} f(x)=\begin{cases} x, &amp; \text{$x\geq0$} \\ 0.01x, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><h3 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h3><p>$$\begin{split} f’(x)=\begin{cases} 1, &amp; \text{$x\geq0$} \\ 0.01, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><h3 id="函数性质-3"><a href="#函数性质-3" class="headerlink" title="函数性质"></a>函数性质</h3><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqp0ieauwoj311z0mq0ve.jpg" alt="1601520171010093621"></p><h1 id="选择激活函数准则"><a href="#选择激活函数准则" class="headerlink" title="选择激活函数准则"></a>选择激活函数准则</h1><ul><li>Sigmoid激活函数常用于分类任务。</li><li>Sigmoid和Tanh函数会产生梯度消失问题。</li><li>ReLU是目前使用最广泛的激活函数。</li><li>如果在神经网络中，出现了静默神经元，可以考虑使用 Leaky ReLU。</li><li>ReLU激活函数只能用于隐藏层。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" target="_blank" rel="noopener">Understanding Activation Functions in Neural Networks</a></p><p>[2]. <a href="https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/" target="_blank" rel="noopener">Fundamentals of Deep Learning – Activation Functions and When to Use Them?</a></p><p>[3]. <a href="https://www.wikiwand.com/en/Activation_function" target="_blank" rel="noopener">维基百科Activation Functions</a></p><p>[4]. <a href="https://analyticsindiamag.com/most-common-activation-functions-in-neural-networks-and-rationale-behind-it/" target="_blank" rel="noopener">Types Of Activation Functions In Neural Networks And Rationale Behind It</a></p><p>[5]. <a href="https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/" target="_blank" rel="noopener">Visualising Activation Functions in Neural Networks</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcgy1fqp0idk6d0j315o0p0tc4.jpg&quot; alt=&quot;nural-network_3&quot;&gt;&lt;/p&gt;
&lt;p&gt;激活函数是神经网络中一个比较重要的概念，激活函数首先计算输入
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="激活函数" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>Neural Networks and Deep Learning Foundation</title>
    <link href="http://conghuai.me/2018/04/24/Neural-Networks-and-Deep-Learning-Foundation/"/>
    <id>http://conghuai.me/2018/04/24/Neural-Networks-and-Deep-Learning-Foundation/</id>
    <published>2018-04-24T06:33:38.000Z</published>
    <updated>2018-09-24T15:28:31.779Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络由于其多层多神经元的特性，参数往往很多，这也导致了学习深度学习的时候会有太“复杂”的感觉。该文章整理出一些神经网络中重要的基础概念和符号表示系统。</p><h1 id="符号表示体系"><a href="#符号表示体系" class="headerlink" title="符号表示体系"></a>符号表示体系</h1><h2 id="单层神经元"><a href="#单层神经元" class="headerlink" title="单层神经元"></a>单层神经元</h2><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqlph8h48ej30f90e7aas.jpg" alt=""></p><p>通常用$a^l$来表示第$l$层的向量，用$a^l_i$来表示该层向量中第$i$个分量。<strong>记住，上标用来表示整体，下标用来表示分量</strong>。</p><h2 id="参数矩阵W"><a href="#参数矩阵W" class="headerlink" title="参数矩阵W"></a>参数矩阵W</h2><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqlph7gyr8j30o70f10ua.jpg" alt=""></p><ul><li>$ W$中的每一行代表的是每一个输出神经元输入的参数；W中的每一列代表的是每一个输入元输出的参数。</li><li>$W_{ij}$是指从下一层的第$i$个神经元指向上一层的第$j$个神经元。</li></ul><h1 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h1><p>假设我们要计算的是$J(a,b,c)=3\cdot (a+bc)$，在深度学习中，我们经常会把每个计算式转化成一个计算图进行计算。针对这个计算图，一般来说，我们通常需要的前向计算损失函数值和后向传播求偏导两个步骤来学习我们的模型。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqorpcvi8qj30kg0bwq3c.jpg" alt="1.jpg"></p><h1 id="计算表示"><a href="#计算表示" class="headerlink" title="计算表示"></a>计算表示</h1><p>深度学习中涉及到很多计算，包括损失函数的计算，偏导的计算等等。并且，由于训练一个模型的时候，通过需要大量的样本，大量的参数，所有编写代码来训练深度学习模型时，通常需要涉及很多重循环。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>我们以简单的逻辑回顾模型为例来说明计算过程。</p><p>逻辑回归：</p><p>$$z = w^Tx+b$$</p><p>$$\hat{y}=a=\sigma(x)$$</p><p>$$L(a,y) = -[ylog(a)+(1-y)log(1-a)]$$</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqorpbe3f8j30x60crab9.jpg" alt="Screen Shot 2018-04-24 at 15.50.28"></p><h3 id="单个样本"><a href="#单个样本" class="headerlink" title="单个样本"></a>单个样本</h3><p>对于单个样本，我们只需要计算其loss function值，然后分别计算导数即可：</p><p>$$\frac{\partial L}{\partial a}=-\frac{y}{a}-\frac{1-y}{1-a}$$</p><p>$$\begin{align} \frac{\partial L}{\partial z} &amp;=\frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z} \\\\  &amp;= a\cdot(1-a)\cdot [-\frac{y}{a}-\frac{1-y}{1-a}] \\\\ &amp;= a- y\end{align}$$</p><p>$$\begin{align} \frac{\partial L}{\partial w_1} &amp;=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w} \\\\  &amp;= x_1\cdot (a-y) \end{align}$$</p><p>$$\begin{align} \frac{\partial L}{\partial w_2} &amp;=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w} \\\\  &amp;= x_2\cdot (a-y) \end{align}$$</p><p>$$\begin{align} \frac{\partial L}{\partial w_3} &amp;=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w} \\\\  &amp;= x_3\cdot (a-y) \end{align}$$</p><p>可以看出来，在对单个样本计算loss function的时候并不困难，我们只需要计算三个导数即可。</p><p>实现分析：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations): // 迭代次数</span><br><span class="line">    z = w*x + b</span><br><span class="line">    a = sigmoid(z)</span><br><span class="line">    J = -[y*loga + (<span class="number">1</span>-y)*log(<span class="number">1</span>-a)]</span><br><span class="line">    dz = a - y</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(para_num):</span><br><span class="line">        dw_i = x_i*dz</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(para_num): // 参数个数</span><br><span class="line">        w_i = w_i - alpha * dw_i</span><br></pre></td></tr></table></figure><h3 id="多个样本"><a href="#多个样本" class="headerlink" title="多个样本"></a>多个样本</h3><p>现在从单个样本扩展到多个样本的情况，我们用上标来表示训练集中不同的样本，即$x^{(i)}$。</p><p>这时候cost funciton值为每个样本的loss function值的求和平均：</p><p>$$J(w,b) = \frac{1}{m}\sum_{i=1}^mL(a^{(i)},y^{(i)})$$</p><p>$$a^{(i)}=\hat{y}^{(i)}=\sigma(z^{(i)})=\sigma(w^Tx^{(i)}+b)$$</p><p>求偏导</p><p>$$\frac{\partial J}{\partial a^{i}}=\frac{1}{m}\cdot [-\frac{y^{(i)}}{a^{(i)}}-\frac{1-y^{(i)}}{1-a^{(i)}}]$$</p><p>$$\begin{align} \frac{\partial J}{\partial z^{(i)}} &amp;=\frac{\partial J}{\partial a^{(i)}}\cdot \frac{\partial a^{(i)}}{\partial z^{(i)}} \\  &amp;= \frac{1}{m}\cdot a^{(i)}\cdot(1-a^{(i)})\cdot [-\frac{y^{(i)}}{a^{(i)}}-\frac{1-y^{(i)}}{1-a^{(i)}}] \\&amp;= \frac{1}{m}\cdot (a^{(i)}- y^{(i)})\end{align}$$</p><p>$$\begin{align} \frac{\partial J}{\partial w_1} &amp;=\sum_{i=1}^N\frac{\partial J}{\partial z^{(i)}}\cdot \frac{\partial z^{(i)}}{\partial w_1} \\  &amp;= \frac{1}{m} \cdot x_1\cdot (a-y) \end{align}$$</p><p>$$\begin{align} \frac{\partial J}{\partial w_2} &amp;=\sum_{i=1}^N\frac{\partial J}{\partial z^{(i)}}\cdot \frac{\partial z^{(i)}}{\partial w_2} \\  &amp;= \frac{1}{m} \cdot x_2\cdot (a-y) \end{align}$$</p><p>$$\begin{align} \frac{\partial J}{\partial w_3} &amp;=\sum_{i=1}^N\frac{\partial J}{\partial z^{(i)}}\cdot \frac{\partial z^{(i)}}{\partial w_3} \\  &amp;= \frac{1}{m} \cdot x_3\cdot (a-y) \end{align}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations): // 迭代次数</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_sample): // 样本个数</span><br><span class="line">        z = w*x + b</span><br><span class="line">        a = sigmoid(z)</span><br><span class="line">        J = -[y*loga + (<span class="number">1</span>-y)*log(<span class="number">1</span>-a)]</span><br><span class="line">        dz = a - y</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(para_num): // 参数个数</span><br><span class="line">            dw_i += x_i*dz</span><br><span class="line">        b += dz_i</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(para_num): // 参数个数</span><br><span class="line">        w_i = w_i - alpha * dw_i</span><br><span class="line">    b = b - alpha * db</span><br></pre></td></tr></table></figure><p>可以看到，算法学习需要3重循环。</p><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>通过使用numpy数组及向量计算，我们可以减少循环次数，提高运算效率。通过定义<code>dw=[dw_1, dw_2, ..., dw_n]</code>和<code>w=[w_1, w_2, ..., w_n]</code>来进行向量化操作。</p><h4 id="一、参数向量化"><a href="#一、参数向量化" class="headerlink" title="一、参数向量化"></a>一、参数向量化</h4><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqorpbyfobj30et0g6dga.jpg" alt="Screen Shot 2018-04-24 at 23.11.18"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations): // 迭代次数</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_sample): // 样本个数</span><br><span class="line">        z = w*x + b</span><br><span class="line">        a = sigmoid(z)</span><br><span class="line">        J = -[y*loga + (<span class="number">1</span>-y)*log(<span class="number">1</span>-a)]</span><br><span class="line">        dz = a - y</span><br><span class="line">        dw += x_i*dz_i</span><br><span class="line">        b += dz_i</span><br><span class="line">w -= alpha*dw</span><br><span class="line">    b = b - alpha * db</span><br></pre></td></tr></table></figure><h4 id="二、样本向量化"><a href="#二、样本向量化" class="headerlink" title="二、样本向量化"></a>二、样本向量化</h4><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqtiqbhsp5j30qp0eeq3z.jpg" alt="Screen Shot 2018-04-29 at 14.26.21"></p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqtiqb8zayj30zf0drdgx.jpg" alt="Screen Shot 2018-04-29 at 14.37.57"></p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqtjjorp99j30f901mjrg.jpg" alt="Screen Shot 2018-04-29 at 15.06.40"></p><p>在上面的实现中，对于样本的处理，并没有转化为向量的操作，现在，我们研究一下如何转化为向量的操作。</p><ul><li><p>样本输入：$X=[x^{(1)},x^{(2)},…,x^{(m)}]$，其中$x^{(i)}$为列向量。</p></li><li><p>得到Z：</p><p>$$\begin{align} Z=[z^{(1)},z^{(2)},…,z^{(m)}] &amp;= w^T\cdot X+[b,b,…,b] \\  &amp;= [w^Tx^{(1)}+b,w^Tx^{(2)}+b,…,w^Tx^{(m)}+b]  \\ \end{align}$$</p><p><code>Z=np.dot(W.T, X)+b</code></p></li><li><p>$A=[\sigma(z^{(1)}),\sigma(z^{(2)}),…,\sigma(z^{(m)})]$</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(W.T, x) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A - Y</span><br><span class="line">dw = <span class="number">1</span>/m * X * dZ.T</span><br><span class="line">db = <span class="number">1</span>/m * np.sum(dZ)</span><br><span class="line">w = w - alpha * dw</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;神经网络由于其多层多神经元的特性，参数往往很多，这也导致了学习深度学习的时候会有太“复杂”的感觉。该文章整理出一些神经网络中重要的基础概念和符号表示系统。&lt;/p&gt;
&lt;h1 id=&quot;符号表示体系&quot;&gt;&lt;a href=&quot;#符号表示体系&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深层网络" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>集成学习 1-2：Adaboost</title>
    <link href="http://conghuai.me/2018/04/22/Adaboost/"/>
    <id>http://conghuai.me/2018/04/22/Adaboost/</id>
    <published>2018-04-22T03:02:07.000Z</published>
    <updated>2018-09-25T03:27:25.961Z</updated>
    
    <content type="html"><![CDATA[<p>提升方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数提升方法就是改变训练数据的概率分布，针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</p><p>这样，对提升方法来说，有两个问题需要回答：一是在每一轮如何改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。关于第1个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低哪些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。至于第2个问题， 即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中其较大的作用，减少分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</p><h1 id="Adaboost算法"><a href="#Adaboost算法" class="headerlink" title="Adaboost算法"></a>Adaboost算法</h1><p>Adaboost算法由以下步骤构成：</p><ol><li><p>初始化训练数据的权值分布</p><p>$$D_1=(w_{11},…,w_{1i},…,w_{1N}),w_{1i}=\frac{1}{N}$$，第一个下标表示boost迭代轮次，第二个下标表示样本编号。</p></li><li><p>对$m=1,2,…,M$迭代轮次，每一轮做如下操作</p><p>(a) 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器</p><p>$$G_m(x):X\rightarrow {-1, +1}$$</p><p>(b) 计算$G_m(x)$在训练数据集上的分类误差率</p><p>$$e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)$$</p><p>上式，表示的其实就是计算被$G_m(x)$模型误分类样本的权值之和。</p><p>(c) 计算$G_m(x)$在最终决策中所占的系数</p><p>$$\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}$$</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-05-24-080234.jpg" alt="Untitled"></p><p>从图像可以看出，$e_m$越大，权重越小；$e_m$越小，权重越大。</p><p>(d) 更新训练数据的权值分布：$D_{m+1}=(w_{m+1,1},…,w_{m+1,i},…,w_{m+1,N})$</p><p>$$\begin{split} w_{m+1,i}=\begin{cases} \frac{w_{mi}}{Z_m}e^{-\alpha_m}, &amp; \text{ $G_m(x_i)=y_i$ } \\ \frac{w_{mi}}{Z_m}e^{\alpha_m}, &amp; \text{$G_m(x_i)\neq y_i$}\end{cases}\end{split}$$</p><p>其中，$Z_m=\sum_{i=1}^Nw_{m,i}e^{-\alpha_my_iG_m(x_i)}$</p><p>将上面表达式写成一个，$w_{m+1,i}=\frac{w_{mi}}{Z_m}e^{-\alpha_my_iG_m(x_i)}$</p><p>我们发现，该样本下一轮的权值由三部分决定：</p><ul><li>$w_{m,i}$的权值，该权值越大，则$w_{m+1,i}$权值也应越大；</li><li>$\alpha_m$，上一轮分类器的权重；</li><li>$y_iG_m(x_i)$，预测是否正确，该值越大，如果正确的话，该值越大，则权重越小；如果不正确，该值越小，则权值越大。</li></ul></li><li><p>得到最终分类器</p><p>$$f(x) = \sum_{m=1}^M\alpha_mG_m(x)$$，即将每一轮的弱分类器进行组合。</p></li></ol><p>AdaBoost在迭代的时候，会期望样本权值越来越低。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航 统计学习方法</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;提升方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="ensemble" scheme="http://conghuai.me/tags/ensemble/"/>
    
  </entry>
  
  <entry>
    <title>Improved Iterative Scaling</title>
    <link href="http://conghuai.me/2018/04/19/Improved-Iterative-Scaling/"/>
    <id>http://conghuai.me/2018/04/19/Improved-Iterative-Scaling/</id>
    <published>2018-04-19T00:18:16.000Z</published>
    <updated>2018-09-24T15:26:23.838Z</updated>
    
    <content type="html"><![CDATA[<p>逻辑斯蒂回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。从最优化的观点看，这时的目标函数具有很好的性质。它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法。</p><h1 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h1><h2 id="第一次下界：-A-delta-w"><a href="#第一次下界：-A-delta-w" class="headerlink" title="第一次下界：$A(\delta|w)$"></a>第一次下界：$A(\delta|w)$</h2><p>改进的迭代尺度法(improved iterative scaling, IIS)是一种最大熵模型学习的最优化算法。</p><p>已知最大熵模型为</p><p>$$P_w(y|x)=\frac{1}{Z_w(x)}exp(\sum_{i=1}^nw_if_i(x,y))$$</p><p>其中，</p><p>$$Z_w(x)=\sum_yexp(\sum_{i=1}^nw_if_i(x,y))$$</p><p>对数似然函数为</p><p>$$L(w)=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\tilde{P}(x)logZ_w(x)$$</p><p>目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值$\hat{w}$</p><p><strong>IIS的想法</strong>是：假设最大熵模型当前的参数向量是$w=(w_1,w_2,…,w_n)^T$，我们希望找到一个新的参数向量$w+\delta =(w_1+\delta_1 ,w_2+\delta_2 ,…,w_n+\delta_n)$，使得模型的对数似然函数值增大，如果能有这样一种参数向量更新的方法：$\tau \rightarrow w+\delta$，那么就可以重复使用这一方法，直到找到对数似然函数的最大值。</p><p>对于给定的经验分布$\tilde{P}(x,y)$，模型参数从$w$到$w+\delta$，对数似然函数的改变量是</p><p>$$\begin{align} L(w+\delta) - L(w) &amp;=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n(w+\delta)_if_i(x,y)-\sum_x\tilde{P}(x)logZ_{w+\delta}(x) \\  &amp;- \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\tilde{P}(x)logZ_w(x) \\  &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)-\sum_x\tilde{P}(x)log\frac{Z_{w+\delta}(x)}{Z_w(x)}  \end{align}$$</p><p>利用不等式</p><p>$$-log\alpha \geq1-\alpha,\alpha&gt;0$$</p><blockquote><p>证明上述不等式：</p><p>令 g(x) = x - lnx -1</p><p>g’(x) = 1 - $\frac{1}{x}$ = 0 得 x = 1</p><p>g(x) $\geq $ g(1) = 0，因此，x - lnx - 1 $\geq$ 0</p></blockquote><p>我们可以得到，注意$Z_w(x)=\sum_yexp(\sum_{i=1}^nw_if_i(x,y))$</p><p>$$\begin{align} L(w+\delta) - L(w) &amp;\geq \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+\sum_x\tilde{P}(x)(1-\frac{Z_{w+\delta}(x)}{Z_w(x)})  \\ &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+\sum_x\tilde{P}(x)- \sum_x\tilde{P}(x)\frac{Z_{w+\delta}(x)}{Z_w(x)} \\ &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+\sum_x\tilde{P}(x)- \sum_x\tilde{P}(x)\frac{\sum_yexp(\sum_{i=1}^n(w_i+\delta_i)f_i(x,y))}{\sum_yexp(\sum_{i=1}^nw_if_i(x,y))} \\&amp;=  \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+\sum_x\tilde{P}(x)- \sum_x\tilde{P}(x)\sum_yexp\sum_{i=1}^n\delta_i f_i(x,y) \end{align} $$</p><p>我们再利用$\sum_x \tilde{P}(x)=1$和$\sum_yP_w(y|x)=1$对上式进行变形，其中前者在式子中出现，我们将其变为1；后者我们强行将其放到式子最后一项的乘法中。</p><p>$$\begin{align} L(w+\delta) - L(w) &amp;=  \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1- \sum_x\tilde{P}(x)\sum_yP_w(y|x)\sum_yexp\sum_{i=1}^n\delta_i f_i(x,y) \\&amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1- \sum_x\tilde{P}(x)\sum_yP_w(y|x)exp\sum_{i=1}^n\delta_i f_i(x,y)\end{align} $$</p><p>将上式右端记为</p><p>$$A(\delta|w)=  \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1- \sum_x\tilde{P}(x)\sum_yP_w(y|x)exp\sum_{i=1}^n\delta_i f_i(x,y)$$</p><p>于是有</p><p>$$L(w+\delta)-L(w)\geq A(\delta|w)$$</p><p>即$A(\delta|w)$是对数似然函数改变量的一个下界。</p><p>如果能找到适当的$\delta$使下界$A(\delta|w)$提高，那么对数似然函数也会提高。然而，函数$A(\delta|w)$中的$\delta$是一个向量，含有多个变量，不易同时优化。IIS试图一次只优化其中一个变量$\delta_i$，而固定其他变量$\delta_j$，$i\neq j$。</p><h2 id="第二次下界：-B-delta-w"><a href="#第二次下界：-B-delta-w" class="headerlink" title="第二次下界：$B(\delta|w)$"></a>第二次下界：$B(\delta|w)$</h2><p>为了达到上述的目的，IIS进一步降低下界$A(\delta|w)$，具体地，IIS引进一个量$f^\#(x,y)$</p><p>$$f^\#(x,y)=\sum_if_i(x,y)$$</p><p>因为$f_i$是二值函数，估$f^\#(x,y)$表示所有特征在$(x,y)$出现的次数，这样，改写$A(\delta|w)$为</p><p>$$A(\delta|w)=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x)exp(f^\#(x,y)\sum_{i=1}^n\frac{\delta_if_i(x,y)}{f^\#(x,y)})$$</p><p>利用指数函数的凸性以及对任意$i$，有$\frac{f_i(x,y)}{f^\#(x,y)}\geq 0$且$\sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}=1$，利用Jensen不等式得到</p><blockquote><p>Jensen不等式的有限形式</p><p>$$\varphi (\sum_{i=1}^ng(x_i)\lambda_i)\leq \sum_{i=1}^n\varphi(g(x_i))\lambda_i$$</p></blockquote><p>将$\varphi \leftarrow exp$，$g(x_i) \leftarrow \delta_if^\#(x,y)$，$\lambda_i \leftarrow \frac{f_i(x,y)}{f^\#(x,y)}$带入到Jensen不等式中，得</p><p>$$exp(\sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}\delta_if^\#(x,y))\leq \sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}exp(\delta_if^\#(x,y))$$</p><p>带入到$A(\delta|w)$式子中，得</p><p>$$A(\delta|w)\geq\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x) \sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}exp(\delta_if^\#(x,y))$$</p><p>记右端为</p><p>$$B(\delta|w)= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x) \sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}exp(\delta_if^\#(x,y))$$</p><p>于是得到</p><p>$$L(w+\delta) - L(w) \geq B(\delta|w)$$</p><p>这里，$B(\delta|w)$是对数似然函数该变量的一个新的下界，求$B(\delta|w)$对$\delta_i$的偏导数</p><p>$$\begin{align} \frac{\partial B(\delta|w)}{\partial \delta_i} &amp;= \sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_x\tilde{P}(x)\sum_yP_w(y|x)\frac{f_i(x,y)}{f^\#(x,y)}exp(\delta_if^\#(x,y))f^\#(x,y) \\ &amp;= \sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_x\tilde{P}(x)\sum_yP_w(y|x)f_i(x,y)exp(\delta_if^\#(x,y)) \end{align}$$</p><p>在上式中，除了$\delta_i$外不含有任何其他变量，令偏导数为0，得到</p><p>$$\sum_{x,y}\tilde{P}(x,y)f_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x)f_i(x,y)exp(\delta_if^\#(x,y)) = 0$$</p><p>于是，依次对$\delta_i$求解上述方程，最终可以得到$\delta$。</p><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>输入：特征函数$f_1,f_2,…,f_n;$经验分布$\hat{P}(X,Y)$，模型$P_w(y|x)$</p><p>输出：最优参数值$w_i^*$；最优模型$P_{w^*}$</p><p>(1) 对所有$i\in \{1,2,…,n\}$，取初值$w_i=0$</p><p>(2) 对每一$i\in \{1,2,…,n\}$ :</p><p>​    (a) 令$\delta_i$是方程</p><p>$$E_\hat{p}(f_i)=\sum_x\tilde{P}(x)\sum_yP_w(y|x)f_i(x,y)exp(\delta_if^\#(x,y))$$</p><p>的解，其中</p><p>$$f^\#(x,y) = \sum_{i=1}^nf_i(x,y)$$</p><p>​    (b) 更新$w_i$的值：$w_i \leftarrow w_i + \delta_i$</p><p>(3) 如果不是所有$w_i$都收敛，重复步骤(2)</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” <em>清华大学出版社, 北京</em> (2012).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;逻辑斯蒂回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。从最优化的观点看，这时的目标函数具有很好的性质。它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法。&lt;/
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型优化" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
      <category term="optimization" scheme="http://conghuai.me/tags/optimization/"/>
    
  </entry>
  
  <entry>
    <title>Expectation Maximization</title>
    <link href="http://conghuai.me/2018/04/18/Expectation-Maximization/"/>
    <id>http://conghuai.me/2018/04/18/Expectation-Maximization/</id>
    <published>2018-04-18T09:07:27.000Z</published>
    <updated>2018-09-24T15:26:18.005Z</updated>
    
    <content type="html"><![CDATA[<p>概率模型有时既含有观测变量，又含有隐变量或潜在变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。</p><h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><p>我们面对一个含有隐变量的概率模型，目标是最大化观测数据（不完全数据）$Y​$关于参数$\theta​$的对数似然函数，即极大化</p><p>$$\begin{align} L(\theta) &amp;= logP(Y|\theta)=log\sum_ZP(Y,Z|\theta) \\  &amp;= log(\sum_ZP(Y|Z,\theta)P(Z|\theta)) \end{align}$$</p><p>注意到这一极大化的主要困难是上述中有未观测数据并有包含和（或积分）的对数。</p><p>事实上，EM算法是通过迭代逐步近似极大化$L(\theta)$的，假设在第$i$次迭代后$\theta$的估计值是$\theta^{(i)}$，我们希望新的估计值$\theta$能使$L(\theta)$增加，即$L(\theta)&gt;L(\theta^{(i)})$，并逐步达到最大值，为此，考虑两者的差</p><p>$$L(\theta)-L(\theta^{(i)})=log(\sum_Z P(Y|Z,\theta)P(Z|\theta))-logP(Y|\theta^{(i)})$$</p><p>利用Jensen不等式，得到其下界</p><p>$$\begin{align} L(\theta) - L(\theta^{(i)}) &amp;= log(\sum_Z P(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})})-logP(Y|\theta^{(i)})\\  &amp;\geq \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-logP(Y|\theta^{(i)}) \\  &amp;= \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-\sum_ZP(Z|Y,\theta^{(i)})logP(Y|\theta^{(i)})  \\ &amp;= \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})} \end{align}$$</p><p>我们令</p><p>$\beta(\theta,\theta^{(i)})=L(\theta^{(i)}) + \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$</p><p>则</p><p>$$L(\theta)\geq \beta(\theta,\theta^{(i)})$$</p><p>因此，任何可以使$\beta(\theta,\theta^{(i)})$增大的$\theta$，也可以使$L(\theta)$增大，为了使$L(\theta)$有尽可能大的增长，选择$\theta^{(i+1)}$使$\beta(\theta, \theta^{(i)})$达到极大，即</p><p>$$\theta^{(i+1)}=argmax_\theta\beta(\theta,\theta^{(i)})$$</p><p>现在求$\theta^{(i+1)}$的表达式，省去对$\theta$的极大化而言是常数的项，有</p><p>$$\begin{align} \theta^{(i+1)} &amp;= argmax_\theta(L(\theta^{(i)}) + \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}) \\  &amp;= argmax_\theta \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}) \\  &amp;= argmax_\theta \sum_ZP(Z|Y,\theta^{(i)})logP(Y|Z,\theta)P(Z|\theta)-\sum_ZP(Z|Y,\theta^{(i)})P(Z|Y,\theta^{(i)})P(Y|\theta^{(i))})  \\ &amp;= argmax_\theta \sum_ZP(Z|Y,\theta^{(i)})logP(Y|Z,\theta)P(Z|\theta) \\ &amp;= argmax_\theta \sum_ZP(Z|Y,\theta^{(i)})logP(Y,Z|\theta) \\ &amp;= argmax_\theta\ Q(\theta,\theta^{(i)}) \end{align}$$</p><p>上述等价于EM算法的一次迭代，即求Q函数及其极大化。EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-095839.jpg" alt="Screen Shot 2018-04-18 at 17.52.20"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” <em>清华大学出版社, 北京</em> (2012).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;概率模型有时既含有观测变量，又含有隐变量或潜在变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型优化" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
      <category term="optimization" scheme="http://conghuai.me/tags/optimization/"/>
    
  </entry>
  
  <entry>
    <title>Optimization Summary</title>
    <link href="http://conghuai.me/2018/04/18/Optimization-for-Machine-Learning/"/>
    <id>http://conghuai.me/2018/04/18/Optimization-for-Machine-Learning/</id>
    <published>2018-04-18T01:47:44.000Z</published>
    <updated>2018-09-24T15:28:59.379Z</updated>
    
    <content type="html"><![CDATA[<p>优化问题在很多地方都会出现，如机器学习、数据挖掘、统计学等。最简单的优化问题的形式如下：</p><p>$$\begin{align} minimize\ \ f(x) \  with\ x\in R^d \ \end{align}$$</p><p>其中</p><ul><li>$x$是变量；</li><li>$f$是目标函数$f:R^d\rightarrow R$；</li></ul><ul><li>通常假设$f$是连续可导的。</li></ul><p>绝大部分的机器学习问题，最后都会转换为优化问题，例如：</p><ul><li><strong>Soft Linear SVM</strong></li></ul><p>$$argmin_w \sum_{i=1}^n||w||^2+C\sum_{i=1}^n\varepsilon _i$$</p><p>$$s.t. 1-y_ix_i^Tw \leq \varepsilon _i,\varepsilon \geq 0$$</p><ul><li><strong>Maximum Likelihood</strong></li></ul><p>$$argmax_\theta \sum_{i=1}^n logp_\theta(x_i)$$</p><ul><li><strong>K-means</strong></li></ul><p>$$argmin_{u_1,u_2,…,u_k}J(\mu)=\sum_{j=1}^k\sum_{i\in C_j}||x_i-\mu_j||^2$$</p><p>在机器学习中，通常我们需要先定义并建立优化的目标函数，然后用一个时间上较优的算法去求解该优化问题。目前，主要的优化方法有：</p><ul><li>Gradient Descent</li><li>Stochastic Gradient Descent(SGD)</li><li>Coordinate Descent</li></ul><p>这些优化方法的提出，其历史如下：</p><ul><li>1847: Cauchy proposes gradient descent</li><li>1950s: Linear Programs, soon followed by non-linear, SGD</li><li>1980s: General optimization, convergence theory</li><li>2005-today: Large scale optimization, convergence of SGD</li></ul><p>Coordinate Descent例子如下：</p><p>$$Goal : Find\ x^*\in R\ minimizing\ f(x)\ \ (Example: d= 2) $$</p><p><strong>Idea</strong>：Update one coordinate at a time, while keeping others fixed.</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090851.jpg" alt="Screen Shot 2018-04-18 at 10.19.46"></p><h1 id="凸优化理论"><a href="#凸优化理论" class="headerlink" title="凸优化理论"></a>凸优化理论</h1><h2 id="Convex-Sets"><a href="#Convex-Sets" class="headerlink" title="Convex Sets"></a>Convex Sets</h2><blockquote><p>A set C is <strong>convex</strong> if the line segment between any two points of C lies in C, i.e., if for any x,y $\in$ C and any $\lambda $ with $0 \leq \lambda\leq 1$,we have</p><p>$$\lambda x+(1-\lambda)y \in C$$</p></blockquote><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090842.jpg" alt="Screen Shot 2018-04-18 at 10.24.07"></p><h2 id="Convex-Functions"><a href="#Convex-Functions" class="headerlink" title="Convex Functions"></a>Convex Functions</h2><blockquote><p>Definition</p><p>A function $f: R^d \rightarrow R$ is convex if (i) dom(f) is a convex set and (ii) for all $x,y \in dom(f)$,and $\lambda$ with $0\leq \lambda \leq 1$, we have </p><p>$$f(\lambda x+(1-\lambda)y)\leq \lambda f(x)+(1-\lambda)f(y)$$</p></blockquote><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090846.jpg" alt="Screen Shot 2018-04-18 at 10.53.05"></p><p>常见的凸函数有：</p><h3 id="Linear-affine-functions"><a href="#Linear-affine-functions" class="headerlink" title="Linear/affine functions"></a>Linear/affine functions</h3><p>$$f(x)=b^Tx+c$$</p><h3 id="Quadratic-functions"><a href="#Quadratic-functions" class="headerlink" title="Quadratic functions"></a>Quadratic functions</h3><p>$$f(x)=\frac{1}{2}x^TAx+b^Tx+c$$</p><h3 id="Norms-如l1、l2范数"><a href="#Norms-如l1、l2范数" class="headerlink" title="Norms(如l1、l2范数)"></a>Norms(如l1、l2范数)</h3><p>$$||\alpha x+(1-\alpha)y||\leq ||\alpha x||+||(1-\alpha)y||=\alpha ||x||+(1-\alpha)||y||$$</p><h3 id="Composition-with-an-affine-function-f-Ax-b"><a href="#Composition-with-an-affine-function-f-Ax-b" class="headerlink" title="Composition with an affine function $f(Ax+b)$"></a>Composition with an affine function $f(Ax+b)$</h3><p>$$f(A(\alpha x+(1-\alpha)y)+b) =f(\alpha (Ax+b)+(1-\alpha)(Ay+b))\leq \alpha f(Ax+b)+(1-\alpha)f(Ay+b)$$</p><h3 id="Log-sum-exp"><a href="#Log-sum-exp" class="headerlink" title="Log-sum-exp"></a>Log-sum-exp</h3><p>$$f(x) = log(\sum_{i=1}^nexp(x_i))$$</p><p>在机器学习中， 有哪些损失函数是凸函数呢？</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090845.jpg" alt="Screen Shot 2018-04-18 at 11.12.36"></p><h3 id="SVM-loss"><a href="#SVM-loss" class="headerlink" title="SVM loss"></a>SVM loss</h3><p>$$f(w) = [1-y_ix_i^Tw]_+$$</p><h3 id="Binary-logistic-loss"><a href="#Binary-logistic-loss" class="headerlink" title="Binary logistic loss"></a>Binary logistic loss</h3><p>$$f(w)=log(1+exp(-y_ix_i^Tw))$$</p><h2 id="Convex-Optimization"><a href="#Convex-Optimization" class="headerlink" title="Convex Optimization"></a>Convex Optimization</h2><blockquote><p>An optimization problem is convex if its objective is a convex function, the inequality constrints $f_j$ are convex, and the equality constraints $h_j$ are affine</p></blockquote><p>$$minimize_x f_0(x)\ (Convex function)$$</p><p>$$s.t. f_i(x) \leq 0\ (Convex\ sets)$$</p><p>$$h_j(x) = 0\ (Affine)$$</p><p>凸优化的性质在于具有全局最优解。</p><h1 id="Unconstrained-optimization"><a href="#Unconstrained-optimization" class="headerlink" title="Unconstrained optimization"></a>Unconstrained optimization</h1><p>对于无约束的优化问题，我们通过会采用梯度下降的方法来求解。</p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqx25izb2bj31hc0zk4kl.jpg" alt="cost"></p><p>$$For\ t=1,…,T$$</p><p>$$x_{t+1}\leftarrow x_t-\eta \triangledown f(x_t)$$</p><p>其中，$\eta_t$称为学习率。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090850.jpg" alt="Screen Shot 2018-04-18 at 16.45.00"></p><h3 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient descent"></a>Batch gradient descent</h3><p>在整个训练数据集上计算梯度，然后更新参数：</p><p>$$\theta=\theta-\eta\cdot \triangledown _\theta J(\theta)$$</p><p>因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure><p>对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。</p><h3 id="Stochastic-Gradient-Descent（SGD）"><a href="#Stochastic-Gradient-Descent（SGD）" class="headerlink" title="Stochastic Gradient Descent（SGD）"></a>Stochastic Gradient Descent（SGD）</h3><p>Batch gradient descent通过训练完所有样本的损失后，才更新参数，总共的更新参数的次数为模型迭代次数。相反，随机梯度下降法（stochastic gradient descent, SGD）根据每一条个训练样本$x^{(i)}$和标签$y^{(i)}$更新参数：</p><p>$$\theta = \theta-\eta\cdot \triangledown_\theta J(\theta;x^{(i)};y^{(i)}) $$</p><p>对于大数据集，因为批梯度下降法在每一个参数更新之前，会对相似的样本计算梯度，所以在计算过程中会有冗余。而SGD在每一次更新中只执行一次，从而消除了冗余。因而，通常SGD的运行速度更快，同时，可以用于在线学习。SGD以高方差频繁地更新，导致目标函数出现如图1所示的剧烈波动。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090848.jpg" alt="20170410195019493"></p><p>与批梯度下降法的收敛会使得损失函数陷入局部最小相比，由于SGD的波动性，一方面，波动性使得SGD可以跳到新的和潜在更好的局部最优。另一方面，这使得最终收敛到特定最小值的过程变得复杂，因为SGD会一直持续波动。然而，已经证明当我们缓慢减小学习率，SGD与批梯度下降法具有相同的收敛行为，对于非凸优化和凸优化，可以分别收敛到局部最小值和全局最小值。与批梯度下降的代码相比，SGD的代码片段仅仅是在对训练样本的遍历和利用每一条样本计算梯度的过程中增加一层循环。注意，如6.1节中的解释，在每一次循环中，我们打乱训练样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure><p>实现SGD需要三层循环：</p><ol><li>循环模型迭代次数</li><li>循环m个训练样本</li><li>循环神经网络中的每一层</li></ol><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqx25mlkrlj311e0a2dhn.jpg" alt="kiank_sgd"></p><h3 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini-Batch Gradient Descent"></a>Mini-Batch Gradient Descent</h3><p>Mini-Batch Gradient Descent方法介于Batch Gradient Descent和Stochastic Gradient Descent之间，每训练完一个Mini-Batch后就更新参数，Mini-Batch的大小通常取2的整数幂，如64，128，256，512。</p><p>$$\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})$$</p><p>在用Mini-Batch Gradient Descent学习时，首先需要构造mini-batches训练集，分为如下两个步骤来构建：</p><h4 id="1-Shuffle"><a href="#1-Shuffle" class="headerlink" title="1. Shuffle"></a>1. Shuffle</h4><p>创建一个随机排序的训练数据，需要注意的是，$X​$和$Y​$以同样的方式被打乱。<img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqx25kj8a6j31220l0goc.jpg" alt="kiank_shuffle"></p><h4 id="2-Partition"><a href="#2-Partition" class="headerlink" title="2. Partition"></a>2. Partition</h4><p>第二步就是对随机打乱的训练数据集和测试数据集进行划分，这里需要注意的是，通常来说数据集是不能被整分的，最后一份mini-batch的样本通常来说比较小。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqx25nhnl6j30v40j0tax.jpg" alt="kiank_partition"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size = <span class="number">64</span>, seed = <span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a list of random minibatches from (X, Y)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(seed)            <span class="comment"># To make your "random" minibatches the same as ours</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                  <span class="comment"># number of training examples</span></span><br><span class="line">    mini_batches = []</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 1: Shuffle (X, Y)</span></span><br><span class="line">    permutation = list(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="comment"># number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+<span class="number">1</span>)*mini_batch_size]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:, mini_batch_size*int(m/mini_batch_size):]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, mini_batch_size*int(m/mini_batch_size):]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure><p>算法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">  np.random.shuffle(data)</span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> get_batches(data, batch_size=<span class="number">50</span>):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, batch, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><h2 id="Gradient-descent-optimization-algorithms"><a href="#Gradient-descent-optimization-algorithms" class="headerlink" title="Gradient descent optimization algorithms"></a>Gradient descent optimization algorithms</h2><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>有一种算法叫做动量（Momentum）或者叫动量梯度下降算法，它几乎总会比标准的梯度下降算法更快，其主要思想就是计算梯度的指数加权平均，然后使用这个梯度来更新权重。</p><p>$$\begin{align}\begin{split}v_t &amp;= \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \  \theta &amp;= \theta - v_t\end{split}\end{align}$$</p><p>通常来说，参数$\gamma$设置为0.9。</p><h3 id="Nesterov-accelerated-gradient"><a href="#Nesterov-accelerated-gradient" class="headerlink" title="Nesterov accelerated gradient"></a>Nesterov accelerated gradient</h3><p>$$\begin{align}\begin{split}v_t &amp;= \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} ) \  \theta &amp;= \theta - v_t\end{split}\end{align}$$</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqx25lji6tj30ro07zjs4.jpg" alt="nesterov_update_vector"></p><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Adagrad是一种基于梯度的优化算法，它可以做到这一点：它将学习速率与参数相适应，对频繁参数进行较少的更新和较小的更新。</p><p>$$g_{t, i} = \nabla_\theta J( \theta_{t, i} )$$</p><p>SGD在步骤$t$更新每个参数$\theta_i$：</p><p>$$\theta_{t+1, i} = \theta_{t, i} - \eta \cdot g_{t, i}$$</p><p>Adagrad基于之前的梯度值修改通常的学习率$\eta$，</p><p>$$\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}$$</p><h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>$$\begin{align}\begin{split}E[g^2]<em>t &amp;= 0.9 E[g^2]</em>{t-1} + 0.1 g^2_t \  \theta_{t+1} &amp;= \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]<em>t + \epsilon}} g</em>{t}\end{split}\end{align}$$</p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adam是一种非常有效的优化算法，它结合了RMSProp和Momentum的有点。</p><p><strong>How does Adam work?</strong></p><ol><li>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). </li><li>It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). </li><li>It updates parameters in a direction based on combining information from “1” and “2”.</li></ol><p>The update rule is, for $l = 1, …, L$: </p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqx25jnx2nj308005i0t0.jpg" alt="Screen Shot 2018-05-02 at 16.06.29"></p><p>where:</p><ul><li>t counts the number of steps taken of Adam </li><li>L is the number of layers</li><li>$\beta_1$ and $\beta_2$ are hyperparameters that control the two exponentially weighted averages. </li><li>$\alpha$ is the learning rate</li><li>$\varepsilon$ is a very small number to avoid dividing by zero</li></ul><p>As usual, we will store all parameters in the <code>parameters</code> dictionary </p><h2 id="Newton’s-method"><a href="#Newton’s-method" class="headerlink" title="Newton’s method"></a>Newton’s method</h2><h1 id="Constrained-optimization"><a href="#Constrained-optimization" class="headerlink" title="Constrained optimization"></a>Constrained optimization</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. coursera deeplearning.ai</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;优化问题在很多地方都会出现，如机器学习、数据挖掘、统计学等。最简单的优化问题的形式如下：&lt;/p&gt;
&lt;p&gt;$$\begin{align} minimize\ \ f(x) \  with\ x\in R^d \ \end{align}$$&lt;/p&gt;
&lt;p&gt;其中&lt;/p&gt;
&lt;ul&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型优化" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Maximum Entropy Model</title>
    <link href="http://conghuai.me/2018/04/16/Maximum-Entropy-Model/"/>
    <id>http://conghuai.me/2018/04/16/Maximum-Entropy-Model/</id>
    <published>2018-04-16T07:56:56.000Z</published>
    <updated>2018-09-24T15:27:42.112Z</updated>
    
    <content type="html"><![CDATA[<p>最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，熵最大的模型是最好的模型，通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。</p><p>直观地，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息的情况下，那些不确定的部分都是“等可能的”。</p><p>最大熵原理是统计学习的一个一般原理，将它应用到分类得到最大熵模型。我们先从一个例子说明一下最大熵模型的特点：</p><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>假设我们的训练样本中有10个样本，每个样本中有两个变量$a、b$，其中$a\in \{x, y\}$，$b\in \{0,1\}$，观测到的训练样本如下：</p><table><thead><tr><th style="text-align:center">a,b</th></tr></thead><tbody><tr><td style="text-align:center">(x, 1)</td></tr><tr><td style="text-align:center">(y, 1)</td></tr><tr><td style="text-align:center">(x, 0)</td></tr><tr><td style="text-align:center">(y ,1)</td></tr><tr><td style="text-align:center">(y, 0)</td></tr><tr><td style="text-align:center">(x, 0)</td></tr><tr><td style="text-align:center">(x, 0)</td></tr><tr><td style="text-align:center">(x, 1)</td></tr><tr><td style="text-align:center">(y, 0)</td></tr><tr><td style="text-align:center">(x, 0)</td></tr></tbody></table><p>用二维表刻画联合概率分布来表示：</p><table><thead><tr><th style="text-align:center">a\b</th><th style="text-align:center">0</th><th style="text-align:center">1</th></tr></thead><tbody><tr><td style="text-align:center">x</td><td style="text-align:center">0.4</td><td style="text-align:center">0.2</td></tr><tr><td style="text-align:center">y</td><td style="text-align:center">0.2</td><td style="text-align:center">0.2</td></tr></tbody></table><p>上面这个表刻画的就是我们在训练样本上的概率分布，对于这个概率分布，我们是绝对相信吗？如果是的话，我们会用极大似然的方式来估计出模型的参数。但是，在有些场景下，对于通过训练样本得到的经验概率分布，我们有时候只关心某些特征，我们希望得到的模型对于我们关心的特征满足经验概率分布，对于我们不关心的特征，只需要达到最大熵要求即可。但是，该如何刻画我们的特征呢？</p><p>通常来说，我们通过定义特征函数来刻画我们关心的特征：</p><p>$$\begin{split}f(a,b)=\begin{cases} 1, &amp; \text{我们关心的条件} \\ 0, &amp; \text{其他}\end{cases}\end{split}$$</p><p>可以看出来，根据不同的条件，我们可以定义出多个不同的特征函数：$f_j:\varepsilon\rightarrow \{0,1\} $。例如，我们关心的是<code>b=0</code>这一列的特征，我们就可以定义特征函数为：</p><p>$$\begin{split}f(a,b)=\begin{cases} 1, &amp; \text{b=0} \\ 0, &amp; \text{其他}\end{cases}\end{split}$$</p><p>有了特征函数的定义，我们现在需要保证模型在特征函数上的期望值$E_pf$和经验概率分布在特征函数上的期望值$E_{\tilde{p}}f$是一致的，即：</p><p>$$E_{\tilde{p}}f = E_pf$$</p><p>根据我们上面定义的特征函数，得：</p><p>$$E_{\tilde{p}}f = \sum_{a,b}\tilde{p}(a,b)f(a,b)= 0.4 + 0.2 = 0.6$$</p><p>即，我们得到经验概率分布在该特征函数上的期望值为：0.6，所以，我们的模型需满足：$E_pf=p(x,0) + p(y,0) = p(x)\cdot p(0|x) + p(y)\cdot p(0|y)= 0.6$</p><p>那么，对于我们不关心的特征呢？对于这些特征，我们只需要保证熵最大即可。</p><h1 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h1><p>有了上面的例子，现在我们形式化给出最大熵模型的定义：</p><p>假设分类模型是一个条件概率分布$P(Y|X)$，$x\in X \subseteq R^n$表示输入，$y\in Y$表示输出，$X$和$Y$分别是输入和输出的集合，这个模型表示的是对于给定的输入$X$，以条件概率$P(Y|X)$输出$Y$。</p><p>首先考虑模型应该满足的条件，给定训练数据集，可以确定联合分布$P(X,Y)$的经验分布和边缘分布$P(X)$的经验分布，分别以$\tilde{P}(X,Y)$和$\tilde{P}(X)$表示。这里，</p><p>$$\tilde{P}(X=x,Y=y)=\frac{v(X=x,Y=y)}{N}$$</p><p>$$\tilde{P}(X=x)=\frac{v(X=x)}{N}$$</p><p>其中，$v(X=x, Y=y)$表示训练数据中样本$(x,y)$出现的频数，$v(X=x)$表示训练数据中输入$x$出现的频数，$N$表示训练样本容量。</p><p>用特征函数$f(x,y)$描述输入x和输出y之间的某一个事实，其定义是：</p><p>$$\begin{split}f(a,b)=\begin{cases} 1, &amp; \text{x与y满足某一事实} \\ 0, &amp; \text{否则}\end{cases}\end{split}$$</p><p>它是一个二值函数，当x和y满足这个事实时取值为1，否则取值为0。</p><p>特征函数$f(x,y)$关于经验分布$\tilde{P}(X,Y)$的期望值，用$E_\tilde{p}(f)$表示：</p><p>$$E_\tilde{p}(f) = \sum_{x,y}\tilde{P}(x,y)f(x,y)$$</p><p>特征函数$f(x,y)$关于模型$P(Y|X)$与经验分布$\tilde{P}(X)$的期望值，用$E_p(f)$表示：</p><p>$$E_p(f)=\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)$$</p><p>如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即：</p><p>$$E_p(f) = E_\tilde{p}(f)$$</p><p>或</p><p>$$\sum_{x,y}\tilde{P}(x,y)f(x,y) = \sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)$$</p><p>上式就是模型的约束条件，如果有n个特征函数$f_i(x,y),i=1,2,…,n$，那么就有n个约束条件。</p><h2 id="定义最大熵模型"><a href="#定义最大熵模型" class="headerlink" title="定义最大熵模型"></a>定义最大熵模型</h2><p>假设满足所有约束条件的模型集合为：</p><p>$$C\equiv \{p\in P|E_p(f_i)=E_{\tilde{p}}(f_i),i=1,2,…,n\}$$</p><p>定义在条件概率分布$P(Y|X)$上的条件熵为：</p><p>$$H(P) = -\sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x)$$</p><p>则模型集合$C$中条件熵$H(P)$最大的模型称为最大熵模型。</p><h1 id="最大熵模型的学习"><a href="#最大熵模型的学习" class="headerlink" title="最大熵模型的学习"></a>最大熵模型的学习</h1><p>对于给定的训练数据集$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$以及特征函数$f_i(x,y)$，$i=1,2,…,n$，最大熵模型的学习等价于约束最优化问题：</p><p>$$max_{P\in C}\ H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x)$$</p><p>$$s.t. E_p(f) = E_\tilde{p}(f), i=1,2,…,n$$</p><p>$$\sum_yP(y|x)=1$$</p><p>改写成等价的求最小值的问题：</p><p>$$min_{P\in C}\ -H(P)=\sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x)$$</p><p>$$s.t. E_p(f) = E_\tilde{p}(f), i=1,2,…,n$$</p><p>$$\sum_yP(y|x)=1$$</p><h2 id="无约束最优化的对偶问题"><a href="#无约束最优化的对偶问题" class="headerlink" title="无约束最优化的对偶问题"></a>无约束最优化的对偶问题</h2><p>将约束最优化的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题求解原始问题。首先，引进拉格朗日乘子$w_0,w_1,w_2,…,w_n$，定义拉格朗日函数$L(P,w)$:</p><p>$$\begin{align} L(P,w) &amp;=-H(P) + w_0(1-\sum_yP(y|x)) + \sum_{i=1}^nw_i(E_\tilde{p}(f)-E_p(f))\\  &amp;= \sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x) \\ &amp;+ w_0(1-\sum_yP(y|x))+ \sum_{i=1}^nw_i(\sum_{x,y}\tilde{P}(x,y)f(x,y)-\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)) \end{align}$$</p><p>最优化的原始问题是：</p><p>$$min_{P\in C}max_wL(P,w)$$</p><p>对偶问题是：</p><p>$$max_wmin_{P\in C}L(P,w)$$</p><p>由于拉格朗日函数$L(P,w)$是$P$的凸函数，原始问题和对偶问题的解是等价的。</p><h3 id="求解-min-P-in-C-L-P-w"><a href="#求解-min-P-in-C-L-P-w" class="headerlink" title="求解$min_{P\in C}L(P,w)$"></a>求解$min_{P\in C}L(P,w)$</h3><p>$min_{P\in C}L(P,w)$是$w$的函数，将其记作：</p><p>$$\psi (x)=min_{P\in C}L(P,w)=L(P_w,w)$$</p><p>$\psi (x)$称为对偶函数，同时将其解记作：</p><p>$$P_w=argmin_{p\in C}L(P,w)=P_w(y|x)$$</p><p>具体地，求$L(P,w)$对$P(y|x)$的偏导数</p><p>$$\begin{align} \frac{\partial{L(P,w)}}{\partial{P(y|x)}} &amp;=\sum_{x,y}\tilde{P}(x)(logP(y|x)+1)-\sum_yw_0-\sum_{x,y}(\tilde{P}(x)\sum_{i=1}^nw_if_i(x,y))\\  &amp;= \sum_{x,y}\tilde{P}(x)(logP(y|x)+1-w_0-\sum_{i=1}^nw_if_i(x,y)) \end{align}$$</p><p>令偏导数等于0，在$\tilde{P}&gt;0$的情况下，解得：</p><p>$$\begin{align} P(y|x) &amp;=e^{\sum_{i=1}^ww_if_i(x,y)+w_0-1}\\  &amp;= \frac{e^{\sum_{i=1}^ww_if_i(x,y)}}{e^{1-w_0}} \\  &amp;= \frac{e^{\sum_{i=1}^ww_if_i(x,y)}}{Z(x)} \end{align}$$</p><p>由于$\sum_yP(y|x)=1$，得</p><p>$$Z_w(x)=e^{1-w_0}=\sum_ye^{\sum_{i=1}^nw_if_i(x,y)}$$</p><p>其中：</p><ul><li>$Z_w(x)$称为规划化因子；</li><li>$f_i(x,y)$是特征函数；</li><li>$w_i$是特征的权值；</li><li>$w$是最大熵模型中的参数向量。</li></ul><h3 id="求解-max-w-psi-x"><a href="#求解-max-w-psi-x" class="headerlink" title="求解$max_w\psi (x)$"></a>求解$max_w\psi (x)$</h3><p>之后，求解对偶问题外部的极大化问题</p><p>$$max_w\psi (x)$$</p><p>将其解记为$w^*$，即</p><p>$$w^*=argmax_w\psi (x)$$</p><p>这就是说，可以应用最优化算法求对偶函数$\psi (x)$的极大化，得到$w^*$，用来表示$P^*$，这里，$P^*=P_{w^*}=P_{w^*}(y|x)$是学习到的最优化模型（最大熵模型）。</p><h1 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h1><p>下面证明对偶函数的极大化等价于最大熵模型的极大似然估计。</p><h2 id="对数似然函数"><a href="#对数似然函数" class="headerlink" title="对数似然函数"></a>对数似然函数</h2><p>已知训练数据的经验概率分布$\tilde{P}(X,Y)$，条件概率分布$P(Y|X)$的对数似然函数表示为：</p><p>$$L_{\tilde{P}}(P_w)=log\prod_{x,y}P(y|x)^{\tilde{P}(x,y)}=\sum_{x,y}\tilde{P}(x,y)logP(y|x)$$</p><p>当条件概率分布$P(y|x)$是最大熵模型时，对数似然函数$L_{\tilde{p}}(P_w)$为：</p><p>$$\begin{align}L_{\tilde{p}}(P_w) &amp;=\sum_{x,y}\tilde{P}(x,y)logP(y|x) \\  &amp;=\sum_{x,y}\tilde{P}(x,y)log[\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{Z_w(x)}]\\  &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\tilde{P}(x)logZ_w(x) \end{align}$$</p><h2 id="对偶函数"><a href="#对偶函数" class="headerlink" title="对偶函数"></a>对偶函数</h2><p>$$\begin{align}\psi (x) &amp;=\sum_{x,y}\tilde{P}(x)P_w(y|x)logP(y|x)+\sum_{i=1}^nw_i(\sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_{x,y}\tilde{P}(x)P_w(y|x)f_i(x,y)) \\  &amp;=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)+\sum_{x,y}\tilde{P}(x)P_w(y|x)(logP_w(y|x)-\sum_{i=1}^nw_if_i(x,y))\\  &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)+\sum_{x,y}\tilde{P}(x)P_w(y|x)logZ_w(x)\\  &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)+\sum_{x}\tilde{P}(x)logZ_w(x) \end{align}$$</p><p>我们发现，</p><p>$$\psi (x) = L_{\tilde{p}}(P_w)$$</p><p>既然对偶函数等价于对数似然函数，于是证明了最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计。</p><h1 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h1><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-16-125513.jpg" alt="Screen Shot 2018-04-16 at 20.53.37"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-16-125507.jpg" alt="Screen Shot 2018-04-16 at 20.54.06"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-16-125510.jpg" alt="Screen Shot 2018-04-16 at 20.54.44"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” 清华大学出版社, 北京 (2012).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，熵最大的模型是最好的模型，通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。&lt;/p&gt;
&lt;p&gt;直观地，最大熵原理认为要
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="algorithm" scheme="http://conghuai.me/tags/algorithm/"/>
    
      <category term="entropy" scheme="http://conghuai.me/tags/entropy/"/>
    
  </entry>
  
  <entry>
    <title>Mean Squared Error</title>
    <link href="http://conghuai.me/2018/04/14/Mean-Squared-Error/"/>
    <id>http://conghuai.me/2018/04/14/Mean-Squared-Error/</id>
    <published>2018-04-14T09:10:01.000Z</published>
    <updated>2018-09-24T15:27:54.608Z</updated>
    
    <content type="html"><![CDATA[<p>数理统计中均方误差是指参数估计值与参数值之差平方的期望值，记为MSE。MSE是衡量“平均误差”的一种较方便的方法，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。</p><h1 id="SSE（和方差）"><a href="#SSE（和方差）" class="headerlink" title="SSE（和方差）"></a>SSE（和方差）</h1><p>在统计学中，该参数计算的是拟合数据和原始对应点的误差的平方和，计算公式为：</p><p>$$SSE =\sum_{i=1}^mw_i(y_i-\hat{y_i})^2$$</p><p>其中$y_i$是真实数据，$\hat{y_i}$是拟合的数据，$w_i&gt;0$，从这里可以看出SSE接近于0，说明模型选择和拟合更好，数据预测也越成功。</p><h1 id="MSE（均方方差）"><a href="#MSE（均方方差）" class="headerlink" title="MSE（均方方差）"></a>MSE（均方方差）</h1><p>该统计参数是预测数据和原始数据对应点误差的平方和的均值，也就是$\frac{SSE}{n}$，和SSE没有太大的区别，计算公式为：</p><p>$$MSE=\frac{SSE}{n}=\frac{1}{n}\sum_{i=1}^mw_i(y_i-\hat{y_i})^2$$</p><p>其中，n为样本的个数。</p><h1 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h1><p>该统计参数，也叫回归系统的拟合标准差，是MSE的平方根，计算公式为：</p><p>$$RMSE=\sqrt{MSE}=\sqrt{\frac{SSE}{n}}=\sqrt{\frac{1}{n}\sum_{i=1}^mw_i(y_i-\hat{y_i})^2}$$</p><h1 id="Mean-Squared-Loss的概率解释"><a href="#Mean-Squared-Loss的概率解释" class="headerlink" title="Mean-Squared Loss的概率解释"></a>Mean-Squared Loss的概率解释</h1><p>假设我们的模型是二维平面的线性回归模型：$h_{\theta}(x_i)=\theta_0+\theta_1x$，对于这个模型，我们定义损失函数为MSE，将得到如下的表达式：</p><p>$$J = \frac{1}{N}\sum_{i=1}^N(y_i-h_{\theta}(x_i))^2$$</p><p>下面我们试着通过概率的角度，推导出上述的MSE损失函数表达式。</p><p>在线性回归模型中，我们最终希望对于输入$X$进行线性组合得到值Y，考虑到输入带有噪声的情况的表达式如下：</p><p>$$Y=\theta_0+\theta_1x+\eta$$</p><p>为了使模型更合理，我们假设$\eta$服从均值为0，方差为1的高斯分布，即$\eta\sim N(0,1)$。所以有：</p><p>$$E[Y]=E[\theta_0+\theta_1x+\eta]=\theta_0+\theta_1x$$</p><p>$$Var[Y]=Var[\theta_0+\theta_1x+\eta]=1$$</p><p>所以，Y服从均值为$\theta_0+\theta_1x$，方差为1的高斯分布，则样本点$(x_i,y_i)$的概率为：</p><p>$$p(y_i|x_i)=e^{-\frac{(y_i-(\theta_0+\theta_1x_i))^2}{2}}$$</p><p>有了单个样本的概率，我们就可以计算样本集的似然概率，我们假设每个样本是独立的：</p><p>$$L(x,y)=\prod_{i=1}^Ne^{-\frac{(y_i-(\theta_0+\theta_1x_i))^2}{2}}$$</p><p>对似然函数取对数，得到对数似然函数：</p><p>$$l(x,y)=-\frac{1}{2}\sum_{i=1}^N(y_i-(\theta_0+\theta_1x_i))^2$$</p><p>这个对数似然函数的形式和我们的MSE损失函数的定义是一样的。所以，使用MSE损失函数意味着，我们假设我们的模型是对噪声的输入做估计，该噪声服从高斯分布。</p><h1 id="损失函数效果"><a href="#损失函数效果" class="headerlink" title="损失函数效果"></a>损失函数效果</h1><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>使用MSE的一个缺点就是其偏导值在输出概率值接近0或者接近1的时候非常小，这可能会造成模型刚开始训练时，偏导值几乎消失。</p><p>假设我们的MSE损失函数为：$J = \frac{1}{2}(y_i - \hat{y_i})^2$，偏导为：$\frac{dJ}{dW} = (y_i - \hat{y_i})\sigma’(Wx_i + b)x_i$，其中$\sigma’(Wx_i + b)$为$\sigma(Wx_i + b)(1 - \sigma(Wx_i + b))$。可以看出来，在$\sigma(Wx_i + b)$值接近0或者1的时候，$\frac{dJ}{dW}$的值都会接近于0，其函数图像如下：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-141825.jpg" alt="sigmoid_and_derivative_plot"></p><p>这导致模型在一开始学习的时候速率非常慢，而使用交叉熵作为损失函数则不会导致这样的情况发生。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="http://rohanvarma.me/Loss-Functions/" target="_blank" rel="noopener">Picking Loss Functions - A comparison between MSE, Cross Entropy, and Hinge Loss</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;数理统计中均方误差是指参数估计值与参数值之差平方的期望值，记为MSE。MSE是衡量“平均误差”的一种较方便的方法，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。&lt;/p&gt;
&lt;h1 id=&quot;SSE（和方差）&quot;&gt;&lt;a href=&quot;#SSE
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="损失函数" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Regularization</title>
    <link href="http://conghuai.me/2018/04/12/Regularization/"/>
    <id>http://conghuai.me/2018/04/12/Regularization/</id>
    <published>2018-04-12T14:37:46.000Z</published>
    <updated>2018-09-24T15:29:40.931Z</updated>
    
    <content type="html"><![CDATA[<h1 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h1><p>过拟合是机器学习中一个比较常见的问题，而正则化是解决模型过拟合的一种手段。我们先看一下范数的定义：</p><blockquote><p>Going a bit further, we define $||x||_p$ as a “p-norm”. Given $x$, a vector with $i$ components, a p-norm is defined as:</p><p>$$|| x ||_p = \left(\sum_i |x_i|^p\right)^{1/p}$$</p><p>The simplest norm conceptually is Euclidean distance. This is what we typically think of as distance between two points in space:</p><p>$$|| x ||_2 = \sqrt{\left(\sum_i x_i^2\right)} = \sqrt{x_1^2 + x_2^2 + \ldots + x_i^2}$$</p><p>Another common norm is taxicab distance, which is the 1-norm:</p><p>$$|| x ||_1 = \sum_i |x_i| = |x_1| + |x_2| + \ldots + |x_i|$$</p><p>Taxicab distance is so-called because it emulates moving between two points as though you are moving through the streets of Manhattan in a taxi cab. Instead of measuring the distance “as the crow flies” it measures the right-angle distance between two points:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-15-114416.jpg" alt="1200px-Manhattan_distance.svg"></p></blockquote><p>正则化（Regularization）是机器学习中一种常用的技术，其主要目的是控制模型复杂度，减小过拟合。最基本的正则化方法是在原目标（代价）函数 中添加惩罚项，对复杂度高的模型进行“惩罚”。其数学表达形式为：</p><p>$$\tilde{J}(w;X,y)=J(w;X,y)+\alpha \Omega (w)$$</p><p>式中：</p><ul><li>$X，y$为训练样本和相应标签；</li><li>$w$为权重系数向量；</li><li>$J()$为目标函数；</li><li>$\Omega(w)$为惩罚项，即模型“规模”的某种度量；</li><li>$\alpha$控制正则化强弱。</li></ul><p>不同的$\Omega$函数对权重$w$的最优解有不同的偏好，因此会产生不同的正则化效果，最常用的$\Omega$函数有两种，即$l_1$范数和$l_2$范数，称为$l_1$正则化和$l_2$正则化：</p><p>$$ l_1: \Omega (w) = || w ||_1 = \sum_{i=1}^k |w_i| $$</p><p>$$ l_2: \Omega (w) = || w ||_2 = \sqrt{\sum_{i=1}^k w_i^2} $$</p><p>带有L1正则化的回归模型通常被称为<strong>Lasso Regression</strong>，带有L2正则化的回归模型通常被称为<strong>Ridge Regression</strong>。</p><p>L1正则化和L2正则化主要的区别在于，L1正比于参数的绝对值，而L2正比于参数的平方。这导致了两种正则化方式会产生不同的效果。</p><h1 id="公式来源分析"><a href="#公式来源分析" class="headerlink" title="公式来源分析"></a>公式来源分析</h1><h2 id="基于约束条件的最优化"><a href="#基于约束条件的最优化" class="headerlink" title="基于约束条件的最优化"></a>基于约束条件的最优化</h2><p>对于模型权重系数$w$求解释通过最小化目标函数实现的，即求解：</p><p>$$min_wJ(w;X,y)$$</p><p>通常情况下，模型复杂度与系数$w$的个数成线性关系：即$w$数量越多，模型越复杂。因此，为了限制模型的复杂度，很自然的想法就是减少系数$w$的个数，即让$w$向量中一些元素为0或者说限制$w$中非零元素的数量。因此，我们可以在原优化问题中加入一个约束条件：</p><p>$$min_wJ(w;X,y),\ s.t. ||w||_0 \leq C$$</p><p>式中，$||\cdot||_0$范数表示向量中非零元素的个数，但由于该问题是一个NP问题，不易求解，为此我们可以放松一下约束条件，为了达到近似效果，我们不严格要求某些权重$w$为0，而是要求权重$w$应接近于0，即尽量小。从而可用$l_1、l_2$范数来近似$l_0$范数，即：</p><p>$$min_wJ(w;X,y),\ s.t. ||w||_1 \leq C$$ 或</p><p>$$min_wJ(w;X,y),\ s.t. ||w||_2^2 \leq C$$（为了后续方便处理，对$||w||_2$进行平方）</p><p>利用拉格朗日算子法，我们可将上述带约束条件的最优化问题转换为不带约束项的优化问题，构建拉格朗日函数：</p><p>$$min_wJ(w;X,y) + \alpha^*||w||_1$$或</p><p>$$min_wJ(w;X,y) + \alpha^*||w||_2^2$$</p><p>因此，我们得到了对$l_1、l_2$正则化的第一种理解：</p><ul><li>$l_1$正则化等价于在原优化目标函数中增加约束条件$||w||_1 \leq C$</li><li>$l_2$正则化等价于在原优化目标函数中增加约束条件$||w||_2^2 \leq C$</li></ul><h2 id="最大后验概率估计"><a href="#最大后验概率估计" class="headerlink" title="最大后验概率估计"></a>最大后验概率估计</h2><p>在最大似然估计中，假设权重$w$是未知的参数，从而求得对数似然函数：</p><p>$$l(w)=log[P(y|X;w)]=log[\prod_iP(y^i|x^i;w)]$$</p><p>通过假设$y^i$的不同概率分布，即可得到不同的模型，例如若假设$y^i\sim N(w^Tx^i, \sigma^2)$的高斯分布，则有：</p><p>$$l(w)=log[\prod_i\frac{1}{\sqrt {2\pi}\sigma}e^{-\frac{(y^i-w^Tx^i)^2}{2\sigma^2}}]=-\frac{1}{2\sigma^2}\sum_i(y_i-w^Tx^i)^2+C$$</p><p>可令$J(w;X,y) = -l(w)$</p><p>在最大后验概率估计中，则将权重$w$看做随机变量，也具有某种分布，从而有：</p><p>$p(w|X,y)=\frac{P(w,X,y)}{P(X,y)}=\frac{P(X,y|w)P(w)}{P(X,y)}\propto P(y|X,w)P(w)$</p><p>对上述取对数有：</p><p>$$MAP = logP(y|X,w)P(w) = logP(y|X,w)+logP(w)$$</p><p>可以看到，后验概率在似然函数的基础上增加一项$logP(w)$，$P(w)$的意义是对权重系数$w$的概率分布的先验假设， 在收集到训练样本${X,y}$后，则可根据$w$在${X,y}$下的后验概率对$w$进行修改正，从而做出对$w$更好地估计。</p><p>若假设$w_j$的先验分布是均值为0的高斯分布，即$w_j\sim N(0, \sigma^2)$，则有：</p><p>$$logP(w) = log\prod_jP(w_j)=log\prod_j[\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(w_j)^2}{2\sigma^2}}]=-\frac{1}{2\sigma^2}\sum_jw_j^2+C’$$</p><p>可以看到，在高斯分布下$logP(w)$的效果等价于在代价函数中增加$l_2$正则项。</p><p>若假设$w_j$服从均值为0、参数为$a$的拉普拉斯分布，即：</p><p>$P(w_j)=\frac{1}{\sqrt{2a}}e^{\frac{-|w_j|}{a}}$</p><p>则有：</p><p>$$logP(w) = log\prod_j\frac{1}{\sqrt{2a}}e^{\frac{-|w_j|}{a}}=-\frac{1}{a}\sum_j|w_j|+C’$$</p><p>可以看到，在拉普拉斯分布下$logP(w)$的效果等价于在代价函数中增加$l_1$正则化。</p><p>因此，我们得到对于$l_1、l_2$正则化的第二种理解：</p><ul><li>$l_1$正则化可通过假设权重$w$的先验分布为拉普拉斯分布，由最大后验概率估计导出；</li><li>$l_2$正则化可通过假设权重$w$的先验分布为高斯分布，由最大后验概率估计导出。</li></ul><h1 id="正则化效果理解"><a href="#正则化效果理解" class="headerlink" title="正则化效果理解"></a>正则化效果理解</h1><h2 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h2><p>有了正则化公式来源分析，我们现在从优化的角度来看一下正则化对目标函数的影响。考虑带约束条件的优化解释，对$l_2$正则化为：</p><p>$$min_w J(w;X,y)\ s.t. ||w||_2 \leq C$$</p><p>该问题的求解示意图如下：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-031232.jpg" alt="v2-7431d8a79deec5d0ab3193b6a3611b95_hd"></p><p>图中椭圆为原目标函数$J(w)$的一条等高线，圆为半径$\sqrt{C}$的$l_2$范数球。由于约束条件的限制，$w$必须位于$l_2$范数球内。考虑边界上的一点$w$，图中蓝色箭头为$J(w)$在该处的梯度方向$\triangledown J(w)$，红色箭头为$l_2$范数球在该处的法线方向。由于$w$不能离开边界（否则就会违反约束条件），因为在使用梯度下降法更新$w$时，只能朝$\triangledown J(w)$在范数球上$w$处的切线方向更新，即图中的绿色箭头的方向。如此$w$将沿着边界移动，当$\triangledown J(w)$与范数球上$w$处的切线方向更新，即图中绿色箭头的方向。如果$w$将沿着边界移动，当$\triangledown J(w)$与范数球上$w$处的法线平行时，此时，$\triangledown J(w)$在切线方向的分量为0，$w$将无法继续移动，从而达到最优解$w^*$（图中红色点所示）。</p><p>对于$l_1$正则化：</p><p>$$min_w J(w;X,y)\ s.t. ||w||_1 \leq C$$</p><p>同理，其求解示意图如下所示：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-031238.jpg" alt="v2-592216faffaa338fc792430a538afefc_hd"></p><p>其主要差别在于$l1、l_2$范数球的形状差异。由于此时每条边界上$w$的切线和法线方向保持不变，在图中$w$一直朝着$\triangledown J(w)$的切线方向的分量沿着边界向左上移动。当$w$跨过顶点到达$w’$处时，$\triangledown J(w)$在切线方向的分量变为右上方，因而$w$将朝右上方移动。最终，$w$将稳定在顶点处，达到最优解$w^*$，此时，可以看到$w_1=0$，这也就是采用$l_1$范数会使得$w$产生稀疏性的原因。</p><p>以上的分析虽然是基于二维的情况，但不难将其推广到多维度情况，其主要目的是为了直观地说明$l_1、l_2$正则化最优解的差异，以及$l_1$范数为什么会产生稀疏性。</p><h1 id="正则化效果分析"><a href="#正则化效果分析" class="headerlink" title="正则化效果分析"></a>正则化效果分析</h1><h2 id="稀疏性"><a href="#稀疏性" class="headerlink" title="稀疏性"></a>稀疏性</h2><p>从以上的正则化效果理解其实已经可以看出$L1$正则化可以使得答案稀疏的效果，如果那个还不好理解，可以看下面这个例子。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>L1正则化和L2正则化哪个能产生稀疏的解呢？答案是<strong>L1正则化</strong>。假设我们现在要求解模型$Ax=b$，也就是在2维空间上找到一条直线来拟合样本点。我们需要两个点才能去固定一条直线，但是，假设我们现在的训练样本中只有一个点。那么我们将得到无穷多个解。假设，该点为(10, 5)，直线为$y=a*x+b$，那么，该例子可形式化为求解模型：$b = 5 - 10*a$ 的参数。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-13-093559.jpg" alt="1_sMS5qc_2O6h87L_NF0B8Mw"></p><p>那么，当我们加上正则化项后，又该如何求解呢？</p><p>假设，我们的正则化的值等于一个常数，它的图像如下所示：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-13-093553.jpg" alt="1_uHXe9qibzdqieBfje7Hggw"></p><p>我们注意到，在红色直线上，并不是所有点都是稀疏点，而只有在顶点处的点才是稀疏的，因为顶点处的点某些维度为0。现在，我们要做的是就是扩大这个红色的形状，让它慢慢的靠近上图中蓝色的直线直到两者有公共点。当我们慢慢增大后，我们发现，最有可能成为公共交点的就是红色形状的顶点。而从刚才的分析中，我们知道，红色形状的顶点是稀疏点，所以，加上L1正则化后，得到的解往往都是稀疏的，而且这些公共点对应的常数$c$也是比较小的。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-13-093556.jpg" alt="1_0QRBxi6dlivROqCSFQeYhA"></p><p>而，L2正则化没有这种稀疏性特点。</p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>L1正则化具有特征选择的功能，这是因为L1正则化通常会产生系数的解，假设我们有100个系数，在L1正则化的作用下只有10个系数非0，那么这就等价于我们从100个特征中抽出10个重要的特征。</p><h2 id="计算复杂度"><a href="#计算复杂度" class="headerlink" title="计算复杂度"></a>计算复杂度</h2><p>在计算效率上，L2优于L1，因为L1正则化通常是不可导的，这导致我们不能用矩阵方式来求解它，而大多数是依赖于近似的方式。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a" target="_blank" rel="noopener">L1 Norm Regularization and Sparsity Explained for Dummies</a></p><p>[2]. <a href="https://www.wikiwand.com/en/Regularization_(mathematics" target="_blank" rel="noopener">Regularization (mathematics)</a>)</p><p>[3]. <a href="https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms" target="_blank" rel="noopener">L1 Norms versus L2 Norms</a></p><p>[4]. <a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf" target="_blank" rel="noopener">Hsuan-Tien Lin. Machine Learning Foundations Lecture 14.</a></p><p>[5]. <a href="https://zhuanlan.zhihu.com/p/29360425" target="_blank" rel="noopener">深入理解L1、L2正则化</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;范数&quot;&gt;&lt;a href=&quot;#范数&quot; class=&quot;headerlink&quot; title=&quot;范数&quot;&gt;&lt;/a&gt;范数&lt;/h1&gt;&lt;p&gt;过拟合是机器学习中一个比较常见的问题，而正则化是解决模型过拟合的一种手段。我们先看一下范数的定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型优化" scheme="http://conghuai.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>损失函数 1-1：Hinge损失</title>
    <link href="http://conghuai.me/2018/04/10/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%201-1%EF%BC%9AHinge%E6%8D%9F%E5%A4%B1/"/>
    <id>http://conghuai.me/2018/04/10/损失函数 1-1：Hinge损失/</id>
    <published>2018-04-10T06:39:46.000Z</published>
    <updated>2018-09-24T15:25:55.450Z</updated>
    
    <content type="html"><![CDATA[<h1 id="函数特性"><a href="#函数特性" class="headerlink" title="函数特性"></a>函数特性</h1><p>在机器学习中，<strong>hinge loss</strong>是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为：</p><p>$$L(y)=max(0,1-\hat{y}y)$$</p><p>其中$\hat{y}$表示预测输出，通常都是软结果（就是说输出不是0，1这种，可能是0.87。），$y$表示正确的类别。</p><ul><li>如果$\hat{y}y&lt;1$，则损失为：$1-\hat{y}y$</li><li>如果$\hat{y}y&gt;=1$，则损失为：0</li></ul><p>其函数图像如下，与0-1损失对比：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-10-065844.jpg" alt="Hinge_loss_vs_zero_one_loss.svg"></p><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p>以支持向量机为例，其模型为：$\hat{y}=w\cdot x$，如果用hinge损失，其求导结果如下：</p><p>$$\begin{split}\frac{\partial L}{\partial w_i}=\begin{cases} -y\cdot x_i, &amp; \text{if $\hat{y}y&lt;1$} \\ 0, &amp; \text{otherwise}\end{cases}\end{split}$$</p><h1 id="变种"><a href="#变种" class="headerlink" title="变种"></a>变种</h1><p>实际应用中，一方面很多时候我们的y的值域并不是[-1,1]，比如我们可能更希望y更接近于一个概率，即其值域最好是[0,1]。另一方面，很多时候我们希望训练的是两个样本之间的相似关系，而非样本的整体分类，所以很多时候我们会用下面的公式： </p><p>$$l(y,y’)=max(0, m-y+y’)$$</p><p>其中，y是正样本的得分，y’是负样本的得分，m是margin（自己选一个数）</p><p>即我们希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。</p><p>比如，我们想训练词向量，我们希望经常同时出现的词，他们的向量内积越大越好；不经常同时出现的词，他们的向量内积越小越好。则我们的hinge loss function可以是： </p><p>$$l(w,w_+,w_-)=max(0, 1-w^T\cdot w_+ + w^T\cdot w_-) $$</p><p>其中，w是当前正在处理的词，$w_+$是w在文中前3个词和后3个词中的某一个词，$w_−$是随机选的一个词。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="https://www.wikiwand.com/en/Hinge_loss" target="_blank" rel="noopener">Wikiwand Hinge loss</a></p><p>[2]. <a href="https://blog.csdn.net/luo123n/article/details/48878759" target="_blank" rel="noopener">损失函数：Hinge Loss（max margin）</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;函数特性&quot;&gt;&lt;a href=&quot;#函数特性&quot; class=&quot;headerlink&quot; title=&quot;函数特性&quot;&gt;&lt;/a&gt;函数特性&lt;/h1&gt;&lt;p&gt;在机器学习中，&lt;strong&gt;hinge loss&lt;/strong&gt;是一种损失函数，它通常用于”maximum-margin
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="损失函数" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Negative Maximum Likehood Loss Function</title>
    <link href="http://conghuai.me/2018/04/08/Negative-Maximum-Likehood-Loss-Function/"/>
    <id>http://conghuai.me/2018/04/08/Negative-Maximum-Likehood-Loss-Function/</id>
    <published>2018-04-08T08:15:39.000Z</published>
    <updated>2018-09-24T15:28:16.204Z</updated>
    
    <content type="html"><![CDATA[<p>损失函数是用来衡量模型好坏的一个标准，在机器学习里，我们通常希望模型有比较小的<code>loss</code>，那么该如何选择我们的损失函数呢？最小化负的似然函数，借鉴了统计学的思想，是一种常见的损失函数。</p><h1 id="Nagative-Maximum-Likehood"><a href="#Nagative-Maximum-Likehood" class="headerlink" title="Nagative Maximum Likehood"></a>Nagative Maximum Likehood</h1><p>首先，假设我们有一堆的样本点：$D={(x_1,-1),(x_2,1),…,(x_N,-1)}$，我们希望我们训练出来的模型能够准确预测$x_i$的类别。通常来说，我们定义的模型都会对每个目标类别输出一个概率值，所以，本质上，我们希望得到一个函数，它能告诉我们样本$x_i$属于+1的概率（或者属于-1的概率，本质上是一样的）：$f(x)=P(+1|x)$</p><p>$$\begin{split}P(y|x)=\begin{cases} f(x) &amp; \text{for $y=+1$} \\ 1-f(x) &amp; \text{for y=-1}\end{cases}\end{split}$$</p><p>所以，我们的模型$h$产生样本集$D$的概率有多大呢：</p><p>$$L=P(x1)h(x1)\cdot P(x_2)(1-h(x_2))\cdot …\cdot P(x_N)(1-h(x_N))$$</p><p>如果我们的模型足够好的话，那么上面的似然函数的值会很大，我们现在需要在假设空间里面，把最好的$h$给找出来，哪个$h$最好呢？就是使得$L$最大的那个模型咯。</p><p>$$L=P(x1)h_1(x1)\cdot P(x_2)(1-h_1(x_2))\cdot …\cdot P(x_N)(1-h_1(x_N))$$</p><p>$$L=P(x1)h_2(x1)\cdot P(x_2)(1-h_2(x_2))\cdot …\cdot P(x_N)(1-h_2(x_N))$$</p><p>…</p><p>$$L=P(x1)h_k(x1)\cdot P(x_2)(1-h_k(x_2))\cdot …\cdot P(x_N)(1-h_k(x_N))$$</p><p>我们发现，对于不同的模型，都要乘上$P(x_1)P(x_2)…P(x_N)$，这对于我们比较不同模型的好坏，没有帮助，所有我们可以把它略去。</p><p>所以，似然函数表达式变为：</p><p>$$L=h_k(x1)\cdot (1-h_k(x_2))\cdot …\cdot(1-h_k(x_N))$$</p><p>我们通常选择sigmoid function当做$h$，本文最后会给出sigmoid function的性质。由于采用了sigmoid函数，似然函数表达式变为：</p><p>$$L=h_k(x1)\cdot h_k(-x_2)\cdot …\cdot h_k(-x_N)$$</p><p>进一步的可以写成：</p><p>$$L=h_k(y_1x_1)\cdot h_k(y_2x_2)\cdot …\cdot h_k(y_Nx_N)=\prod_{n=1}^Nh(y_nx_n)$$</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对上述的似然函数取负数，再取log，可以得到最终的负对数似然损失函数：</p><p>$$MLE = \frac{1}{N}\sum_{n=1}^N-ln\theta(y_nw^Tx_n)=\frac{1}{N}\sum_{n=1}^Nln(1+e^{-y_nw^Tx_n})$$</p><h2 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-08-085532.jpg" alt="Screen Shot 2018-04-08 at 16.55.05"></p><h1 id="sigmoid-function"><a href="#sigmoid-function" class="headerlink" title="sigmoid function"></a>sigmoid function</h1><p>我们通常会选择<code>sigmoid function</code>当做$h$。</p><h2 id="函数特性"><a href="#函数特性" class="headerlink" title="函数特性"></a>函数特性</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-08-085535.jpg" alt="Screen Shot 2018-04-08 at 16.40.42"></p><p>$$\theta(s)=\frac{e^s}{1+e^s}=\frac{1}{1+e^{-s}}$$</p><ul><li>$\theta(负无穷)$=0；</li><li>$\theta(0)=\frac{1}{2}$</li><li>$\theta(正无穷)=1$</li><li>连续、单调</li><li>$\theta(-s)=1-\theta(s)$</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/" target="_blank" rel="noopener">机器学习-损失函数</a></p><p>[2]. 林轩田 “机器学习基石”</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;损失函数是用来衡量模型好坏的一个标准，在机器学习里，我们通常希望模型有比较小的&lt;code&gt;loss&lt;/code&gt;，那么该如何选择我们的损失函数呢？最小化负的似然函数，借鉴了统计学的思想，是一种常见的损失函数。&lt;/p&gt;
&lt;h1 id=&quot;Nagative-Maximum-Lik
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="损失函数" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>损失函数 1-1：交叉熵损失函数</title>
    <link href="http://conghuai.me/2018/04/08/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%201-1%EF%BC%9A%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    <id>http://conghuai.me/2018/04/08/损失函数 1-1：交叉熵损失函数/</id>
    <published>2018-04-08T02:54:16.000Z</published>
    <updated>2018-09-24T15:24:15.412Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，讨论的Cross Entropy损失函数常用于分类问题中，但是为什么它会在分类问题中这么有效呢？我们先从一个简单的分类例子来入手。</p><h1 id="预测政治倾向例子"><a href="#预测政治倾向例子" class="headerlink" title="预测政治倾向例子"></a>预测政治倾向例子</h1><p>我们希望根据一个人的年龄、性别、年收入等相互独立的特征，来预测一个人的政治倾向，有三种可预测结果：民主党、共和党、其他党。假设我们当前有两个模型，这两个模型最后输出都是通过softmax的方式得到对于每个预测结果的概率：</p><p><strong>模型1</strong>：</p><table><thead><tr><th style="text-align:center">COMPUTED</th><th style="text-align:center">TARGETS</th><th style="text-align:center">CORRECT?</th></tr></thead><tbody><tr><td style="text-align:center">0.3 0.3 0.4</td><td style="text-align:center">0 0 1（民主党）</td><td style="text-align:center">正确</td></tr><tr><td style="text-align:center">0.3 0.4 0.3</td><td style="text-align:center">0 1 0（共和党）</td><td style="text-align:center">正确</td></tr><tr><td style="text-align:center">0.1 0.2 0.7</td><td style="text-align:center">1 0 0 （其他党）</td><td style="text-align:center">错误</td></tr></tbody></table><p><strong>模型1</strong>对于样本1和样本2以非常微弱的优势判断正确，对于样本3的判断则彻底错误。</p><p><strong>模型2</strong>：</p><table><thead><tr><th style="text-align:center">COMPUTED</th><th style="text-align:center">TARGETS</th><th style="text-align:center">CORRECT?</th></tr></thead><tbody><tr><td style="text-align:center">0.1 0.2 0.7</td><td style="text-align:center">0 0 1（民主党）</td><td style="text-align:center">正确</td></tr><tr><td style="text-align:center">0.1 0.7 0.2</td><td style="text-align:center">0 1 0（共和党）</td><td style="text-align:center">正确</td></tr><tr><td style="text-align:center">0.3 0.4 0.3</td><td style="text-align:center">1 0 0 （其他党）</td><td style="text-align:center">错误</td></tr></tbody></table><p><strong>模型2</strong>对于样本1和样本2判断非常准确，对于样本3判断错误，但是相对来说没有错得太离谱。</p><p>好了，有了模型之后，我们需要通过定义损失函数来判断模型在样本上的表现了，那么我们可以定义哪些损失函数呢？</p><h2 id="Classification-Error（分类错误率）"><a href="#Classification-Error（分类错误率）" class="headerlink" title="Classification Error（分类错误率）"></a>Classification Error（分类错误率）</h2><p>最为直接的损失函数定义为：$classification\ error=\frac{count\ of\ error\ items}{count\ of \ all\ items}$</p><p><strong>模型1：</strong>$classification\ error=\frac{1}{3}$</p><p><strong>模型2：</strong>$classification\ error=\frac{1}{3}$</p><p>我们知道，<strong>模型1</strong>和<strong>模型2</strong>虽然都是预测错了1个，但是相对来说<strong>模型2</strong>表现的更好，损失函数值照理来说应该更小，但是，很遗憾的是，$classification\ error$并不能判断出来，所以这种损失函数虽然好理解，但表现不太好。</p><h2 id="Mean-Squared-Error-平方和"><a href="#Mean-Squared-Error-平方和" class="headerlink" title="Mean Squared Error (平方和)"></a>Mean Squared Error (平方和)</h2><p>平方和损失也是一种比较常见的损失函数，其定义为：$MSE=\frac{1}{n}\sum_{i}^n(\hat{y_i}-y_i)$</p><p><strong>模型1：</strong>$MSE=\frac{0.54+0.54+1.34}{3}=0.81$</p><p><strong>模型2：</strong>$MSE=\frac{0.14+0.14+0.74}{3}=0.34$</p><p>MSE能够判断出来<strong>模型2</strong>优于<strong>模型1</strong>，那为什么不采样这种损失函数呢？原因在于，使用该损失函数时，得到的表达式是非凸函数，有很多局部的极值点。在做优化的时候不太好处理。</p><p>有了上面的直观分析，我们可以清楚的看到，对于分类问题的损失函数来说，分类错误率和平方和损失都不是很好的损失函数，下面我们来看一下交叉熵损失函数是怎么解决这个问题的。</p><h1 id="Cross-Entropy-Error-Function"><a href="#Cross-Entropy-Error-Function" class="headerlink" title="Cross Entropy Error Function"></a>Cross Entropy Error Function</h1><p>交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和softmax函数一起出现。</p><h2 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h2><h3 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h3><p>在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为$p$和$1-p$。此时表达式为：$−(ylog(p)+(1−y)log(1−p))$</p><h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><p>多分类的情况实际上就是对二分类的扩展：$-\sum_{c=1}^My_{o,c}\log(p_{o,c})$</p><p>其中：</p><ul><li>$M$——类别的数量；</li><li>$y$——指示变量（0或1）,如果该类别和样本观测到的类别相同就是1，否则是0；</li><li>$p$——对于观测样本属于类别c的预测概率。</li></ul><h2 id="函数图像"><a href="#函数图像" class="headerlink" title="函数图像"></a>函数图像</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-08-024353.jpg" alt="cross_entropy"></p><p>可以看出，该函数是凸函数，求导时能够得到全局最优值。</p><h2 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h2><p>我们用神经网络最后一层输出的情况，来看一眼整个模型预测及获得损失的的流程：</p><ol><li>神经网络最后一层得到每个类别的得分<strong>scores</strong>；</li><li>该得分经过softmax转换为概率输出；</li><li>模型预测的类别概率输出与真实类别的one hot形式进行cross entropy损失函数的计算。</li></ol><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-08-064410.jpg" alt="Screen Shot 2018-04-08 at 11.56.55"></p><p>下面，我们以二分类的情形来推导一下整个求导公式，我们将求导分成两个过程，即拆成两项偏导的乘积。：</p><p>$$\frac{\partial E}{\partial score_i}=\frac{\partial E}{\partial p_i}\cdot \frac{\partial p_i}{\partial score_i}$$</p><h3 id="计算第一项：-frac-partial-E-partial-p-i"><a href="#计算第一项：-frac-partial-E-partial-p-i" class="headerlink" title="计算第一项：$\frac{\partial E}{\partial p_i}$"></a>计算第一项：$\frac{\partial E}{\partial p_i}$</h3><p>\begin{align}<br>\frac{\partial E}{\partial p_i} &amp;= \frac{\partial −(ylog(p)+(1−y)log(1−p))}{\partial p_i} \\<br> &amp;= -\frac{\partial y_ilogp_i}{\partial p_i}-\frac{\partial (1-y_i)log(1-p_i)}{\partial p_i} \\<br> &amp;= -\frac{y_i}{p_i}-[(1-y_i)\cdot \frac{1}{1-p_i}\cdot (-1)] \\<br> &amp;= -\frac{y_i}{p_i}-\frac{1-y_i}{1-p_i} \\<br>\end{align}</p><h3 id="计算第二项：-frac-partial-p-i-partial-score-i"><a href="#计算第二项：-frac-partial-p-i-partial-score-i" class="headerlink" title="计算第二项：$\frac{\partial p_i}{\partial score_i}$"></a>计算第二项：$\frac{\partial p_i}{\partial score_i}$</h3><p>这一项要计算的是softmax函数对于score的导数，我们先回顾一下分数求导的公式：</p><blockquote><p>$$f(x) = \frac{g(x)}{h(x)}=\frac{g’(x)h(x)-g(x){h}’(x)}{h^2(x)}$$</p></blockquote><p>考虑$k$等于$i$的情况：</p><p>\begin{align}<br>\frac{\partial p_i}{\partial score_i} &amp;= \frac{({e^{y_i}})’\cdot (\sum_ie^{y_i})-e^{y_i}\cdot {(\sum_j e^{y_i})}’}{(\sum_je^{y_i})^2} \\<br> &amp;= \frac{e^{y_i}\cdot \sum_ie^{y_i}-{(e^{y_i})}^2}{(\sum_je^{y_i})^2} \\<br> &amp;= \frac{e^{y_i}}{\sum_je^{y_i}} - \frac{(e^{y_i})^2}{(\sum_je^{y_i})^2} \\<br> &amp;= \frac{e^{y_i}}{\sum_je^{y_i}}\cdot (1 - \frac{e^{y_i}}{\sum_je^{y_i}}) \\<br> &amp;= \sigma(y_i)(1-\sigma(y_i)) \\<br>\end{align}</p><p>考虑k不等于i的情况：</p><p>\begin{align}<br>\frac{\partial p_i}{\partial score_i} &amp;= \frac{(e^{y_k})’\cdot (\sum_ie^{y_i})-e^{y_i}\cdot {(\sum_j e^{y_i})}’}{(\sum_je^{y_i})^2} \\<br> &amp;= \frac{0\cdot \sum_ie^{y_i}-(e^{y_i})\cdot (e^{y_k})}{(\sum_je^{y_i})^2} \\<br> &amp;= -\frac{e^{y_i}\cdot e^{y_k}}{(\sum_je^{y_i})^2} \\<br> &amp;= -\frac{e^{y_i}}{\sum_je^{y_i} }\cdot \frac{e^{y_k} }{\sum_je^{y_i}} \\<br> &amp;= -\sigma(y_i)\cdot \sigma(y_k) \\<br>\end{align}</p><p>综上可得softmax损失函数的求导结果：</p><p>\begin{split}\frac{\partial pi}{\partial score_i}=\begin{cases} \sigma(y_i)(1-\sigma(y_i)) &amp; \text{$if\ j=k$} \\ -\sigma(y_i)\cdot \sigma(y_k) &amp; \text{$if\ j \neq k$}\end{cases}\end{split}</p><h3 id="计算结果-frac-partial-E-partial-score-i"><a href="#计算结果-frac-partial-E-partial-score-i" class="headerlink" title="计算结果$\frac{\partial E}{\partial score_i}$"></a>计算结果$\frac{\partial E}{\partial score_i}$</h3><p>\begin{align}<br>\frac{\partial E}{\partial score_i} &amp;= \frac{\partial E}{\partial p_i}\cdot \frac{\partial p_i}{\partial score_i} \\<br> &amp;= [-\frac{y_i}{\sigma(y_i)}-\cdot \frac{1-y_i}{1-\sigma(y_i)}]\cdot  \sigma(y_i)(1-\sigma(y_i) \\<br> &amp;= -\frac{c_i}{\sigma(y_i)}\cdot \sigma(y_i)\cdot (1-\sigma(y_i))+\frac{1-c_i}{1-\sigma(y_i)}\cdot \sigma(y_i)\cdot (1-\sigma(y_i)) \\<br> &amp;= -c_i+c_i\cdot \sigma(y_i)+\sigma(y_i)-c_i\cdot \sigma(y_i) \\<br> &amp;= \sigma(y_i)-c_i \\<br>\end{align}</p><p>可以看到，我们得到了一个非常漂亮的结果，所以，Cross Entropy损失函数，不仅可以很好的衡量模型的效果，又可以很容易的的进行求导计算。</p><h1 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h1><p>我们对结果进一步对参数求导：即$\frac{\partial E}{\partial w_i}=\frac{\partial E}{\partial score_i}\cdot \frac{\partial score_i}{\partial w_i}=x_i\cdot [\sigma(y_i)-c_i]$</p><p>在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、学习率；二、偏导值。其中，学习率是我们需要设置的超参数，所以我们重点关注偏导值。从上面的式子中，我们发现，偏导值的大小取决于$x_i$和$[\sigma(y_i)-c_i]$，我们重点关注后者，后者的值大小反映了我们模型的错误程度，该值越大，说明模型效果越差，但是该值越大同时也会使得模型学习速度更快。所以，用交叉熵当损失函数在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢，这是我们希望得到的。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="http://jackon.me/posts/why-use-cross-entropy-error-for-loss-function/" target="_blank" rel="noopener">神经网络的分类模型 LOSS 函数为什么要用 CROSS ENTROPY</a></p><p>[2]. <a href="http://sefiks.com/2017/11/08/softmax-as-a-neural-networks-activation-function/" target="_blank" rel="noopener">Softmax as a Neural Networks Activation Function</a></p><p>[3]. <a href="https://sefiks.com/2017/12/17/a-gentle-introduction-to-cross-entropy-loss-function/" target="_blank" rel="noopener">A Gentle Introduction to Cross-Entropy Loss Function</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章中，讨论的Cross Entropy损失函数常用于分类问题中，但是为什么它会在分类问题中这么有效呢？我们先从一个简单的分类例子来入手。&lt;/p&gt;
&lt;h1 id=&quot;预测政治倾向例子&quot;&gt;&lt;a href=&quot;#预测政治倾向例子&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="损失函数" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>K-Means</title>
    <link href="http://conghuai.me/2017/12/07/K-Means/"/>
    <id>http://conghuai.me/2017/12/07/K-Means/</id>
    <published>2017-12-07T01:01:09.000Z</published>
    <updated>2018-09-24T15:27:03.166Z</updated>
    
    <content type="html"><![CDATA[<p>​聚类，是机器学习的任务之一。同分类算法一样，聚类算法也被广泛的应用在各个领域，如根据话题，对文章、网页和搜索结果做聚类；根据社区发现对社交网络中的用户做聚类；根据购买历史记录对消费者做聚类。和分类算法不同的是，聚类算法的样本是没有标签的，也就是说，我们并不知道样本有哪些类别，算法需要根据样本的特征，对样本进行聚类，形成不同的聚类中心点。这篇文章，主要介绍比较著名的聚类算法——K-means算法。</p><p>​首先，我们看一下基于目标来做聚类的算法定义:</p><p><strong>Input</strong> : A set S of n points, also a distance/dissimilarity measure specifying the distance d(x, y) between pairs (x, y). </p><p><strong>Goal</strong>: output a partition of the data</p><p>​基于这个定义，选择不同的距离计算公式，有以下三种具体的算法:</p><ul><li><strong>k-means</strong>: find center partitions $c_1, c_2, …, c_k$ to minimize<br>$$ \sum min_{j \in{i, …,k}}d^2(x^i, c_j) $$</li><li><strong>k-median</strong>: find center partitions $c_1, c_2, …, c_k$ to minimize<br>$$ \sum min_{j \in{i, …,k}}d(x^i, c_j) $$ </li><li><strong>k-center</strong>: find partition to minimize the maximum radius</li></ul><h1 id="Euclidean-k-means-clustering"><a href="#Euclidean-k-means-clustering" class="headerlink" title="Euclidean k-means clustering"></a>Euclidean k-means clustering</h1><p>采用欧拉距离公式的k-means算法定义如下:</p><p><strong>Input</strong>: A set of n datapoints $x^1, x^2, …, x^n$ in $R^d$ (target #clusters k)</p><p><strong>Output</strong>: k representatives $c_1, c_2, …, c_k \in R^d$ </p><p><strong>Objective</strong>: choose $c_1, c_2, …, c_k \in R^d$ to minimize<br>$$ \sum min_{j \in {1,…,k}}||x^i - c_j||^2 $$</p><p>求解该算法的最优解是一个NP难的问题，所有我们没有办法获得最优解，当然，当k=1或d=1这种特殊情况下，是可以获得最优解，有兴趣的可以自行推导一下， 这里不在赘述，这里我们主要介绍Lloyd’s method[1]，该方法的核心算法如下:</p><p><strong>Input</strong>: A set of n datapoints $x^1, x^2, …, x^n$ in $R^d$</p><p><strong>Initialize</strong> centers $c_1, c_2, …, c_k \in R^d$ and clusters $C_1, C_2, …, C_k$ in any way.</p><p><strong>Repeat</strong> until there is no further change in the cost.</p><ol><li>For each j: $C_j \leftarrow {x \in S\ whose\ closest\ center\ is\ c_j}$</li><li>For each j: $c_j \leftarrow mean\ of\ C_j $</li></ol><p>对于该算法，难度不是特别大，最重要的地方在Repeat中的1，2两个步骤，其中，步骤1将固定住聚类中心$c_1, c_2, …, c_k$，更新聚类集$C_1, C_2, …, C_k$。步骤2固定住聚类集$C_1, C_2, …, C_k$，更新聚类中心$c_1, c_2, …, c_k$（本文用大写字符表示集合，这里用C指聚类簇；用小写字符表示单个点，这里用c指聚类中心。在本人的其他博文中，也会采用这种表述方式）。</p><p>大部分学习k-means算法的人理解了步骤1和步骤2就觉得已经理解了k-means了，其实不然，先不说k-means中比较重要的聚类中心的初始化问题，任何一个机器学习算法，它要是有效的，必须证明其可收敛，也需要给出其时间复杂度和空间复杂度。</p><h1 id="Converges"><a href="#Converges" class="headerlink" title="Converges"></a>Converges</h1><ul><li>目标函数的值在每一轮的迭代都会降低，这个特性由算法中步骤1和步骤2保证，因为对于每个样本点，我们每次都是选择最接近的聚类中心；而且，在每个聚类簇里，我们选择平均值作为其聚类中心。</li><li>目标函数有最小值0。</li></ul><p>由于目标函数有最小值，而且在每一轮中都是值都是减少的，所有算法必然会收敛。</p><h1 id="Running-Time"><a href="#Running-Time" class="headerlink" title="Running Time"></a>Running Time</h1><ul><li>O(nkd)  n为样本数 k为聚类中心数 d为维度</li></ul><h1 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h1><p>介绍完了整个算法过程、收敛性和时间复杂度之后，该算法的两个核心点需要我们思考: 1. 如何选择k的值; 2. 算法刚开始，并没有聚类中心，如何初始化聚类中心。对于问题1，我目前还没有过多的认识。这里主要介绍问题2，如何初始化聚类中心。</p><h2 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h2><p>这种初始化方式是最简单的方式，就是随机选k个点作为聚类中心，虽然简单，但是会存在问题，我们看下面的这个例子:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010551.jpg" alt="random init"></p><p>由于，我们采用了随机初始化的方式，对于这个样本，我们随机初始化的三个点如上图的绿、红、黑三个样本点，再后面的迭代中，我们最后的聚类簇如上图的箭头所示，这样的效果好吗？显然是不好的，为什么呢？因为很显然最左边三个、中间三个、最右边三个应该是被归为一个聚类簇的:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010534.jpg" alt="random init2"></p><p>我们可以看到，聚类中心初始化得不好，直接影响我们最后聚类的效果，可能上面举的例子样本分布和初始化聚类中心太极端，不能说明问题， 我们现在假设样本的分布是多个高斯分布的情况下，聚类中心初始化不好导致的最后聚类的效果:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010548.jpg" alt="random init4"></p><p>我们现在计算一下假设有k个高斯分布，我们随机初始化正确的概率有大(所谓正确是指任何两个随机初始化中心不在同一个高斯分布中):$\frac {k!}{k^k} \approx \frac {1}{e^k}$，当k增大时，这个概率会越来越低。</p><h2 id="Furthest-Point-Heuristic"><a href="#Furthest-Point-Heuristic" class="headerlink" title="Furthest Point Heuristic"></a>Furthest Point Heuristic</h2><p>这种方法是一个中心点一个中心点依次进行初始化的，首先随机初始化$c_1$，然后选择距离$c_1$最远的点来初始化$c_2$，以此类推。</p><p>算法描述如下:</p><p>Choose $c_1$ arbitrarily (or at random).</p><p>For j = 2, …, k</p><p>Pick $c_j$ among datapoints $x^1, x^2, …, x^n$ that is farthest from previously chosen $c_1, c_2, …, c_{j-1}$</p><p>这种方法解决了随机初始化高斯分布例子中的问题:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010552.jpg" alt="dist 1"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010546.jpg" alt="dist 2"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010549.jpg" alt="dist 3"></p><p>但是，这种方法的问题是容易受噪声点干扰，请看下面的例子:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010545.jpg" alt="dist 4"></p><p>所以这种方式进行初始化也是不行的，一旦出现噪声点，就极大的影响了最后聚类的结果。虽然实际上，几乎没有哪一个k-means算法会采用上面两种初始化方式，但是这里这样介绍是顺着我们的思维方式进行的，一般思考的方式都是从简单到复杂，所以下面，我们也顺理成章的引出<code>k-means++</code>这个初始化算法， 该算法很好的反映出随机化思想在算法中的重要性。</p><h2 id="k-means"><a href="#k-means" class="headerlink" title="k-means++"></a>k-means++</h2><p>算法描述如下:</p><ul><li><p>Choose $c_1$ at random.</p></li><li><p>For j = 2, …, k</p></li><li><p>Pick $c_j$ among $x^1, x^2, …, x^n$ according to the distribution</p><p>  $ Pr(c_j = x^i) \propto min_{j’ &lt; j}\left | x^i - c_{j’} \right |^2 $</p></li></ul><p>这就是k-means++的初始化过程，这个过程比较不好理解。关于这个过程，作以下几点说明:</p><ul><li>这个初始化算法引入随机化，下一个被选为中心点的样本不是固定的，而是一个概率值，这个概率值正比于“离最近中心点的距离“。</li><li>”离最近中心点的距离“如何计算，实际上非常简单，就是当前样本距离各个中心点的距离中，最小的那个距离。</li><li>既然概率正比于 ”距离“ ，那么离群点的”距离“肯定是最大的，它的概率肯定是最大的，可是为什么算法不一定会选择它呢？举个例子说明，如果我们现在有一个聚类集合$S={x_1,x_2,x_3}$,和离群点$x_o$，假设选中 $x_o$的概率为 $1/3$ , 选中 $x_1, x_2, x_3$的概率分别为 $2/9$，这样看，即使$x_o$的概率很大，但是它只有1个，而 $x_1, x_2, x_3$ 即使每个概率不大，但是我们只要随便选中其中一个都是可以的(这是因为它们都在一个聚类簇中，只要选择聚类簇中任何一个点当聚类中心都可以)，所以可以把他们的概率相加，最后得到的概率就大于选中 $x_o$的概率。</li></ul><h1 id="In-Action"><a href="#In-Action" class="headerlink" title="In Action"></a>In Action</h1><p>当然，在实际项目中，我们可能不会自己实现<code>k-means</code>算法， 一般我们都会用现成的比较好的一些机器学习库，我们这里结合<code>scikit-learn</code>来看一下，它是如何实现<code>k-means</code>算法的。</p><p>首先看一下，<code>sklearn.cluster.k_means</code>模块下的函数<code>k_means</code>方法:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">k_means</span><span class="params">(X, n_clusters, init=<span class="string">'k-means++'</span>, precompute_distances=<span class="string">'auto'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            n_init=<span class="number">10</span>, max_iter=<span class="number">300</span>, verbose=False,</span></span></span><br><span class="line"><span class="function"><span class="params">            tol=<span class="number">1e-4</span>, random_state=None, copy_x=True, n_jobs=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            algorithm=<span class="string">"auto"</span>, return_n_iter=False)</span>:</span></span><br></pre></td></tr></table></figure><p>首先，我们看到参数有一个<code>init</code>，这里是指定k-means初始化方法，这里我们看下注释:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">    init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional</span></span><br><span class="line"><span class="string">        Method for initialization, default to 'k-means++':</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        'k-means++' : selects initial cluster centers for k-mean</span></span><br><span class="line"><span class="string">        clustering in a smart way to speed up convergence. See section</span></span><br><span class="line"><span class="string">        Notes in k_init for more details.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        'random': generate k centroids from a Gaussian with mean and</span></span><br><span class="line"><span class="string">        variance estimated from the data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If an ndarray is passed, it should be of shape (n_clusters, n_features)</span></span><br><span class="line"><span class="string">        and gives the initial centers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If a callable is passed, it should take arguments X, k and</span></span><br><span class="line"><span class="string">        and a random state and return an initialization.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>可以看到，<code>sklearn</code>实现了2种初始化算法，一个是随机初始化算法，另一个是<code>k-means++</code>算法，默认采用的是<code>k-means++</code>算法。然后，我们先看一下<code>sklearn</code>实现<code>k-means++</code>的代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_k_init</span><span class="params">(X, n_clusters, x_squared_norms, random_state, n_local_trials=None)</span>:</span></span><br><span class="line">    <span class="string">"""Init n_clusters seeds according to k-means++</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    X : array or sparse matrix, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">        The data to pick seeds for. To avoid memory copy, the input data</span></span><br><span class="line"><span class="string">        should be double precision (dtype=np.float64).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    n_clusters : integer</span></span><br><span class="line"><span class="string">        The number of seeds to choose</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    x_squared_norms : array, shape (n_samples,)</span></span><br><span class="line"><span class="string">        Squared Euclidean norm of each data point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    random_state : numpy.RandomState</span></span><br><span class="line"><span class="string">        The generator used to initialize the centers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    n_local_trials : integer, optional</span></span><br><span class="line"><span class="string">        The number of seeding trials for each center (except the first),</span></span><br><span class="line"><span class="string">        of which the one reducing inertia the most is greedily chosen.</span></span><br><span class="line"><span class="string">        Set to None to make the number of trials depend logarithmically</span></span><br><span class="line"><span class="string">        on the number of seeds (2+log(k)); this is the default.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Notes</span></span><br><span class="line"><span class="string">    -----</span></span><br><span class="line"><span class="string">    Selects initial cluster centers for k-mean clustering in a smart way</span></span><br><span class="line"><span class="string">    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.</span></span><br><span class="line"><span class="string">    "k-means++: the advantages of careful seeding". ACM-SIAM symposium</span></span><br><span class="line"><span class="string">    on Discrete algorithms. 2007</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,</span></span><br><span class="line"><span class="string">    which is the implementation used in the aforementioned paper.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">    centers = np.empty((n_clusters, n_features), dtype=X.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> x_squared_norms <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, <span class="string">'x_squared_norms None in _k_init'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the number of local seeding trials if none is given</span></span><br><span class="line">    <span class="keyword">if</span> n_local_trials <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># This is what Arthur/Vassilvitskii tried, but did not report</span></span><br><span class="line">        <span class="comment"># specific results for other than mentioning in the conclusion</span></span><br><span class="line">        <span class="comment"># that it helped.</span></span><br><span class="line">        n_local_trials = <span class="number">2</span> + int(np.log(n_clusters))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pick first center randomly</span></span><br><span class="line">    center_id = random_state.randint(n_samples)</span><br><span class="line">    <span class="keyword">if</span> sp.issparse(X):</span><br><span class="line">        centers[<span class="number">0</span>] = X[center_id].toarray()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        centers[<span class="number">0</span>] = X[center_id]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize list of closest distances and calculate current potential</span></span><br><span class="line">    closest_dist_sq = euclidean_distances(</span><br><span class="line">        centers[<span class="number">0</span>, np.newaxis], X, Y_norm_squared=x_squared_norms,</span><br><span class="line">        squared=<span class="keyword">True</span>)</span><br><span class="line">    current_pot = closest_dist_sq.sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pick the remaining n_clusters-1 points</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">1</span>, n_clusters):</span><br><span class="line">        <span class="comment"># Choose center candidates by sampling with probability proportional</span></span><br><span class="line">        <span class="comment"># to the squared distance to the closest existing center</span></span><br><span class="line">        rand_vals = random_state.random_sample(n_local_trials) * current_pot</span><br><span class="line">        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),</span><br><span class="line">                                        rand_vals)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute distances to center candidates</span></span><br><span class="line">        distance_to_candidates = euclidean_distances(</span><br><span class="line">            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decide which candidate is the best</span></span><br><span class="line">        best_candidate = <span class="keyword">None</span></span><br><span class="line">        best_pot = <span class="keyword">None</span></span><br><span class="line">        best_dist_sq = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">for</span> trial <span class="keyword">in</span> range(n_local_trials):</span><br><span class="line">            <span class="comment"># Compute potential when including center candidate</span></span><br><span class="line">            new_dist_sq = np.minimum(closest_dist_sq,</span><br><span class="line">                                     distance_to_candidates[trial])</span><br><span class="line">            new_pot = new_dist_sq.sum()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store result if it is the best local trial so far</span></span><br><span class="line">            <span class="keyword">if</span> (best_candidate <span class="keyword">is</span> <span class="keyword">None</span>) <span class="keyword">or</span> (new_pot &lt; best_pot):</span><br><span class="line">                best_candidate = candidate_ids[trial]</span><br><span class="line">                best_pot = new_pot</span><br><span class="line">                best_dist_sq = new_dist_sq</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Permanently add best center candidate found in local tries</span></span><br><span class="line">        <span class="keyword">if</span> sp.issparse(X):</span><br><span class="line">            centers[c] = X[best_candidate].toarray()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            centers[c] = X[best_candidate]</span><br><span class="line">        current_pot = best_pot</span><br><span class="line">        closest_dist_sq = best_dist_sq</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> centers</span><br></pre></td></tr></table></figure><p>该算法的是基于 k-means++:the advantages of careful seeding[2]实现的，有兴趣的可以看一下这篇论文。代码第49行，可以看到，第一个初始中心是随机初始化的。代码62行，通过循环，依次初始化其他的聚类中心。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>Lloyd, Stuart P. Least squares quantization in PCM[J]. IEEE Transactions on Information Theory, 1982, 28(2):129-137.</li><li>Arthur D, Vassilvitskii S. k-means++:the advantages of careful seeding[C]// Eighteenth Acm-Siam Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, 2007:1027-1035.</li><li>Julyedu 机器学习算法班</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​聚类，是机器学习的任务之一。同分类算法一样，聚类算法也被广泛的应用在各个领域，如根据话题，对文章、网页和搜索结果做聚类；根据社区发现对社交网络中的用户做聚类；根据购买历史记录对消费者做聚类。和分类算法不同的是，聚类算法的样本是没有标签的，也就是说，我们并不知道样本有哪些类
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="algorithm" scheme="http://conghuai.me/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Java多线程总结</title>
    <link href="http://conghuai.me/2017/06/20/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <id>http://conghuai.me/2017/06/20/Java多线程总结/</id>
    <published>2017-06-20T15:26:49.000Z</published>
    <updated>2018-09-24T15:26:45.112Z</updated>
    
    <content type="html"><![CDATA[<h1 id="并发基础"><a href="#并发基础" class="headerlink" title="并发基础"></a>并发基础</h1><p>CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停地切换线程执行，让我们感觉多个线程是同时执行的。    CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下一次切换回这个任务时，可以再加载这个任务的状态。</p><ul><li>上下文切换需要时间和开销。</li><li>减少上下文切换的方法：无锁并发编程、CAS算法、使用最少线程和使用协程。</li></ul><h2 id="三个重要概念"><a href="#三个重要概念" class="headerlink" title="三个重要概念"></a>三个重要概念</h2><ul><li><strong>原子性</strong>：一个操作或者多个操作，要么都成功；要么都失败，中间不能由于任何的因素取消。</li><li><strong>可见性</strong>：一个线程对共享变量值的修改，能够及时地被其他线程看到。<ul><li>共享变量：如果一个变量在多个线程的工作内存中都存在副本，那么这个变量就是几个线程的共享变量。 </li></ul></li><li><strong>有序性</strong><ul><li>代码的执行顺序，编写在前面的发生在编写在后面的</li><li>unlock必须发生在lock之后</li><li>volatile修饰的变量，对变量的写操作先于该变量的读操作</li><li>传递规则，操作A先于B，B先于C，那么A肯定先于C</li><li>线程启动规则，start方法肯定先于线程run</li><li>线程中断规则，interrupt这个动作，必须发生在捕获该动作之前</li><li>对象销毁规则，初始化必须发生在finalize之前</li><li>线程终结规则，所有的操作都发生在线程死亡之前</li></ul></li></ul><h2 id="线程安全"><a href="#线程安全" class="headerlink" title="线程安全"></a>线程安全</h2><ul><li>安全性的含义是，永远不发生糟糕的事情，当多个线程访问某个类时，这个类始终都能表现出正确的行为。</li><li>核心在于要对状态访问操作进行管理，特别是对共享的和可变的状态的访问。对象的状态指存储在状态变量中的数据。</li><li>修复多个线程访问同一个可变的状态变量，不使用同步的方法：<ul><li>不在线程之间共享该状态变量</li><li>将状态遍历修改为不可变的变量</li></ul></li></ul><h3 id="无状态性"><a href="#无状态性" class="headerlink" title="无状态性"></a>无状态性</h3><ul><li>不包含任何域，也不包含任何对其他类中域的引用。计算过程中的临时状态仅存在于线程栈上的局部变量中，并且只能由正在执行的线程访问。多个线程之间没有共享状态。</li><li>无状态对象一定是线程安全的。</li></ul><h3 id="竞态条件"><a href="#竞态条件" class="headerlink" title="竞态条件"></a>竞态条件</h3><ul><li>由于多线程共享相同的内存地址空间，并且是并发运行的，因此它们可能会访问或修改其他线程正在使用的变量。</li><li>当某个计算正确性取决于多个线程的交替执行时序时，就会发生竟态条件。观察结果的失效性是大多数竟态条件的本质：<strong>基于一种可能失效的观察结果来做出判断或者执行某个计算</strong>。</li><li>常见的竟态条件<ul><li><strong>先检查后执行</strong>：延迟初始化</li></ul></li></ul><h1 id="重要的概念"><a href="#重要的概念" class="headerlink" title="重要的概念"></a>重要的概念</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a>Thread</h2><ul><li><p>Thread是线程对象，用来创建和开始一个线程。</p></li><li><p>Thread().start()方法用来启动线程，里面会执行Runnable对象的run()方法，这是采用的是策略设计模式。</p></li><li><p>start0()是native方法，用c++写的，与底层交互。</p></li><li><p>Thread的命名规则：创建线程对象，默认有一个线程名，从Thread-0开始一次加1。</p></li><li><p>如果在构造Thread的时候，没有传递Runnable或者没有复写，则该方法不会调用任何东西。</p></li><li><p>如果构造线程对象时，未传入ThreadGroup，此时会用父线程的ThreadGroup。</p></li><li><p>构造Thread的时候传入stacksize代表着该线程占用的stack大小，如果没有指定stacksize的大小，默认是0, 0代表着会忽略该参数，该参数会被JNI函数去使用。注意：该参数在一些平台有效，在一些平台无效。</p></li><li><p>可以通过ThreadFactory的工厂类来创建线程对象。</p></li><li><p>| name         |<br>| ———— |<br>| priority     |<br>| group        |<br>| tid          |<br>| threadStatus |</p></li></ul><h2 id="Runnable"><a href="#Runnable" class="headerlink" title="Runnable"></a>Runnable</h2><p>Runnable是给线程执行的对象，必须重写run方法，该方法供对象调用。</p><h2 id="Volatile"><a href="#Volatile" class="headerlink" title="Volatile"></a>Volatile</h2><ul><li><strong>volatile</strong>：Java编程语言允许线程访问共享变量，为了确保变量能够准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。将当前处理器缓存行的数据写回到系统内存。<strong>这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效</strong>。<ul><li>volatile变量不会被缓存在寄存器或者其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回<strong>最新</strong>写入的值。</li><li>在访问volatile变量时不会执行加锁操作，但是我们可以把它的行为想成加锁的get和set方法。</li><li><strong>加锁机制既可以确保可见性又可以确保原子性，而volatile变量只能确保可见性</strong>。</li><li>使用条件：<ul><li>对变量的写入操作不依赖变量的当前值，或者你能确保只有单个线程更新变量的值。</li><li>该变量不会与其他状态变量一起纳入不变性条件中。</li><li>在访问变量时不需要加锁。</li></ul></li><li>典型用法：<ul><li>检查某个状态标记以判断是否退出循环。</li></ul></li></ul></li><li>CPU引入cache解决了速度问题，但是又引入了缓存不一致的问题。当CPU在操作的时候，会先在缓存中进行操作，然后将缓存中的内容刷新到CPU中。<br>​          <strong>main memory -&gt; cache （进行操作 如 i++）-&gt; cache -&gt; main memory</strong><br>​          <strong>main memory -&gt; cache （进行操作如 i++）-&gt; cache -&gt; main memory</strong></li></ul><ul><li>解决缓存不一致的方法如下：</li><li><ul><li>给数据总线加锁：当第一个线程访问主内存的时候就对这块内存区域加锁，在线程操作完并写回主内存之前，其他的线程都不可以读取这块主内存的内容。（效率低下）</li><li>CPU高速缓存一致性协议：Intel MESI，核心思想如下：</li><li><ul><li>当cpu写入数据的时候，如果发现该变量被共享（也就是说，在其他cpu也存在该变量的副本），会发出一个信号，通知其他cpu该变量的缓存无效。</li><li>当其他线程访问该变量，重新到内存中读取该变量。</li></ul></li></ul></li></ul><p>结合Java内存模型分析图如下:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-162034.jpg" alt=""></p><p>volatile关键字可以理解为就是缓存一致性协议。保证各个线程中的缓存内容是一致的。</p><h2 id="Wait-Set"><a href="#Wait-Set" class="headerlink" title="Wait Set"></a>Wait Set</h2><ul><li>如果某个线程调用<strong>LOCK.wait()</strong>，那么该线程会把自己放到<strong>该锁</strong>的wait set中。 </li><li>每一个对象都会有一个wait set，用来存放调用了该对象wait方法之后进入block状态线程</li><li>wait set 本身是一个抽象的概念，具体的实现方式交给不同的JVM去实现。</li><li>线程被notify之后，不一定立即得到执行，线程从wait set中被唤醒顺序不一定是FIFO。</li><li>线程从wait set中被唤醒后<strong>还需要去竞争LOCK</strong> ，但是抢完锁之后，会直接从<strong>地址恢复后</strong>的位置继续向下执行。</li></ul><h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><ul><li>对于费时的任务，我们不希望当前线程等待着任务执行结束才做其他事情，我们希望当前线程可以继续执行后面的操作，我们通过Future对象这个凭证来告知之前任务的情况。（实际上，我们会另开一个线程来执行任务）。</li><li>这种模式中的角色：<ul><li>Future: 代表的是未来的一个凭据</li><li>FutureTask: 将你的调用逻辑进行隔离</li><li>FutureService: 桥接 Future和 FutureTask</li></ul></li></ul><h2 id="读写锁"><a href="#读写锁" class="headerlink" title="读写锁"></a>读写锁</h2><p>读写分离，提高效率；</p><ol><li>read read 并行化</li><li>read write 不允许</li><li>write write 不允许</li></ol><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">READ</th><th style="text-align:center">WRITE</th></tr></thead><tbody><tr><td style="text-align:center"><strong>READ</strong></td><td style="text-align:center">NO</td><td style="text-align:center">YES</td></tr><tr><td style="text-align:center"><strong>WRITE</strong></td><td style="text-align:center">YES</td><td style="text-align:center">YES</td></tr></tbody></table><h2 id="不可变对象"><a href="#不可变对象" class="headerlink" title="不可变对象"></a>不可变对象</h2><p>不可变对象一定是线程安全的，可变对象不一定是不安全的。</p><ul><li>属性定义为private final</li><li>不提供任何改变这个变量的方法</li><li>不让子类继承，即类定义为final</li></ul><p>在JDK中有哪些是不可变的对象呢？</p><ul><li>String</li></ul><p>谈到了String，我们来讲一讲String，StringBuffer和StringBuilder的区别：</p><ol><li>三者中，只有String是不可变的，StringBuffer和StringBuilder都是可变的。</li><li>三者中，String和StringBuffer都是线程安全的，StringBuilder不是线程安全的。前两者保证线程安全的方式不一样，其中String是通过不可变对象来保证的，而StringBuffer是通过synchronized关键字进行加锁保证的，所以，StringBuffer效率比较低。</li></ol><h2 id="线程封闭"><a href="#线程封闭" class="headerlink" title="线程封闭"></a>线程封闭</h2><ul><li>仅在单线程内访问数据，不共享数据。</li><li>栈封闭是线程封闭的一种特例，局部变量的固有属性之一就是封闭在执行线程中，它们位于执行线程的栈中，其他线程无法访问这个栈。</li><li>维持线程封闭性的一种更规范方法是使用<strong>ThreadLocal</strong>, 这个类能使线程中的某个值与保存值的对象关联起来。ThreadLocal提供了get与set等访问接口或方法，这些方法为每个使用该变量的线程都存有一份独立的副本， 因此get总是返回由当前执行线程在调用set时设置的最新值。<ul><li>当某个线程初次调用ThreadLocal.get方法时，就会调用initialValue来获取初始值。</li><li>从概念上来讲，可以将<code>ThreadLocal&lt;T&gt;</code>视为包含了<code>Map&lt;Thread,T&gt;</code>对象，其中保存了特定于该线程的值。</li><li>ThreadLocalRandom是线程本地变量，每个生成随机数的线程都有一个不同的生成器。但都在同一个类中被管理，对于程序员来说时透明的。</li><li>相比于使用共享的Random对象为所有线程生成随机数，这种机制具有更好的性能。</li></ul></li></ul><h1 id="原子操作、原子变量"><a href="#原子操作、原子变量" class="headerlink" title="原子操作、原子变量"></a>原子操作、原子变量</h1><p>原子操作是指，对于访问同一个状态的所有操作来说，这个操作是一个以原子方式执行的操作。</p><p>要保持状态的一致性，就需要在单个原子操作中更新所有相关的状态变量。</p><p>每个Java对象都可以被用做一个实现同步的锁，这些锁被称为内置锁或监视锁。线程在进入同步代码块之前会自动获得锁，并且在退出后自动释放锁。</p><p>Java的内置锁相当于一个互斥体，这意味着最多只有一个线程能持有这种锁。这个锁保护的同步代码块会以原子方式执行。</p><p>每个共享的和可变的变量都应该只由一个锁来保护。</p><p>对于每个包含多个变量的不变性条件，其中涉及的所有变量都需要由同一个锁来保护。</p><p>在访问某个共享且可变的变量时要求所有线程在同一个锁上同步，是为了确保某个线程写入该变量的值对于其他线程来说是可见的。</p><p>加锁的含义不仅仅局限于互斥行为，还包括<strong>内存可见性</strong>，为了确保所有线程都能看到共享变量的最新值，所有执行读操作或者写操作的线程都必须在同一个锁上同步。</p><h2 id="同步锁"><a href="#同步锁" class="headerlink" title="同步锁"></a>同步锁</h2><p><strong>Synchronized</strong>：使得共享资源在多个线程之间访问时采用了同步的方式，避免线程之间的安全问题。</p><ul><li>直接使用synchronized给对象和方法加锁，默认使用的<strong>this</strong>这把锁。同一时刻只有一个执行线程被允许访问，其他线程如果试图访问这个对象的其他方法，都会被挂起。</li><li>如果给静态的方法加锁，那么此时使用的是类名.class 这把锁。对静态方法加锁，同时只能够被一个执行线程访问，但是其他线程可以访问这个对象的非静态方法。</li><li>Java中的每一个对象都可以作为锁。<ul><li>对于普通同步方法，锁是当前实例对象。</li><li>对于静态同步方法，锁是当前类的Class对象。</li><li>对于同步方法块，锁是synchonized括号里配置的对象。</li></ul></li></ul><h2 id="Lock"><a href="#Lock" class="headerlink" title="Lock"></a>Lock</h2><p>Lock是一种比synchonized关键字更强大也更灵活的机制。</p><ul><li>支持更灵活的同步代码块结构。</li><li>提供<strong>tryLock()</strong>方法的实现。这个方法试图获取锁，如果锁已被其他线程获取，他将返回false并继续往下执行代码。</li><li><strong>Lock</strong>允许分离读和写操作，允许多个读线程和一个写线程。</li><li><strong>Condition：</strong><ul><li>一个锁可能关联一个或者多个条件，这些条件通过<strong>Condition</strong>接口声明，目的是允许线程获取锁并且查看等待的某一个条件是否满足。如果不满足就挂起直到某个线程唤醒它们。</li><li>与锁绑定的所有条件对象都是通过<strong>Lock</strong>接口声明的<strong>newCondition</strong>()方法创建的，在使用条件的时候，必须获取这个条件绑定的锁。所以带条件的代码必须在调用Lock对象的lock()和unlock()方法之间。</li><li>当线程调用条件<strong>await</strong>()方法时，它将自动释放这个条件绑定的锁，其他某个线程才可以获取这个锁并且执行相同的操作，或者执行这个锁保护的另一个临界区代码。</li><li><strong>awaitUninterruptibly</strong>()是不可中断的，这个线程将休眠直到其他某个线程调用了将它挂起的条件的<strong>signal</strong>()或<strong>signalAll</strong>()方法。</li></ul></li></ul><h2 id="原子变量"><a href="#原子变量" class="headerlink" title="原子变量"></a>原子变量</h2><ul><li>它能保证<strong>可见性</strong>、<strong>有序性</strong>、<strong>原子性</strong>。</li><li>原子变量是从Java5开始引入的，它提供了单个变量上的原子操作。</li><li>一般来说，这种操作先获取变量值，然后在本地改变变量的值，然后试图用这个改变的值去替换之前的值。如果之前的值没有被其他线程改变，就可以执行这个替换操作。否则，方法将再执行这个操作。这种操作称为<strong>CAS</strong>原子操作。</li><li>采用比较和交换机制不需要使用同步机制，不仅可以避免死锁，而且性能更好。</li><li>在Java中可以通过<strong>锁</strong>和<strong>循环CAS</strong>的方式来实现原子操作。存在的问题：<ul><li><strong>ABA</strong>问题 如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化。那么解决该问题的方法是使用版本号，每次变量更新的时候把版本号加1。JDK提供了一个类AtomicStampedReference来解决ABA问题。</li><li>循环时间开销大。</li><li>只能保证一个共享变量的原子操作。</li></ul></li></ul><h3 id="CAS-Compare-and-swap"><a href="#CAS-Compare-and-swap" class="headerlink" title="CAS(Compare-and-swap)"></a>CAS(Compare-and-swap)</h3><p>In <a href="https://www.wikiwand.com/en/Computer_science" target="_blank" rel="noopener">computer science</a>, <strong>compare-and-swap</strong> (<strong>CAS</strong>) is an <a href="https://www.wikiwand.com/en/Atomic_(computer_science" target="_blank" rel="noopener">atomic</a>) <a href="https://www.wikiwand.com/en/Instruction_(computer_science" target="_blank" rel="noopener">instruction</a>) used in <a href="https://www.wikiwand.com/en/Thread_(computer_science" target="_blank" rel="noopener">multithreading</a>#Multithreading) to achieve <a href="https://www.wikiwand.com/en/Synchronization_(computer_science" target="_blank" rel="noopener">synchronization</a>). It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">compare_and_swap</span><span class="params">(<span class="keyword">int</span>* reg, <span class="keyword">int</span> oldval, <span class="keyword">int</span> newval)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  ATOMIC();</span><br><span class="line">  <span class="keyword">int</span> old_reg_val = *reg;</span><br><span class="line">  <span class="keyword">if</span> (old_reg_val == oldval)</span><br><span class="line">     *reg = newval;</span><br><span class="line">  END_ATOMIC();</span><br><span class="line">  <span class="keyword">return</span> old_reg_val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CAS比较与交换的伪代码可以表示为：</p><p>do{<br>​       备份旧数据；<br>​       基于旧数据构造新数据；<br>}while(!CAS( 内存地址，备份的旧数据，新数据 ))  </p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-17-160954.jpg" alt="191145387966044"></p><p>（上图的解释：CPU去更新一个值，但如果想改的值不再是原来的值，操作就失败，因为很明显，有其它操作先改变了这个值。）</p><p>就是指当两者进行比较时，如果相等，则证明共享数据没有被修改，替换成新值，然后继续往下运行；如果不相等，说明共享数据已经被修改，放弃已经所做的操作，然后重新执行刚才的操作。容易看出 CAS 操作是基于共享数据不会被修改的假设，采用了类似于数据库的 commit-retry 的模式。当同步冲突出现的机会很少时，这种假设能带来较大的性能提升。</p><h3 id="CAS的问题："><a href="#CAS的问题：" class="headerlink" title="CAS的问题："></a>CAS的问题：</h3><p>Some CAS-based algorithms are affected by and must handle the problem of a <a href="https://www.wikiwand.com/en/Type_I_error#False_negative_vs._false_positive" target="_blank" rel="noopener">false positive</a> match, or the <a href="https://www.wikiwand.com/en/ABA_problem" target="_blank" rel="noopener">ABA problem</a>. It is possible that between the time the old value is read and the time CAS is attempted, some other processors or threads change the memory location two or more times such that it acquires a bit pattern which matches the old value. </p><p>解决的方式一般都是多设置一个计数器或者说设置一个版本号。</p><h1 id="线程封闭-1"><a href="#线程封闭-1" class="headerlink" title="线程封闭"></a><strong>线程封闭</strong></h1><ul><li>仅在单线程内访问数据，不共享数据。</li><li>栈封闭是线程封闭的一种特例，局部变量的固有属性之一就是封闭在执行线程中，它们位于执行线程的栈中，其他线程无法访问这个栈。</li><li>维持线程封闭性的一种更规范方法是使用<strong>ThreadLocal</strong>, 这个类能使线程中的某个值与保存值的对象关联起来。ThreadLocal提供了get与set等访问接口或方法，这些方法为每个使用该变量的线程都存有一份独立的副本， 因此get总是返回由当前执行线程在调用set时设置的最新值。<ul><li>当某个线程初次调用ThreadLocal.get方法时，就会调用initialValue来获取初始值。</li><li>从概念上来讲，可以将<code>ThreadLocal&lt;T&gt;</code>视为包含了<code>Map&lt;Thread,T&gt;</code>对象，其中保存了特定于该线程的值。</li><li>ThreadLocalRandom是线程本地变量，每个生成随机数的线程都有一个不同的生成器。但都在同一个类中被管理，对于程序员来说时透明的。</li><li>相比于使用共享的Random对象为所有线程生成随机数，这种机制具有更好的性能。</li></ul></li></ul><h2 id="ThreadLocal"><a href="#ThreadLocal" class="headerlink" title="ThreadLocal"></a>ThreadLocal</h2><ul><li>可以理解为用HashMap存储数据，Key是Thread，Value是数据。</li><li>通过重写<code>initialValue</code>方法来定义初始值。</li><li>可以用于线程上下文带来的参数传递。</li></ul><h1 id="并发数据结构"><a href="#并发数据结构" class="headerlink" title="并发数据结构"></a>并发数据结构</h1><h2 id="并发集合"><a href="#并发集合" class="headerlink" title="并发集合"></a>并发集合</h2><p>Java提供了一些可以用于并发程序中的数据结合，分阻塞式和非阻塞式的。</p><h3 id="ConcurrentHashMap"><a href="#ConcurrentHashMap" class="headerlink" title="ConcurrentHashMap"></a>ConcurrentHashMap</h3><p>ConcurrentHashMap也是基于散列的Map，但它使用了不同的加锁策略来提供更高的并发性和伸缩性。采用了一种粒度更细的加锁机制来实现更大程度的共享，称为分段锁。</p><h3 id="CopyOnWriteArrayList"><a href="#CopyOnWriteArrayList" class="headerlink" title="CopyOnWriteArrayList"></a>CopyOnWriteArrayList</h3><p>在迭代期间不需要对容器进行加锁或复制。在每次修改时，都会创建并重新发布一个新的容器副本，从而实现可变性。</p><p>仅当迭代操作远远多于修改操作时，才应该使用写入时复制。</p><h3 id="BlockingQueue"><a href="#BlockingQueue" class="headerlink" title="BlockingQueue"></a>BlockingQueue</h3><p>BlockingQueue简化了生产者-消费者设计的实现过程，它支持任意数量的生产者和消费者。</p><p>一种最常见的生产者-消费者设计模式就是线程池与工作队列的组合，在Executor任务执行框架中就体现这种模式。</p><ul><li>LinkedBlockingQueue</li><li>ArrayBlockingQueue</li><li>SynchronousQueue</li><li>Deque</li><li>ConcurrentLinkedDeque</li><li>LinkedBlockingDeque</li><li>PriorityBlockingQueue</li><li>DelayQueue</li><li>ConcurrentSkipListMap</li></ul><h2 id="线程通信"><a href="#线程通信" class="headerlink" title="线程通信"></a>线程通信</h2><p>我们希望多个线程之间能够相互之间交换信息，从而能够更好的协作，完成任务。线程之间通过wait notify方法进行通信。同步工具类可以是任何一个对象，只要它根据其自身的状态来协调线程的控制流，阻塞队列可以作为同步工具类，其他类型的同步工具类包括<strong>信号量</strong>、<strong>栅栏</strong>、以及<strong>闭锁</strong>。</p><p>线程之间的通信机制有两种：共享内存和消息传递。</p><ul><li>共享内存，线程之间共享程序的公共状态，通过写-读内存中的公共状态来隐式通信。</li><li>消息传递，线程之间必须通过发送消息来显式通信。</li></ul><h3 id="Interrupt-wait-notify"><a href="#Interrupt-wait-notify" class="headerlink" title="Interrupt wait notify"></a>Interrupt wait notify</h3><ul><li>Thread提供了<strong>interrupt</strong>方法，用于中断线程或者查询线程是否已经被中断。每个线程都有一个布尔类型的属性，表示线程的中断状态。</li><li>中断是一种协作机制。当线程A中断B时，A仅仅是要求B在执行到某个可以暂停的地方停止正在执行的操作-<strong>前提是如果线程B愿意停止下来</strong>。</li><li>调用interrupt并不意味着立即停止目标线程正在进行的工作，而是传递了请求中断的消息。</li><li>由于每个线程拥有各自的中断策略，因此除非你知道中断对该线程的含义，否则就不应该中断这个线程。</li><li>所有的对象都会有一个wait set， 用来存放调用了该对象wait方法之后进入block状态线程。</li><li>线程从wait set中被唤醒顺序不一定是FIFO。</li><li>wait方法会释放锁对象，当被唤醒之后需要再去请求锁对象，得到锁对象之后，代码会从wait的地方开始往下执行。</li></ul><h3 id="Latch"><a href="#Latch" class="headerlink" title="Latch"></a>Latch</h3><p>闭锁是一种不同工具类，可以延迟线程的进度直到其到达终止状态。闭锁相当于一扇门，在闭锁到达结束状态之前，这扇门一直是关闭的。闭锁可以用来确保某些活动直到其他活动都完成后才继续执行。</p><ul><li><strong>CountDownLatch</strong><ul><li>CountDownLatch是一种灵活的闭锁实现，可以在上述各种情况使用，它可以是一个或多个线程等待一组事件发生。</li><li>这个类使用一个<strong>整数</strong>进行初始化，这个整数就是线程要等待完成的操作的数目。当一个线程要等待某些操作先执行完时，需要调用await()方法，这个方法让线程进入休眠直到等待的所有操作都完成，当某个操作完成后，它将调用countDown()方法将CountDownLatch类的内部计数器减1,，当计数器变为0的时候，CountDownLatch将唤醒所有调用await()方法而进入休眠的线程。</li><li>CountDownLatch类有三个基本元素：<ul><li>一个初始值，即定义必须等待的先行完成的操作的数目。</li><li>await()方法</li><li>countDown()方法，每个被等待的事件在完成的时候调用。</li></ul></li><li>CountDownLatch机制<ul><li>CountDownLatch机制不是用来保护共享资源或者临界区的，它是用来同步执行多个任务的一个或者多个线程；</li><li>CountDownLatch只准许进入一次。</li></ul></li></ul></li></ul><h3 id="Counting-Semaphore"><a href="#Counting-Semaphore" class="headerlink" title="Counting Semaphore"></a>Counting Semaphore</h3><p>计算信号量用来控制同时访问某个特定资源的操作数量，或者执行某个指定操作的数量。</p><p>Semaphore中管理着一组虚拟的许可，许可的初始数量可通过构造函数来指定，在执行操作时可以首先获得许可，并在使用以后释放许可。如果没有许可，那么acquire将阻塞直到有许可。release方法将返回一个许可给信号量。</p><p>Semaphore可以用来实现资源池。</p><h3 id="Barrier"><a href="#Barrier" class="headerlink" title="Barrier"></a>Barrier</h3><p>栅栏类似于闭锁，它能阻塞一组线程直到某个事件发生。栅栏和闭锁的关键区别在于，所有线程必须同时到达栅栏位置，才能继续执行。闭锁用于等待事件，而栅栏用于等待其他线程。</p><ul><li><strong>CyclicBarrier</strong><ul><li>CyclicBarrier可以使一定数量的参与方反复的在栅栏位置汇集。</li><li>CyclicBarrier类使用一个整型数进行初始化，这个数是需要在某个点上同步的线程数。</li><li>当一个线程到达的指定的点后，它将调用await()方法等待其他的线程，当线程调用await()方法后，CyclicBarrier类将阻塞这个线程并使之休眠直到所有其他线程到达。当最后一个线程调用CyclicBarrier类的await()方法时，CyclicBarrier对象将唤醒所有在等待的线程，然后这些线程将继续执行。</li><li>CyclicBarrier类有一个很有意义的改进，即它可以传入另一个Runnable对象作为初始化参数。当所有的线程都到达集合点后，CyclicBarrier类将这个Runnable对象作为线程执行。</li></ul></li></ul><h3 id="Phaser"><a href="#Phaser" class="headerlink" title="Phaser"></a><strong>Phaser</strong></h3><p>允许执行并发多阶段任务。当我们有并发任务并且需要分解成几步执行时，这种机制就非常适用。Phaser类机制是在每一步结束的位置对线程进行同步，当所有的线程都完成了这一步，才允许执行下一步。</p><h3 id="Exchanger"><a href="#Exchanger" class="headerlink" title="Exchanger"></a>Exchanger</h3><p>Exchanger允许在并发任务之间交换数据。Exchanger类允许在两个线程之间定义同步点，当两个线程都到达同步点时，它们交换数据结构，因此第一个线程的数据结构进入到第二个线程中，同时第二个线程的数据结构进入到第一个线程中。</p><h2 id="并发编程框架"><a href="#并发编程框架" class="headerlink" title="并发编程框架"></a>并发编程框架</h2><h3 id="Executor框架"><a href="#Executor框架" class="headerlink" title="Executor框架"></a>Executor框架</h3><p>任务是一组逻辑工作单元，而线程则是使任务异步执行的机制。串行执行的问题在于其糟糕的响应性和吞吐量，而“为每个任务分配一个线程”的问题在于资源管理的复杂性。</p><p>通过使用Executor，可以实现各种調优、管理、监视、记录日志、错误报告和其他功能，如果不使用任务执行框架，那么要增加这些功能非常困难。</p><p>当运行大量的并发任务，手动创建Runnable对象和Thread对象来执行它们有如下缺点:</p><ul><li>必须实现所有与Thread对象管理相关的代码，比如线程的创建、结束以及结果获取。</li></ul><p>执行器的另一个优势是Callable接口，这个接口的主方法名称为call()，可以返回结果。当发送一个Callable对象给执行器时，将获得一个实现了Future接口的对象，可以使用这个对象控制Callable对象的状态和结果。</p><p>执行器框架(Executor Framework)将任务的创建和执行进行了分离，通过这个框架，只需要实现Runnable接口的对象和使用Executor对象，然后将Runnable对象发送给执行器。执行器再负责运行这些任务所需要的线程，包括线程的创建，线程的管理以及线程的结束。</p><p>它提供了一种标准的方法将任务的提交过程与执行过程解偶开来，并用Runnable来表示任务。</p><p>Executor的实现还提供了生命周期的支持，以及统计信息收集，应用程序管理机制和性能监视等机制。</p><p>Executor基于生产者-消费者模式，提交任务的操作相当于生产者，执行任务的线程则相当于消费者。</p><h3 id="ExecutorService"><a href="#ExecutorService" class="headerlink" title="ExecutorService"></a>ExecutorService</h3><ul><li>为了解决执行服务的生命周期问题，Executor扩展了ExecutorService接口，添加了一些用于生命管理的方法。</li><li>在ExecutorService关闭后提交的任务将由“拒绝执行处理器”来处理，它会抛弃任务，或者使得execute方法将转入终止状态。</li></ul><h3 id="ThreadPool"><a href="#ThreadPool" class="headerlink" title="ThreadPool"></a>ThreadPool</h3><ul><li>线程池，是指管理一组同构工作线程的资源池。</li><li>在线程池中执行任务比为每个任务分配一个线程优势更多，通过重用现有的线程而不是创建新线程，可以在处理多个请求时分摊在线程创建和销毁过程中产生的巨大开销。</li><li>可以通过调用Executors中的静态工厂方法之一来创建一个线程池。</li><li>当有界队列被填满后，饱和策略开始发挥作用。ThreadPoolExecutor的饱和策略可以通过调用setRejectedExecutionHandler来修改。<ul><li><strong>AbortPolicy</strong>：中止策略，是默认的饱和策略，该策略抛出未经检查的RejectedExecutionException。</li><li><strong>CallerRunsPolicy</strong>：调用者策略，实现了一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。</li></ul></li><li>每当线程池需要创建一个线程时，都是通过线程工厂方法来完成的。默认的线程工厂方法将创建一个新的、非守护的线程，并且不包含特殊的配置信息。在ThreadFactory中只定义了一个方法newThread，每当线程池需要创建一个新线程时，都会调用这个方法。</li></ul><table><thead><tr><th style="text-align:center">方法</th><th style="text-align:center">作用</th></tr></thead><tbody><tr><td style="text-align:center">newFixedThreadPool</td><td style="text-align:center">创建一个固定长度的线程池；<br>在默认情况下将使用一个无界的LinkedBlockingQueue。</td></tr><tr><td style="text-align:center">newCachedThreadPool</td><td style="text-align:center">1. 创建一个可缓存的线程池，如果线程池的当前规模超过了处理需求时，那么将回收空闲的线程，而当需求增加时，则可以添加新的线程，线程池的规模不存在任何限制。<br>2. 对于非常大的或者无界的线程池，可以通过使用SynchronousQueue来避免任务排队。<br>3. 只有当线程池是无界的或者可以拒绝任务时，SynchronousQueue才有实际价值。</td></tr><tr><td style="text-align:center">newSingleThreadExecutor</td><td style="text-align:center">1. 是一个单线程的Executor，它创建单个工作者线程来执行任务，如果这个线程异常结束，会创建另一个线程来替代。newSingleThreadExecutor能确保依照任务在队列中的顺序来串行执行。<br></td></tr><tr><td style="text-align:center">newScheduledThreadPool</td><td style="text-align:center">1. 创建一个固定长度的线程池，而且以延迟或定时的方式来执行任务，类似于Timer。<br>2.为了在定时执行器中等待一段给定的时间后执行一个任务，需要使用schedule()方法。</td></tr><tr><td style="text-align:center">Customized ThreadPoolExecutor</td><td style="text-align:center">1. Java并发API提供了大量接口和类来实现并发应用程序，这些接口和类既包含底层机制，如Thread类，Runnable接口或Callable接口，synchronized关键字，也包含了高层机制，如Executor框架，尽管如此，开发应用中，仍会发现已有的Java类无法满足需求。<br>2. 步骤：<br>2.1 实现一个接口以拥有接口定义的功能，如 ThreadFactory接口。<br>2.2 覆盖类的一些方法，改变这些方法的行为，来满足需求，例如，覆盖Thread类的run()方法。<br>3. 任务：<br>3.1 通过Runnable接口实现的任务，不返回结果。<br>3.2 通过Callable接口实现的任务，它返回结果。</td></tr></tbody></table><h3 id="Callable"><a href="#Callable" class="headerlink" title="Callable"></a>Callable</h3><ul><li>Callable这个接口声明了call()方法，可以在这个方法里实现任务的具体逻辑操作。Callable接口时一个泛型接口，这就意味着必须声明call()方法返回的数据类型。</li><li>Future这个接口声明了一些方法来获取由Callabel对象产生的结果，并管理它们的状态。</li></ul><h3 id="ThreadFactory"><a href="#ThreadFactory" class="headerlink" title="ThreadFactory"></a>ThreadFactory</h3><p>工厂模式在面向对象编程中是一个应用广泛的设计模式，它是一种创建模式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;并发基础&quot;&gt;&lt;a href=&quot;#并发基础&quot; class=&quot;headerlink&quot; title=&quot;并发基础&quot;&gt;&lt;/a&gt;并发基础&lt;/h1&gt;&lt;p&gt;CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停
      
    
    </summary>
    
      <category term="计算机基础与软件工程" scheme="http://conghuai.me/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E4%B8%8E%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="Java" scheme="http://conghuai.me/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E4%B8%8E%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/Java/"/>
    
      <category term="并发" scheme="http://conghuai.me/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E4%B8%8E%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/Java/%E5%B9%B6%E5%8F%91/"/>
    
    
      <category term="java" scheme="http://conghuai.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Perceptron Learning Algorithm</title>
    <link href="http://conghuai.me/2017/04/14/Perceptron-Learning-Algorithm/"/>
    <id>http://conghuai.me/2017/04/14/Perceptron-Learning-Algorithm/</id>
    <published>2017-04-14T07:25:03.000Z</published>
    <updated>2018-09-24T15:29:15.969Z</updated>
    
    <content type="html"><![CDATA[<p>感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。</p><h1 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h1><p>假设输入空间（特征空间）是$\chi \subseteq  R^n$，输出空间是$Y={+1, -1}$。输入$x\in \chi$表示实例的特征向量，对应于输入空间（特征空间）的点；输出为$y\in Y$表示实例的类别，由输入空间到输出空间的如下函数：</p><p>$$f(x) = sign(w\cdot x+b)$$</p><p>称为感知机。其中，$w$和$b$为感知机模型参数，$w\in R^n$称为权值向量，$b\in R$称为偏置，sign是符号函数，即：</p><p>$$\begin{split}sign(x)=\begin{cases} +1, &amp; \text{$x\geq 0$} \\ -1, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><p>感知机模型的假设空间是定义在特征空间中的所有线性分类模型，即函数集合$\{f|f(x)=w\cdot x + b\}$。</p><h2 id="几何解释"><a href="#几何解释" class="headerlink" title="几何解释"></a>几何解释</h2><p>线性方程$w\cdot x+b=0$，对应于特征空间$R^n$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分，位于两部分的点分别被称为正、负两类。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-084814.jpg" alt="Screen Shot 2018-04-14 at 16.14.35"></p><h1 id="学习策略"><a href="#学习策略" class="headerlink" title="学习策略"></a>学习策略</h1><p>假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数$w,b$，需要确定一个学习策略，即定义（经验）损失函数并将损失函数极小化。</p><p>损失函数选择是误分类点到超平面$S$的总距离：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-084817.jpg" alt="distance"></p><p>对于误分类的数据$(x_i,y_i)$来说，当$w\cdot x+b &gt;0$时，$y_i=-1$，而当$w\cdot x_i+b&lt;0$时，$y_i=+1$，则我们可以得出，误分类点$x_i$到超平面$S$的距离是：</p><p>$$-\frac{1}{||w||}y_i(w\cdot x_i+b)$$</p><p>这样，假设超平面$S$的误分类点集合为M，那么所有误分类点到超平面S的总距离为：</p><p>$$-\frac{1}{||w||}\sum _{x_i\in M}y_i(w\cdot x_i+b)$$</p><p>不考虑$\frac{1}{||w||}$，就得到感知机学习的损失函数：</p><p>$$L(w,b)=-\sum_{x_i \in M}y_i(w\cdot x_i+b)$$</p><h1 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h1><p>感知机学习算法是对以下最优化问题的算法那，给定一个训练数据集$T=\{(x_1, y_1),(x_2,y_2),…,(x_N,y_N)\}$，求参数$w,b$，使其为以下损失函数极小化问题的解：</p><p>$$min_{w,b} L(w,b)=-\sum_{x_i \in M}y_i(w\cdot x_i + b)$$</p><p>感知机学习算法是误分类驱动的，具体采用随机梯度下降法。首选，任意选取一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数。极小化过程中不是一次使M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。</p><p>假设误分类点集合M是固定的，那么损失函数$L(w,b)$的梯度由</p><p>$$\triangledown_wL(w,b)=-\sum_{x_i\in M}y_ix_i$$</p><p>$$\triangledown_bL(w,b)=-\sum_{x_i\in M}y_i$$</p><p>给出，随机选择一个误分类点$(x_i, y_i)$，对$w,b$进行更新：</p><p>$$w\leftarrow w+\eta y_ix_i$$</p><p>$$b\leftarrow b+\eta y_i$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。&lt;/p&gt;
&lt;h1 id=&quot;感知机模型&quot;&gt;&lt;a href=&quot;#感知机模型&quot; class=&quot;header
      
    
    </summary>
    
      <category term="机器学习" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="分类算法" scheme="http://conghuai.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>网页正文抽取算法</title>
    <link href="http://conghuai.me/2016/10/05/%E7%BD%91%E9%A1%B5%E6%AD%A3%E6%96%87%E6%8A%BD%E5%8F%96%E7%AE%97%E6%B3%95/"/>
    <id>http://conghuai.me/2016/10/05/网页正文抽取算法/</id>
    <published>2016-10-05T07:51:56.000Z</published>
    <updated>2018-04-09T05:47:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>在爆炸式增长的互联网时代，互联网上有大量的资源，如何爬取这些资源成为一个热门的研究点。其中如何高效的对网页正文进行抽取、清洗和存储成为一个重要的研究领域。但是，在网页上，除了正文部分，通常还会包含大量的导航栏、广告、版权等信息。相较于正文，这些信息对于我们来说用处不是很大，这部分信息，在网页正文抽取中，被称为噪声信息。为了提高网页正文采集的性能，我们需要把这这些噪声去除。</p><p>在这篇博文中，会介绍几个比较经典、效果也比较好的算法，一是<strong>CETD：Content Extraction via Text Density</strong>；二是<strong>CETR：Content Extraction via Tag Ratios。</strong>三是<strong>CEPR：Content Extraction via Path Ratios</strong>。</p><h1 id="CETD：Content-Extraction-via-Text-Density"><a href="#CETD：Content-Extraction-via-Text-Density" class="headerlink" title="CETD：Content Extraction via Text Density"></a>CETD：Content Extraction via Text Density</h1><p>该算法是主要思想是：在典型的网页结构中，噪声信息（指正文信息以外）通常被高度格式化，因此包含的文本信息通常很少，而正文通常包含大量文本。而且，正文通常在页面中保持完整性，即其内容通常不会被分到多个DOM树节点中。</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li>没有对页面结构作任何假设。</li><li>保持原页面的信息。</li></ul><h2 id="Text-Density"><a href="#Text-Density" class="headerlink" title="Text Density"></a>Text Density</h2><ul><li>CharNumber：该节点下所有子树中的字符数；</li><li>TagNumber：该节点下所有子树的标签数；</li></ul><p>定义Text Density为：$TD_i=\frac{C_i}{T_i}$,$C_i$表示CharNumber，$T_i$表示TagNumber，当$T_i$为0时，将其设置为1。通常来说，该值越高，该节点内容越有可能是正文。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Algorithm <span class="number">1</span> <span class="function">Pseudocode of <span class="title">ComputeDensity</span><span class="params">(N)</span></span></span><br><span class="line">1: INPUT: N </span><br><span class="line"><span class="number">2</span>: OUTPUT: N </span><br><span class="line"><span class="number">3</span>: <span class="keyword">for</span> all child node C in N <span class="keyword">do</span> </span><br><span class="line"><span class="number">4</span>:    ComputeDensity(C)</span><br><span class="line"><span class="number">5</span>: end <span class="keyword">for</span> </span><br><span class="line"><span class="number">6</span>: N.CharNumber ←CountChar(N) </span><br><span class="line"><span class="number">7</span>: N.TagNumber ←CountTag(N) </span><br><span class="line"><span class="number">8</span>: <span class="keyword">if</span> N.TagNumber == <span class="number">0</span> then </span><br><span class="line"><span class="number">9</span>:    N.TagNumber ←<span class="number">1</span></span><br><span class="line"><span class="number">10</span>: end <span class="keyword">if</span> <span class="number">11</span>: N.Density ←N.CharNumber/N.TagNumber</span><br></pre></td></tr></table></figure><h2 id="Composite-Text-Density"><a href="#Composite-Text-Density" class="headerlink" title="Composite Text Density"></a>Composite Text Density</h2><p>在Text Density的基础上加了关于超链接的统计信息。论文作者经过研究发现，大部分的噪声节点中都包含超链接，这个信息可以用来进一步判断该节点内容是正文还是噪声。基于这个发现，定义另外两个统计信息：</p><ul><li>LinkCharNumber：该节点下所有子树中的超链接字符数；</li><li>LinkTagNumber：该节点下所有子树中的超链接标签数；</li></ul><p>定义Composite Text Density为：$CTD_i=\frac{C_i}{T_i}log_{ln(\frac{C_i}{-LC_i}LC_i+\frac{LC_b}{C_b}C_i+e)}(\frac{C_i}{LC_i}\frac{T_i}{LT_i})$</p><ul><li>$C_i$：字符数；</li><li>$T_i$：标签数</li><li>$LC_i$：链接字符；</li><li>$-LC_i$：非链接字符；</li><li>$LT_i$：链接标签；</li><li>$LC_b$：<code>&lt;body&gt;</code>标签下的超链接字符数</li><li>$C_b$：<code>&lt;body&gt;</code>标签下的字符数</li></ul><h2 id="Content-Extraction"><a href="#Content-Extraction" class="headerlink" title="Content Extraction"></a>Content Extraction</h2><p>通过计算每个节点的TD或CTD，我们可以根据该值来判断是否抽取该节点下的文本当做正文，判断方式就是设置一个阈值，如果大于该阈值就抽取，小于该阈值就不抽取。论文作者用了一个非常巧妙的值当做阈值，即<code>&lt;body&gt;</code>的Text Density。</p><h2 id="DensitySum"><a href="#DensitySum" class="headerlink" title="DensitySum"></a>DensitySum</h2><p>在实践中发现，有一些正文包含的Text Density值很低，如正文的日期，正文的引用等。论文作者发现，一般来说，正文块都是属于DOM树中的某个祖先节点的，又因为正文节点的text density大于噪声节点，所以正文块对应的节点，如果把它所有孩子的text densities相加，将会得到最大的text densities值。从而可以通过DensitySum可以解决该问题，定义DensitySum为：$DensitySum_N=\sum_{i\in C}TextDensity_i$，$C$是N的孩子集合。<br>在算法的具体实现中，如果网页只包含一个content block，我们只需要在<code>&lt;body&gt;</code>标签下寻找最大的DensitySum，然后将其标记为content即可。对于有多个content block的情况，我们需要对于text density大于阈值的所有节点，都用上述方法进行抽取。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Algorithm <span class="number">2</span> <span class="function">Pseudocode of <span class="title">ExtractContent</span><span class="params">(N)</span></span></span><br><span class="line">1: INPUT: N </span><br><span class="line"><span class="number">2</span>: <span class="keyword">if</span> N.TextDensity &gt;= threshold then </span><br><span class="line"><span class="number">3</span>:       T ←FindMaxDensitySumTag(N)</span><br><span class="line"><span class="number">4</span>:       MarkContent(T) </span><br><span class="line"><span class="number">5</span>:       <span class="keyword">for</span> all child node C in N <span class="keyword">do</span></span><br><span class="line"><span class="number">6</span>:          ExtractContent(C)</span><br><span class="line"><span class="number">7</span>:       end <span class="keyword">for</span></span><br><span class="line"><span class="number">8</span>: end <span class="keyword">if</span></span><br></pre></td></tr></table></figure><p>上面说过，论文作者将<code>&lt;body&gt;</code>标签的text density当做是阈值，但是在实践中，有一些content block的text density会低于该阈值，这会导致正文内容的丢失。为了解决这一问题，先把阈值设为0，然后找到最大的DensitySum的标签。紧接着，从<code>&lt;body&gt;</code>标签到该标签路径上，将最小的text density值设为阈值。</p><h1 id="CEPR：Content-Extraction-via-Path-Ratios"><a href="#CEPR：Content-Extraction-via-Path-Ratios" class="headerlink" title="CEPR：Content Extraction via Path Ratios"></a>CEPR：Content Extraction via Path Ratios</h1><p>该算法能够通过$TPR/ETPR$直方图快速、准确的从网页中提取新闻内容。</p><h2 id="Document-Object-Model"><a href="#Document-Object-Model" class="headerlink" title="Document Object Model"></a>Document Object Model</h2><p><img src="https://github.com/conghuaicai/cs-skill-tree/raw/master/spider/web%20content%20extraction/images/1.png" alt="文档树"></p><h2 id="Extended-Labeled-Ordered-Tree"><a href="#Extended-Labeled-Ordered-Tree" class="headerlink" title="Extended Labeled Ordered Tree"></a>Extended Labeled Ordered Tree</h2><p>正文和噪声的主要区别有：</p><ol><li>正文通常只包含在一个部分中，但是噪声信息在很多部分中都会出现；</li><li>正文通常都有相同的tag paths；</li></ol><p><strong>定义：</strong></p><ul><li>$L={l_0,l_1,l_2,…}$,$l_i$表示标记，即tag；</li><li>$T=(V,E,v_0,\prec ,L,l(\cdot),c(\cdot))$<ul><li>$l:V\rightarrow L$是label function，即求出节点所属的标签$l(v)$；</li><li>$c：V\rightarrow String$是content function，即求出节点的本文内容$c(v)$；</li></ul></li></ul><h2 id="Tag-Path"><a href="#Tag-Path" class="headerlink" title="Tag Path"></a>Tag Path</h2><p>$l(v_0),l(v_1)…l(v_k)$称为节点v的tag path，表示为$path(v)$。</p><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><ol><li>正文节点有相似的tag paths；</li><li>噪声节点有相似的tag paths；</li><li>正文节点包含更多的文本数据；</li><li>噪声节点包含更少的文本数据；</li><li>所有的节点都是叶子节点；</li></ol><h2 id="Text-to-Tag-Path-Ratio"><a href="#Text-to-Tag-Path-Ratio" class="headerlink" title="Text to Tag Path Ratio"></a>Text to Tag Path Ratio</h2><ul><li><strong>pathNum</strong>：tag path在tree T中出现的次数，这个概念比较容易造成误解，现在的理解是，叶节点是不算在tag path里面的。tag path是一个tag序列标识，tag下可能包含多个叶节点，这些叶结点对应的tag path就是一样的，所以这个序列标识是有可能重复的。</li><li><strong>txtNum</strong>：节点中所有字符的个数；</li><li>$accNodes(p)={v_p^1,v_p^2,…,v_p^m}$是tag path p上可访问的节点的集合；</li><li><strong>Text to Tag Path Radio：</strong>$TPR(p)=\frac{\sum_{v\in accNodes(p)}length(c(v))}{|accNodes(p)|}$<ul><li>对于包含长文本的路径，该值很高；</li><li>对于其他路径，该值很低；</li></ul></li></ul><p>举例子说明：</p><p>上述文档图的TPR计算方式如下:</p><ol><li>#1 text node : <code>tag path = &lt;div.div.div.h1&gt;</code>，txtNum = 40, pathNum=1, TPR=40；</li><li>#2 text node: <code>tag path = &lt;div.div.div.p&gt;</code>，txtNum=645，pathNum=2(因为这个序列出现两次)，TPR=322.5；</li><li>#3 text node: <code>tag path=&lt;div.div.div.p.a&gt;</code>，txtNum=7，pathNum=1，TPR=7；</li><li>#4 text node: <code>tag path=&lt;div.div.div.p&gt;</code>，txtNum=645，pathNum=2，TPR=322.5；</li></ol><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="https://github.com/conghuaicai/cs-skill-tree/raw/master/spider/web%20content%20extraction/images/2.png" alt="算法流程"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] Sun, F., Song, D., &amp; Liao, L. (2011). DOM based content extraction via text density. Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information - SIGIR ’11, l, 245. </p><p>[2] Wu, G., Li, L., Hu, X., &amp; Wu, X. (2013). Web news extraction via path ratios. Proceedings of the 22nd ACM International Conference on Conference on Information &amp; Knowledge Management - CIKM ’13, 2059–2068.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在爆炸式增长的互联网时代，互联网上有大量的资源，如何爬取这些资源成为一个热门的研究点。其中如何高效的对网页正文进行抽取、清洗和存储成为一个重要的研究领域。但是，在网页上，除了正文部分，通常还会包含大量的导航栏、广告、版权等信息。相较于正文，这些信息对于我们来说用处不是很大，
      
    
    </summary>
    
      <category term="Spider" scheme="http://conghuai.me/categories/Spider/"/>
    
      <category term="Content Extraction" scheme="http://conghuai.me/categories/Spider/Content-Extraction/"/>
    
    
      <category term="algorithm" scheme="http://conghuai.me/tags/algorithm/"/>
    
  </entry>
  
</feed>
