<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>蔡聪怀</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://conghuai.me/"/>
  <updated>2018-05-01T04:04:17.162Z</updated>
  <id>http://conghuai.me/</id>
  
  <author>
    <name>Conghuai Cai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Dropout</title>
    <link href="http://conghuai.me/2018/05/01/Dropout/"/>
    <id>http://conghuai.me/2018/05/01/Dropout/</id>
    <published>2018-05-01T03:26:03.000Z</published>
    <updated>2018-05-01T04:04:17.162Z</updated>
    
    <content type="html"><![CDATA[<p>过拟合是机器学习和深度学习的一个比较重要的问题，模型一旦过拟合，就会出现在训练样本上表现很好，但是在测试样本上表现很差的情况。所以，过拟合导致模型变得不可用。</p><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqvphars2ej30h209475u.jpg" alt="dropout"></p><blockquote><p>In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing. Therefore, units may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit, Dropout prevents co-adaptation by making the presence of other hidden units unreliable. Therefore, a hidden unit cannot rely on other specific units to correct its mistakes.</p></blockquote><p>Dropout方法在每次迭代中都会随机关闭一部分的神经元，这就相当于修改了模型，所以在每次迭代中，实际上训练的就是不同的模型，每个模型都只用到一部分的神经元。这样做的好处在于，使得某个神经元不会过于依赖另一个神经元，避免了互相适应的效果，所以有效的防止了过拟合。</p><p>被关闭的神经元对于前向传播和反向传播都不起作用。</p><h2 id="Directly-Dropout"><a href="#Directly-Dropout" class="headerlink" title="Directly Dropout"></a>Directly Dropout</h2><p>因为在训练阶段，某个神经元被保留的概率为$p$，在测试阶段，为了模拟组合不同神经网络的结果，我们需要对激活函数值乘以概率$p$。</p><ul><li>Train phase : $O_i = X_i a(\sum_{k=1}^{d_i}w_kx_k+b)$</li><li>Test phase : $O_i = pa(\sum_{k=1}^{d_i}w_kx_k+b)$</li></ul><h2 id="Inverted-Dropout"><a href="#Inverted-Dropout" class="headerlink" title="Inverted Dropout"></a>Inverted Dropout</h2><p>与上面方式不同是，该方法在训练过程就考虑的缩放因子，所以在测试阶段就不需要做任何处理。</p><ul><li>Train phase : $O_i = \frac{1}{p}X_i a(\sum_{k=1}^{d_i}w_kx_k+b)$</li><li>Test phase：$O_i = a(\sum_{k=1}^{d_i}w_kx_k+b)$</li></ul><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="前向运算"><a href="#前向运算" class="headerlink" title="前向运算"></a>前向运算</h2><ol><li>对于每层神经元$a^{[l]}$，我们创建一个维度一样的变量$d^{[l]}$，其中$d^{[l]}_i\in(0,1)$。因为每一层的输入有多个，我们采用向量化方式来处理，将$d^{[l]}$进行扩充得，$D^{[l]}=[d^{<a href="1">l</a>},d^{<a href="2">l</a>},…,d^{<a href="m">l</a>}]$，该维度与$A^{[l]}$的维度一致；</li><li>将$D^{[l]}$中的每个元素根据与keep_prob相比，置位0或1；</li><li>$A^{[l]} = A^{[l]}*D^{[l]}$；</li><li>$A^{[l]} /= keep\_prob$</li></ol><h2 id="后向运算"><a href="#后向运算" class="headerlink" title="后向运算"></a>后向运算</h2><ol><li>在前向运算中，我们利用$D^{[l]}$关闭了某些神经元的前向传播，在后向运算的时候，我们同样关闭该神经元的后向传播：$dA^{[l]} = dA^{[l]}*D^{[l]}$;</li><li>$dA^{[l]} /= keep\_prob$</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>Dropout是一种防止过拟合的技术，通常用于神经网络中；</li><li>在训练阶段使用Dropout，而不要在测试阶段使用它；</li><li>在前向运算和后向运算都使用Dropout；</li><li>Dropout存在两种版本：direct 和 inverted。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] . <a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">Analysis of Dropout</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;过拟合是机器学习和深度学习的一个比较重要的问题，模型一旦过拟合，就会出现在训练样本上表现很好，但是在测试样本上表现很差的情况。所以，过拟合导致模型变得不可用。&lt;/p&gt;
&lt;h1 id=&quot;Dropout&quot;&gt;&lt;a href=&quot;#Dropout&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Overfiting" scheme="http://conghuai.me/categories/Machine-Learning/Overfiting/"/>
    
      <category term="Deep Learning" scheme="http://conghuai.me/categories/Deep-Learning/"/>
    
      <category term="Overfiting" scheme="http://conghuai.me/categories/Deep-Learning/Overfiting/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="overfiting" scheme="http://conghuai.me/tags/overfiting/"/>
    
  </entry>
  
  <entry>
    <title>Parameters Initialization</title>
    <link href="http://conghuai.me/2018/04/30/Parameters-Initialization/"/>
    <id>http://conghuai.me/2018/04/30/Parameters-Initialization/</id>
    <published>2018-04-30T07:16:45.000Z</published>
    <updated>2018-05-01T04:04:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络通常有大量的参数需要训练，在模型一开始训练时，需要初始化模型的参数，我们该采用何种策略来初始化参数呢？不同的初始化方法将会带来如下的效果：</p><ul><li>加速梯度下降的收敛速度；</li><li>可能会使得模型在训练集上的错误率更低。</li></ul><h1 id="Zero-initialization"><a href="#Zero-initialization" class="headerlink" title="Zero initialization"></a>Zero initialization</h1><p>如果我们将权重设置为零，那么所有层的所有神经元都执行相同的计算，给出相同的输出，则整个深层网络的复杂度将与单个神经元的复杂度相同，并且预测不会比随机更好。这样的初始化方式带来了隐藏层神经元<strong>对称问题</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l], layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">    parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fquqmny4l2j30dj0hktbc.jpg" alt="Screen Shot 2018-04-30 at 15.35.56"></p><h1 id="Random-initialization"><a href="#Random-initialization" class="headerlink" title="Random initialization"></a>Random initialization</h1><p>为了解决对称问题，我们可以随机初始化$W$参数，这样可以使得每个神经元得到不同的输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class="number">-1</span>]) * <span class="number">10</span></span><br><span class="line">    parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fquqmmnj2qj30ey0hk413.jpg" alt="Screen Shot 2018-04-30 at 15.38.06"></p><p>随机初始化参数可能会带来两个问题：（1）梯度消失；（2）梯度爆炸。</p><h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><blockquote><p>The weight update is minor and results in slower convergence. This makes the optimization of the loss function slow. In the worst case, this may completely stop the neural network from training further.</p></blockquote><p>在神经网络中，对于任何激活函数，当我们对损失做反向传播的时候，$dW$将会越来越小。所以，越靠近输入层的神经元其改变量将越小。</p><h2 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h2><blockquote><p>This may result in oscillating around the minima or even overshooting the optimum again and again and the model will never learn!</p></blockquote><p>该问题和上面的相反，当我们参数值很大时，经过多层神经元累积后，得到的值将会非常大。</p><h2 id="梯度消失于梯度爆炸分析"><a href="#梯度消失于梯度爆炸分析" class="headerlink" title="梯度消失于梯度爆炸分析"></a>梯度消失于梯度爆炸分析</h2><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqvomchpu7j30ga08hjtd.jpg" alt="v2-82873a89ff3c14c1d3b42d1862917f35_hd"></p><p>如图含有3个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的hidden layer3等的权值更新相对正常，单前面的hidden layer1的权值更新会变得很慢，导致前面的层权值几乎不变， 仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做一个同一映射，这时此深层网络的学习就等价于只有后几层的浅层网络的学习了。</p><p>而这种问题为何会产生呢？以下图的反向传播为例（假设每一层只有一个神经元且对于每一层，$y_i=\sigma(z_i)=\sigma(w_ix_i+b_i)$，其中$\sigma$为sigmoid函数）</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqvomc3qyqj30cm01qglh.jpg" alt="v2-b9e0d6871fbcae05d602bab65620a3ca_hd"></p><p>可以推导出</p><p>$$\begin{align} \frac{\partial C}{\partial b_1} &amp;= \frac{\partial C}{\partial y_4}\cdot \frac{\partial y_4}{\partial z_4}\cdot \frac{\partial z_4}{\partial x_4}\cdot \frac{\partial x_4}{\partial z_3}\cdot \frac{\partial z_3}{\partial x_3}\cdot \frac{\partial x_3}{\partial z_2}\cdot \frac{\partial z_2}{\partial x_2}\cdot \frac{\partial x_2}{\partial z_1}\cdot \frac{\partial z_1}{\partial b_1} \\  &amp;= \frac{\partial C}{\partial y_4}\cdot \sigma’(z_4)\cdot w_4\cdot \sigma’(z_3)\cdot w_3\cdot \sigma’(z_2)\cdot w_2 \sigma’(z_1)  \\   \end{align}$$</p><p>而sigmoid的导数$\sigma’(x)$如下图所示</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqvomcwj7zj30ai06kt8p.jpg" alt="v2-da5606a2eebd4d9b6ac4095b398dacf5_hd"></p><p>可见，$\sigma’(x)$的最大值为$\frac{1}{4}$，而我们初始化的网络权值$|w|$通常都小于1，因此$|\sigma’(z)w|\leq\frac{1}{4}$，因此对于上面的链式求导，层数越多，求导结果$\frac{\partial C}{\partial b_1}$越小，因而导致梯度消失的情况出现。</p><p>这样，梯度爆炸问题的出现原因就显而易见了，即$|\sigma’(z)w|&gt;1$，也就是$w$比较大的情况下。</p><h1 id="Best-Practices"><a href="#Best-Practices" class="headerlink" title="Best Practices"></a>Best Practices</h1><p>在神经网络模型中，我们通常会使用<code>ReLU</code>或者<code>leaky ReLU</code>作为激活函数，该激活函数对于梯度消失和梯度爆炸问题具有较好的鲁棒性。我们可以根据不同的激活函数，采用启发式方式来初始化参数。</p><h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>$$\sqrt {\frac{2}{size^{[l-1]}}}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(size_l, size_l<span class="number">-1</span>) * np.sqrt(<span class="number">2</span>/size_l<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h2><p>$$\sqrt {\frac{1}{size^{[l-1]}}}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(size_l, size_l<span class="number">-1</span>) * np.sqrt(<span class="number">1</span>/size_l<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks" target="_blank" rel="noopener">coursera deeplearning.ai</a></p><p>[2]. <a href="https://towardsdatascience.com/deep-learning-best-practices-1-weight-initialization-14e5c0295b94" target="_blank" rel="noopener">Deep Learning Best Practices (1) — Weight Initialization</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;神经网络通常有大量的参数需要训练，在模型一开始训练时，需要初始化模型的参数，我们该采用何种策略来初始化参数呢？不同的初始化方法将会带来如下的效果：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加速梯度下降的收敛速度；&lt;/li&gt;
&lt;li&gt;可能会使得模型在训练集上的错误率更低。&lt;/li&gt;
&lt;/u
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://conghuai.me/categories/Deep-Learning/"/>
    
      <category term="Optimization" scheme="http://conghuai.me/categories/Deep-Learning/Optimization/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
      <category term="regularization" scheme="http://conghuai.me/tags/regularization/"/>
    
      <category term="hyperparameter tuning" scheme="http://conghuai.me/tags/hyperparameter-tuning/"/>
    
  </entry>
  
  <entry>
    <title>Building blocks of deep neural networks</title>
    <link href="http://conghuai.me/2018/04/27/Building-blocks-of-deep-neural-networks/"/>
    <id>http://conghuai.me/2018/04/27/Building-blocks-of-deep-neural-networks/</id>
    <published>2018-04-27T12:16:45.000Z</published>
    <updated>2018-04-29T07:47:43.118Z</updated>
    
    <content type="html"><![CDATA[<p>深度学习中通常都有很多个隐藏层， 每个隐藏层会有很多个神经元，这些神经元的输出连接到下一层，然后经过激活函数产生新的输出。数据输入到神经元，经过多个隐藏层，最后产生最终的输出结果。我们对预测结果和真实结果计算损失函数，然后利用该损失对各层的参数求偏导，进而通过偏导值更新参数。这个过程会迭代很多次。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqtkpzf23hj30g30793zh.jpg" alt="Screen Shot 2018-04-29 at 13.47.34"></p><h1 id="构建运算块"><a href="#构建运算块" class="headerlink" title="构建运算块"></a>构建运算块</h1><p>神经网络运算从大的方向上主要分为两个，一个是前向运算，另一个是后向运算。两个运算的输入、输出已经所需参数都是不一样的。</p><h2 id="前向运算"><a href="#前向运算" class="headerlink" title="前向运算"></a>前向运算</h2><p>前向运算指训练数据输入到神经网络中，经过多个隐藏层，最后得到损失函数值。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqtkpzvds9j30qp0eeq3z.jpg" alt="Screen Shot 2018-04-29 at 14.26.21"></p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqtkq0sj3uj30zf0drdgx.jpg" alt="Screen Shot 2018-04-29 at 14.37.57"></p><p>对于<code>Layer L</code></p><ul><li>输入<ul><li>$a^{[l-1]}$</li></ul></li><li>输出<ul><li>$a^{[l]}$</li></ul></li><li>参数<ul><li>$W^{[l]}$,$b^{[l]}$</li></ul></li><li>运算</li></ul><p>$$Z^{[l]}=W^{[l]}A^{[l-1]}+b$$</p><p>$$A^{[l]}=g^{[l]}(Z^{[l]})$$</p><ul><li>缓存<ul><li>$Z^{[l]}$</li></ul></li></ul><h2 id="后向运算"><a href="#后向运算" class="headerlink" title="后向运算"></a>后向运算</h2><p>后向运算指损失函数值对各个隐藏层参数求偏导的过程。</p><ul><li>输入<ul><li>$da^{[l]}$</li></ul></li><li>输出<ul><li>$da^{[l-1]}$,$dW^{[l]}$,$db^{[l]}$</li></ul></li><li>运算</li></ul><p>$$dZ^{[l]}=dA^{[l]}\cdot g^{[l]}‘(Z^{[l]})$$</p><p>$$dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}$$</p><p>$$db^{[l]}=\frac{1}{m}\cdot np.sum(dZ^{[l]}, axis=1, keepdims=True)$$</p><p>$$dA^{[l-1]}=W^{[l]T}dZ^{[l]}$$    </p><ul><li>计算分析如下</li></ul><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqtkq1t8qkj30sp0cu0ug.jpg" alt="Screen Shot 2018-04-29 at 15.22.48"></p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqtkq17h0fj30w50d7myv.jpg" alt="Screen Shot 2018-04-29 at 15.32.43"></p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqtkq0b17bj30t60d7wfx.jpg" alt="Screen Shot 2018-04-29 at 15.44.07"></p><h2 id="运算块"><a href="#运算块" class="headerlink" title="运算块"></a>运算块</h2><p>我们可以通过运算块接连图的方式来直观的理解前向运算和后向运算的过程。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqtkq28olzj30yz0ijwi9.jpg" alt="Screen Shot 2018-04-29 at 15.46.20"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. Coursera deeplearning.ai</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;深度学习中通常都有很多个隐藏层， 每个隐藏层会有很多个神经元，这些神经元的输出连接到下一层，然后经过激活函数产生新的输出。数据输入到神经元，经过多个隐藏层，最后产生最终的输出结果。我们对预测结果和真实结果计算损失函数，然后利用该损失对各层的参数求偏导，进而通过偏导值更新参数
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://conghuai.me/categories/Machine-Learning/Deep-Learning/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>Activation Functions</title>
    <link href="http://conghuai.me/2018/04/25/Activation-Functions/"/>
    <id>http://conghuai.me/2018/04/25/Activation-Functions/</id>
    <published>2018-04-25T07:38:44.000Z</published>
    <updated>2018-05-01T03:33:05.873Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqp0idk6d0j315o0p0tc4.jpg" alt="nural-network_3"></p><p>激活函数是神经网络中一个比较重要的概念，激活函数首先计算输入的线性值，然后决定是否“激活”该值。</p><p>$$Y=Activation(\sum(weight\ *\ input ) + bias)$$</p><p>上面的线性组合的值的范围为$[-\infty, +\infty]$，激活函数根据上一层输出的线性组合值来决定是否将该值传到下一层，并决定以何种方式传到下一层。</p><p>正如绝大多数神经网络借助某种形式的梯度下降进行优化，激活函数需要是可微分的。此外，复杂的激活函数也许产生一些梯度消失或梯度爆炸的问题。常见的激活函数有很多，我们该如何选择合适的激活函数呢？我们先来分析一下每种激活函数的特点和性质。</p><h1 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h1><p>激活函数显然增加了神经网络的复杂度，那么 ，我们是否可以不同激活函数呢？答案是不可行的，如果不用激活函数，那么神经网络模型最终将会只是一个线性模型。而线性模型由于其模型的复杂度不够，在解决很多复杂问题上都显得力不从心。</p><h1 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h1><h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><h3 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h3><p>$$g(z) = \frac{1}{1+e^{-z}}$$</p><h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><p>$$g’(z) = g(z)(1-g(z))$$</p><h3 id="函数性质"><a href="#函数性质" class="headerlink" title="函数性质"></a>函数性质</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1fqp0icy6xwj311z0moacm.jpg" alt="5561420171010093434"></p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x, derivative=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> (derivative == <span class="keyword">True</span>):</span><br><span class="line">        <span class="keyword">return</span> x * (<span class="number">1</span> - x)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br></pre></td></tr></table></figure><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>梯度消失问题</li></ol><p>在导函数的两端，函数值非常的小，这意味着在做梯度下降时，参数的改变量将非常小。这就可能带来梯度消失的问题。</p><ol><li>饱和问题。</li></ol><p>饱和问题是指当初始参数过大时，会导致sigmoid导数值很小，造成学习速率非常慢。</p><h2 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h2><h3 id="表达式-1"><a href="#表达式-1" class="headerlink" title="表达式"></a>表达式</h3><p>$$g(z) = tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}$$</p><h3 id="导数-1"><a href="#导数-1" class="headerlink" title="导数"></a>导数</h3><p>$$g’(z)= 1-g(z)^2$$</p><h3 id="函数性质-1"><a href="#函数性质-1" class="headerlink" title="函数性质"></a>函数性质</h3><p>在分类任务中，双曲正切函数（Tanh）逐渐取代Sigmoid函数作为标准的激活函数。<img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqp0ig26d5j311m0jy0vf.jpg" alt="8940520171010093544"></p><h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x, derivative=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> (derivative == <span class="keyword">True</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="number">1</span> - (x ** <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br></pre></td></tr></table></figure><h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>修正线性单元（Rectified linear unit，ReLU）是神经网络中最常用的激活函数。它保留了 step 函数的生物学启发（只有输入超出阈值时神经元才激活），不过当输入为正的时候，导数不为零，从而允许基于梯度的学习（尽管在 x=0 的时候，导数是未定义的）。使用这个函数能使计算变得很快，因为无论是函数还是其导数都不包含复杂的数学运算。然而，当输入为负值的时候，ReLU 的学习速度可能会变得很慢，甚至使神经元直接无效，因为此时输入小于零而梯度为零，从而其权重无法得到更新，在剩下的训练过程中会一直保持静默。</p><h3 id="表达式-2"><a href="#表达式-2" class="headerlink" title="表达式"></a>表达式</h3><p>$$\begin{split} f(x)=\begin{cases} x, &amp; \text{$x\geq 0$} \\ 0, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><h3 id="导数-2"><a href="#导数-2" class="headerlink" title="导数"></a>导数</h3><p>$$\begin{split} f’(x)=\begin{cases} 1, &amp; \text{$x\geq 0$} \\ 0, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><h3 id="函数性质-2"><a href="#函数性质-2" class="headerlink" title="函数性质"></a>函数性质</h3><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqp0if00ohj31240mujtv.jpg" alt="4217520171010093357"></p><h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ol><li>静默神经元。</li></ol><p>由于该激活函数在$x&lt;0$的时候，其导数值为0，就会使得一些神经元一直得不到更新。</p><ol><li>只能用于隐藏层。</li></ol><h2 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h2><p>经典（以及广泛使用的）ReLU 激活函数的变体，带泄露修正线性单元（Leaky ReLU）的输出对负值输入有很小的坡度。由于导数总是不为零，这能减少静默神经元的出现，允许基于梯度的学习（虽然会很慢）。</p><h3 id="表达式-3"><a href="#表达式-3" class="headerlink" title="表达式"></a>表达式</h3><p>$$\begin{split} f(x)=\begin{cases} x, &amp; \text{$x\geq0$} \\ 0.01x, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><h3 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h3><p>$$\begin{split} f’(x)=\begin{cases} 1, &amp; \text{$x\geq0$} \\ 0.01, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><h3 id="函数性质-3"><a href="#函数性质-3" class="headerlink" title="函数性质"></a>函数性质</h3><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqp0ieauwoj311z0mq0ve.jpg" alt="1601520171010093621"></p><h1 id="选择激活函数准则"><a href="#选择激活函数准则" class="headerlink" title="选择激活函数准则"></a>选择激活函数准则</h1><ul><li>Sigmoid激活函数常用于分类任务。</li><li>Sigmoid和Tanh函数会产生梯度消失问题。</li><li>ReLU是目前使用最广泛的激活函数。</li><li>如果在神经网络中，出现了静默神经元，可以考虑使用 Leaky ReLU。</li><li>ReLU激活函数只能用于隐藏层。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" target="_blank" rel="noopener">Understanding Activation Functions in Neural Networks</a></p><p>[2]. <a href="https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/" target="_blank" rel="noopener">Fundamentals of Deep Learning – Activation Functions and When to Use Them?</a></p><p>[3]. <a href="https://www.wikiwand.com/en/Activation_function" target="_blank" rel="noopener">维基百科Activation Functions</a></p><p>[4]. <a href="https://analyticsindiamag.com/most-common-activation-functions-in-neural-networks-and-rationale-behind-it/" target="_blank" rel="noopener">Types Of Activation Functions In Neural Networks And Rationale Behind It</a></p><p>[5]. <a href="https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/" target="_blank" rel="noopener">Visualising Activation Functions in Neural Networks</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tKfTcgy1fqp0idk6d0j315o0p0tc4.jpg&quot; alt=&quot;nural-network_3&quot;&gt;&lt;/p&gt;
&lt;p&gt;激活函数是神经网络中一个比较重要的概念，激活函数首先计算输入
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://conghuai.me/categories/Machine-Learning/Deep-Learning/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>Neural Networks and Deep Learning Foundation</title>
    <link href="http://conghuai.me/2018/04/24/Neural-Networks-and-Deep-Learning-Foundation/"/>
    <id>http://conghuai.me/2018/04/24/Neural-Networks-and-Deep-Learning-Foundation/</id>
    <published>2018-04-24T06:33:38.000Z</published>
    <updated>2018-04-30T07:18:08.868Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络由于其多层多神经元的特性，参数往往很多，这也导致了学习深度学习的时候会有太“复杂”的感觉。该文章整理出一些神经网络中重要的基础概念和符号表示系统。</p><h1 id="符号表示体系"><a href="#符号表示体系" class="headerlink" title="符号表示体系"></a>符号表示体系</h1><h2 id="单层神经元"><a href="#单层神经元" class="headerlink" title="单层神经元"></a>单层神经元</h2><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqlph8h48ej30f90e7aas.jpg" alt=""></p><p>通常用$a^l$来表示第$l$层的向量，用$a^l_i$来表示该层向量中第$i$个分量。<strong>记住，上标用来表示整体，下标用来表示分量</strong>。</p><h2 id="参数矩阵W"><a href="#参数矩阵W" class="headerlink" title="参数矩阵W"></a>参数矩阵W</h2><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqlph7gyr8j30o70f10ua.jpg" alt=""></p><ul><li>$ W$中的每一行代表的是每一个输出神经元输入的参数；W中的每一列代表的是每一个输入元输出的参数。</li><li>$W_{ij}$是指从下一层的第$i$个神经元指向上一层的第$j$个神经元。</li></ul><h1 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h1><p>假设我们要计算的是$J(a,b,c)=3\cdot (a+bc)$，在深度学习中，我们经常会把每个计算式转化成一个计算图进行计算。针对这个计算图，一般来说，我们通常需要的前向计算损失函数值和后向传播求偏导两个步骤来学习我们的模型。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqorpcvi8qj30kg0bwq3c.jpg" alt="1.jpg"></p><h1 id="计算表示"><a href="#计算表示" class="headerlink" title="计算表示"></a>计算表示</h1><p>深度学习中涉及到很多计算，包括损失函数的计算，偏导的计算等等。并且，由于训练一个模型的时候，通过需要大量的样本，大量的参数，所有编写代码来训练深度学习模型时，通常需要涉及很多重循环。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>我们以简单的逻辑回顾模型为例来说明计算过程。</p><p>逻辑回归：</p><p>$$z = w^Tx+b$$</p><p>$$\hat{y}=a=\sigma(x)$$</p><p>$$L(a,y) = -[ylog(a)+(1-y)log(1-a)]$$</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqorpbe3f8j30x60crab9.jpg" alt="Screen Shot 2018-04-24 at 15.50.28"></p><h3 id="单个样本"><a href="#单个样本" class="headerlink" title="单个样本"></a>单个样本</h3><p>对于单个样本，我们只需要计算其loss function值，然后分别计算导数即可：</p><p>$$\frac{\partial L}{\partial a}=-\frac{y}{a}-\frac{1-y}{1-a}$$</p><p>$$\begin{align} \frac{\partial L}{\partial z} &amp;=\frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z} \\\\  &amp;= a\cdot(1-a)\cdot [-\frac{y}{a}-\frac{1-y}{1-a}] \\\\ &amp;= a- y\end{align}$$</p><p>$$\begin{align} \frac{\partial L}{\partial w_1} &amp;=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w} \\\\  &amp;= x_1\cdot (a-y) \end{align}$$</p><p>$$\begin{align} \frac{\partial L}{\partial w_2} &amp;=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w} \\\\  &amp;= x_2\cdot (a-y) \end{align}$$</p><p>$$\begin{align} \frac{\partial L}{\partial w_3} &amp;=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w} \\\\  &amp;= x_3\cdot (a-y) \end{align}$$</p><p>可以看出来，在对单个样本计算loss function的时候并不困难，我们只需要计算三个导数即可。</p><p>实现分析：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations): // 迭代次数</span><br><span class="line">    z = w*x + b</span><br><span class="line">    a = sigmoid(z)</span><br><span class="line">    J = -[y*loga + (<span class="number">1</span>-y)*log(<span class="number">1</span>-a)]</span><br><span class="line">    dz = a - y</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(para_num):</span><br><span class="line">        dw_i = x_i*dz</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(para_num): // 参数个数</span><br><span class="line">        w_i = w_i - alpha * dw_i</span><br></pre></td></tr></table></figure><h3 id="多个样本"><a href="#多个样本" class="headerlink" title="多个样本"></a>多个样本</h3><p>现在从单个样本扩展到多个样本的情况，我们用上标来表示训练集中不同的样本，即$x^{(i)}$。</p><p>这时候cost funciton值为每个样本的loss function值的求和平均：</p><p>$$J(w,b) = \frac{1}{m}\sum_{i=1}^mL(a^{(i)},y^{(i)})$$</p><p>$$a^{(i)}=\hat{y}^{(i)}=\sigma(z^{(i)})=\sigma(w^Tx^{(i)}+b)$$</p><p>求偏导</p><p>$$\frac{\partial J}{\partial a^{i}}=\frac{1}{m}\cdot [-\frac{y^{(i)}}{a^{(i)}}-\frac{1-y^{(i)}}{1-a^{(i)}}]$$</p><p>$$\begin{align} \frac{\partial J}{\partial z^{(i)}} &amp;=\frac{\partial J}{\partial a^{(i)}}\cdot \frac{\partial a^{(i)}}{\partial z^{(i)}} \\  &amp;= \frac{1}{m}\cdot a^{(i)}\cdot(1-a^{(i)})\cdot [-\frac{y^{(i)}}{a^{(i)}}-\frac{1-y^{(i)}}{1-a^{(i)}}] \\&amp;= \frac{1}{m}\cdot (a^{(i)}- y^{(i)})\end{align}$$</p><p>$$\begin{align} \frac{\partial J}{\partial w_1} &amp;=\sum_{i=1}^N\frac{\partial J}{\partial z^{(i)}}\cdot \frac{\partial z^{(i)}}{\partial w_1} \\  &amp;= \frac{1}{m} \cdot x_1\cdot (a-y) \end{align}$$</p><p>$$\begin{align} \frac{\partial J}{\partial w_2} &amp;=\sum_{i=1}^N\frac{\partial J}{\partial z^{(i)}}\cdot \frac{\partial z^{(i)}}{\partial w_2} \\  &amp;= \frac{1}{m} \cdot x_2\cdot (a-y) \end{align}$$</p><p>$$\begin{align} \frac{\partial J}{\partial w_3} &amp;=\sum_{i=1}^N\frac{\partial J}{\partial z^{(i)}}\cdot \frac{\partial z^{(i)}}{\partial w_3} \\  &amp;= \frac{1}{m} \cdot x_3\cdot (a-y) \end{align}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations): // 迭代次数</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_sample): // 样本个数</span><br><span class="line">        z = w*x + b</span><br><span class="line">        a = sigmoid(z)</span><br><span class="line">        J = -[y*loga + (<span class="number">1</span>-y)*log(<span class="number">1</span>-a)]</span><br><span class="line">        dz = a - y</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(para_num): // 参数个数</span><br><span class="line">            dw_i += x_i*dz</span><br><span class="line">        b += dz_i</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(para_num): // 参数个数</span><br><span class="line">        w_i = w_i - alpha * dw_i</span><br><span class="line">    b = b - alpha * db</span><br></pre></td></tr></table></figure><p>可以看到，算法学习需要3重循环。</p><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>通过使用numpy数组及向量计算，我们可以减少循环次数，提高运算效率。通过定义<code>dw=[dw_1, dw_2, ..., dw_n]</code>和<code>w=[w_1, w_2, ..., w_n]</code>来进行向量化操作。</p><h4 id="一、参数向量化"><a href="#一、参数向量化" class="headerlink" title="一、参数向量化"></a>一、参数向量化</h4><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1fqorpbyfobj30et0g6dga.jpg" alt="Screen Shot 2018-04-24 at 23.11.18"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations): // 迭代次数</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_sample): // 样本个数</span><br><span class="line">        z = w*x + b</span><br><span class="line">        a = sigmoid(z)</span><br><span class="line">        J = -[y*loga + (<span class="number">1</span>-y)*log(<span class="number">1</span>-a)]</span><br><span class="line">        dz = a - y</span><br><span class="line">        dw += x_i*dz_i</span><br><span class="line">        b += dz_i</span><br><span class="line">w -= alpha*dw</span><br><span class="line">    b = b - alpha * db</span><br></pre></td></tr></table></figure><h4 id="二、样本向量化"><a href="#二、样本向量化" class="headerlink" title="二、样本向量化"></a>二、样本向量化</h4><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqtiqbhsp5j30qp0eeq3z.jpg" alt="Screen Shot 2018-04-29 at 14.26.21"></p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1fqtiqb8zayj30zf0drdgx.jpg" alt="Screen Shot 2018-04-29 at 14.37.57"></p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1fqtjjorp99j30f901mjrg.jpg" alt="Screen Shot 2018-04-29 at 15.06.40"></p><p>在上面的实现中，对于样本的处理，并没有转化为向量的操作，现在，我们研究一下如何转化为向量的操作。</p><ul><li><p>样本输入：$X=[x^{(1)},x^{(2)},…,x^{(m)}]$，其中$x^{(i)}$为列向量。</p></li><li><p>得到Z：</p><p>$$\begin{align} Z=[z^{(1)},z^{(2)},…,z^{(m)}] &amp;= w^T\cdot X+[b,b,…,b] \\  &amp;= [w^Tx^{(1)}+b,w^Tx^{(2)}+b,…,w^Tx^{(m)}+b]  \\ \end{align}$$</p><p><code>Z=np.dot(W.T, X)+b</code></p></li><li><p>$A=[\sigma(z^{(1)}),\sigma(z^{(2)}),…,\sigma(z^{(m)})]$</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(W.T, x) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A - Y</span><br><span class="line">dw = <span class="number">1</span>/m * X * dZ.T</span><br><span class="line">db = <span class="number">1</span>/m * np.sum(dZ)</span><br><span class="line">w = w - alpha * dw</span><br><span class="line">b = b - alpha * db</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;神经网络由于其多层多神经元的特性，参数往往很多，这也导致了学习深度学习的时候会有太“复杂”的感觉。该文章整理出一些神经网络中重要的基础概念和符号表示系统。&lt;/p&gt;
&lt;h1 id=&quot;符号表示体系&quot;&gt;&lt;a href=&quot;#符号表示体系&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://conghuai.me/categories/Deep-Learning/"/>
    
    
      <category term="dl" scheme="http://conghuai.me/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>Adaboost</title>
    <link href="http://conghuai.me/2018/04/22/Adaboost/"/>
    <id>http://conghuai.me/2018/04/22/Adaboost/</id>
    <published>2018-04-22T03:02:07.000Z</published>
    <updated>2018-04-22T12:21:57.705Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Ensemble" scheme="http://conghuai.me/categories/Machine-Learning/Ensemble/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="ensemble" scheme="http://conghuai.me/tags/ensemble/"/>
    
  </entry>
  
  <entry>
    <title>Support Vector Machine</title>
    <link href="http://conghuai.me/2018/04/19/Support-Vector-Machine/"/>
    <id>http://conghuai.me/2018/04/19/Support-Vector-Machine/</id>
    <published>2018-04-19T02:18:56.000Z</published>
    <updated>2018-04-22T02:41:47.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090106.jpg" alt="svm"></p><p>支持向量机（support vector machines,SVM）是一种二类分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器，支持向量机的学习策略就是间隔最大化，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法是求解凸二次规划的最小化算法。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>感知机模型告诉我们一个线性分类器就是要在$n$维的数据空间中找到一个超平面</p><p>$$w^Tx+b = 0$$</p><p>通过这个超平面可以把两类数据分隔开，比如，在超平面一边的数据点所对应的$y$全是-1；而另一边全是1。具体来说，我们令$f(x) = w^Tx+b$，显然，如果$f(x) = 0$，那么$x$是位于超平面上的点。我们不妨要求对于所有满足$f(x)&lt;0$的点，其对应的$y$等于-1，而$f(x)&gt;0$则对应$y=1$的样本点。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090113.jpg" alt="Hyper-Plane"></p><p>从几何直观上来说，由于超平面是用于分隔两类数据的，越接近超平面的点越难分隔，因为如果超平面稍微转动一下，他们就有可能跑到另一边去。反之，如果是距离超平面很远的点，则很容分辨出其类别。所以，我们希望找到一个超平面，使得样本点到超平面的距离都尽量远，那么我们应该如何定义这个距离？</p><h2 id="函数距离"><a href="#函数距离" class="headerlink" title="函数距离"></a>函数距离</h2><p>一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度，在超平面$w\cdot x+b=0$确定的情况下，$|w\cdot x+b|$能够相对地表示点$x$距离超平面的远近。而$w\cdot x + b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以，可用量$y(w\cdot x+b)$来表示分类的正确性及确信度，这就是函数间隔的概念。</p><p>对于给定的训练数据集T和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_t,y_t)$的函数间隔为</p><p>$$\hat{\gamma}_i=y_i(w\cdot x_i+b) $$</p><p>定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，即</p><p>$$\hat{\gamma}=min_{i=1,..,N} \hat{\gamma}_i$$</p><p>函数间隔可以表示分类预测的正确性及确信度，但是选择分离超平面时，只有函数间隔不够，因为只要成比例地改变$w$和$b$，例如将它们改为$2w$和$2b$，超平面并没有改变，但函数间隔却成为原来的2倍。如果我们对函数间隔加某些约束，如规范化，$||w||=1$，使得间隔是确定的，这时函数间隔就成为几何间隔。</p><h2 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h2><p>对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为</p><p>$$\gamma_i = y_i(\frac{w}{||w||}\cdot x_i+\frac{b}{||w||})$$</p><p>定义超平面$(w,b)$关于训练数据集$T$的几何间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，即</p><p>$$\gamma=min_{i=1,…,N}\ \gamma_i$$</p><p>我们也可以通过几何关系，求解出集合间隔</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090102.jpg" alt="distance"></p><h2 id="两种间隔的关系"><a href="#两种间隔的关系" class="headerlink" title="两种间隔的关系"></a>两种间隔的关系</h2><p>从函数间隔和几何间隔的定义可知，函数间隔和几何间隔有下面的关系：</p><p>$$\gamma_i = \frac{\hat{\gamma_i}}{||w||}$$</p><p>$$\gamma = \frac{\hat{\gamma}}{||w||}$$</p><p>如果$||w||=1$，那么函数间隔和几何间隔相等，如果超平面参数$w$和$b$成比例地改变，函数间隔也按此比例改变，而几何间隔不变。</p><h1 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h1><p>支持向量机学习的基本想法是求解能够正确划分训练数据集并且集合间隔最大的分离超平面。对线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机），但是几何间隔最大的分离超平面是唯一的。这里的间隔最大化又称为硬间隔最大化。间隔最大化的直观解释是：对训练数据集找到集合间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。</p><h2 id="最大间隔分离超平面"><a href="#最大间隔分离超平面" class="headerlink" title="最大间隔分离超平面"></a>最大间隔分离超平面</h2><p>$$max_{w,b}\ \ \ \frac{\hat{\gamma}}{||w||}$$</p><p>$$s.t.\ y_i(w\cdot x_i+b)\geq \hat{\gamma},i=1,2,…,N$$</p><p>函数间隔$\hat{\gamma}$的取值并不影响最优化问题的解。事实上，假设将$w$和$b$按比例改变为$\lambda w$和$\lambda b$，这时函数间隔成为$\lambda \hat{\gamma}$。函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，它产生一个等价的最优化问题。这样，就可以取$\hat{\gamma}=1$，带入上式</p><p>$$max_{w,b}\ \ \ \frac{1}{||w||}$$</p><p>$$s.t.\ y_i(w\cdot x_i+b)\geq 1,i=1,2,…,N$$</p><p>注意到最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$是等价的，于是就得到了下面的线性可分支持向量学习的最优化问题</p><p>$$min_{w,b}\ \ \ \frac{1}{2}||w||^2$$</p><p>$$s.t.\ y_i(w\cdot x_i+b) - 1\geq 0,i=1,2,…,N$$</p><h2 id="凸二次规划算法"><a href="#凸二次规划算法" class="headerlink" title="凸二次规划算法"></a>凸二次规划算法</h2><p>当凸优化问题的目标函数是二次函数且约束函数$g_i(w)$是仿射函数时，凸最优化问题就成为凸二次规划问题，所以上述最优化问题是凸二次规划问题，已经有现成的解法可以求解这类问题，我们按照模板把参数设置进去即可。</p><blockquote><p>Quadratic Programming</p><p>optimal $u \leftarrow QP(Q,p,A,c)$，其中：Q是二次项的系数，p是一次项的系数</p><p>​    $min_u$         $\ \ \frac{1}{2}u^TQu+p^Tu$</p><p>subject to     $\ \ a_m^Tu \geq c_m$, for m = 1,2,…,M</p></blockquote><p>上面就是凸二次规划问题求解的参数模板，对比线性可分支持向量机最优化问题</p><blockquote><p>optimal (b, w) = ?</p><p>​      $min_{b, w}$   $\frac{1}{2}w^Tw$</p><p>subject to   $y_n(w^Tx_n+b)\geq1$, for n = 1, 2, …, N</p></blockquote><p>参数设置如下</p><p>object function :       $u = \begin{bmatrix} b \\ w\end{bmatrix}$; $Q = \begin{bmatrix}0 &amp;0_d^T \\ 0_d &amp; I_d\end{bmatrix}$;$p=0_{d+1}$</p><p>constrains :             $a_n^T=y_n\begin{bmatrix} 1 &amp; x_n^T\end{bmatrix}; c_n=1;M=N$</p><h3 id="模型理解"><a href="#模型理解" class="headerlink" title="模型理解"></a>模型理解</h3><p>如果求出了约束最优化问题的解$w^*$，$b^*$，那么就可以得到最大间隔分离超平面$w^*\cdot x+b^* = 0$及分类决策函数$f(x)=sign(w^*\cdot x+b^*)$。</p><p>在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector），支持向量是使约束条件等号成立的点，即</p><p>$$y_i(w\cdot x_i+b)-1=0$$</p><p>对$y_i=+1$的正例点，支持向量在超平面$H_1$上</p><p>$$H_1:w\cdot x+b = 1$$</p><p>对$y_i = -1$的负例点，支持向量在超平面$H_2$上</p><p>$$H_2:w\cdot x+b = -1$$</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090110.jpg" alt="Screen Shot 2018-04-19 at 11.43.51"></p><p>注意到$H_1$和$H_2$平行，并且没有实例点落在它们中间。在$H_1$与$H_2$之间形成了一条长带，分离超平面与它们平行且位于它们中央。长带的宽度，即$H_1$与$H_2$之间的距离称为间隔。间隔依赖于分离超平面的法向量$w$，等于$\frac{2}{||w||}$，$H_1$和$H_2$称为间隔边界。</p><p>在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用，如果移动支持向量将改变所求的解；但是如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。支持向量的个数一般很少，所有支持向量机由很少的“重要的”训练样本确定。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090112.jpg" alt="Screen Shot 2018-04-19 at 16.55.32"></p><h2 id="学习的对偶算法"><a href="#学习的对偶算法" class="headerlink" title="学习的对偶算法"></a>学习的对偶算法</h2><p>为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。这样做的有点有：一、是对偶问题往往更容易求解；二、自然引入核函数，进而推广到非线性分类的问题。</p><h3 id="定义拉格朗日函数"><a href="#定义拉格朗日函数" class="headerlink" title="定义拉格朗日函数"></a>定义拉格朗日函数</h3><p>$$\begin{align} L(w,b,\alpha) &amp;= \frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1) \\  &amp;= \frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\sum_{i=1}^N\alpha_i \end{align}$$</p><p>其中，$\alpha=(\alpha_1,\alpha_2,…,\alpha_N)^T$为拉格朗日乘子向量。</p><p>这样，我们得到了拉格朗日形式</p><p>$$min_{w,b}max_\alpha\ L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\sum_{i=1}^N\alpha_i, \ \ \ \alpha_i\geq0,i=1,2,…,N $$</p><blockquote><ol><li>原始问题</li></ol><p>说明原始最优化问题和拉格朗日极小极大问题是等价：</p><p>考虑这个形式：$L(w,b,\alpha) = \frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1)$</p><p>其中，$y_i(w\cdot x_i+b)-1\geq 0$，则使得$max_\alpha\ L(w,b,\alpha)$最大的$\alpha$，必须是所有分量都为0（因为$\alpha_i\geq0$）。这样，拉格朗日形式就变为$min_{w,b}\ L(w,b,\alpha)=\frac{1}{2}||w||^2$。可以发现，我们定义的拉格朗日形式和原始问题表达式是一致的，我们把这两者表达方式都叫原始问题。</p><p>更形式化的证明</p><p>假设$f(x)$,$c_i(x)$,$h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题</p><p>$$min_{x\in R^n}\ f(x)$$</p><p>$$s.t.\ \ c_i(x)\leq0,\ i=1,2,…,k$$</p><p>$$h_j(x)=0,\ j=1,2,…,l$$</p><p>称此约束最优化问题为原始最优化问题或原始问题。</p><p>首先，引进广义拉格朗日函数</p><p>$$L(x,\alpha,\beta)=f(x) + \sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)$$</p><p>这里，$x=(x^{(1)},x^{(2)},…,x^{(n)})^T\in R$，$\alpha_i，\beta_j$是拉格朗日乘子，$\alpha_i\geq0$，考虑$x$的函数：</p><p>$$\theta_p(x)=max_{\alpha,\beta;\alpha_i\geq0}\ L(x,\alpha,\beta)$$</p><p>这里，下标P表示原始问题。</p><p>假设对于某个$x$，如果$x$违反原始问题的约束条件，即存在某个$i$使得$c_i(w)&gt;0$或则存在某个$j$使得$h_j(w)\neq0$，那么就有</p><p>$$\theta_P(x)= max_{\alpha,\beta;\alpha_i\geq0}[f(x) + \sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)]=+\infty $$</p><p>相反的，如果$x$满足约束条件，则$\theta_P(x)=f(x)$。因此</p><p>$$\begin{split} \theta_P(x)=\begin{cases} f(x), &amp; \text{x满足原始问题约束} \\ +\infty, &amp; \text{otherwise}\end{cases}\end{split}$$</p><p>所以如果考虑极小化问题</p><p>$$min_x\ \theta_P(x) = min_x max_{\alpha,\beta;\alpha_i\geq0}\ L(x, \alpha, \beta)$$</p><p>它是与原始最优化问题等价的，即他们有相同的解，这样一来，就把原始问题最优化问题表示为广义拉格朗日函数的极小极大问题。为了方便，定义原始问题的最优值</p><p>$$p^*=min_x\theta_P(x)$$</p><p>称为原始问题的值。</p></blockquote><p>通过以上分析，你可能会觉得，既然广义拉格朗日函数的极小极大问题和原始问题最优化问题本质上是一致的，那为什么要转化为拉格朗日表达式呢？原因在于，我们往往能够通过求解广义拉格朗日函数的极小极大问题的对偶问题来帮助我们求解原始问题。</p><h3 id="原始问题的对偶问题"><a href="#原始问题的对偶问题" class="headerlink" title="原始问题的对偶问题"></a>原始问题的对偶问题</h3><blockquote><ol><li>对偶问题</li></ol><p>定义</p><p>$$\theta_D(\alpha,\beta)=min_xL(x,\alpha,\beta)$$</p><p>再考虑极大化$\theta_D(\alpha,\beta)$，即</p><p>$$max_{\alpha,\beta;\alpha_i\geq0}\ \theta_D(\alpha,\beta)=max_{\alpha,\beta;\alpha_i\geq0}min_x\ L(x,\alpha,\beta)$$</p><p>问题$max_{\alpha,\beta;\alpha_i\geq0}min_x\ L(x,\alpha,\beta)$称为广义拉格朗日函数的极大极小问题。</p><p>可以将广义拉格朗日函数的极大极小问题表示为约束最优化问题：</p><p>$$max_{\alpha,\beta}\theta_D(\alpha,\beta)=max_{\alpha,\beta}min_x\ L(x, \alpha,\beta)$$</p><p>称为原始问题的对偶问题，定义对偶问题的最优值</p><p>$$d^*=max_{\alpha,\beta}\theta_D(\alpha,\beta)$$</p><p>称为对偶问题的值。</p></blockquote><p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：</p><p>$$max_\alpha min_{w,b}L(w,b,\alpha)$$</p><p>所以，为了得到对偶问题的解，需要先求$L(w,b,\alpha)$对$w,b$的极小，再求对$\alpha$的极大。</p><h4 id="1-求-min-w-b-L-w-b-alpha"><a href="#1-求-min-w-b-L-w-b-alpha" class="headerlink" title="(1) 求$min_{w,b}\ L(w, b, \alpha)$"></a>(1) 求$min_{w,b}\ L(w, b, \alpha)$</h4><p>$$\triangledown_wL(w,b,\alpha)=w-\sum_{i=1}^N\alpha_iy_ix_i=0\rightarrow w=\sum_{i=1}^N\alpha_iy_ix_i$$</p><p>$$\triangledown_bL(w,b,\alpha)=\sum_{i=1}^N\alpha_iy_i=0 \rightarrow\sum_{i=1}^N\alpha_iy_i=0$$</p><p>将上述结果带入到拉格朗日函数中，得</p><p>$$\begin{align} min_{w,b}L(w,b,\alpha) &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_iy_i((\sum_{j=1}^N\alpha_jy_jx_j)\cdot x_i+b)+\sum_{i=1}^N\alpha_i \\  &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-b\cdot \sum_{i=1}^N\alpha_iy_i + \sum_{i=1}^N\alpha_i\\  &amp;= -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i   \end{align}$$</p><h4 id="2-求-min-w-b-L-w-b-alpha-对-alpha-的极大"><a href="#2-求-min-w-b-L-w-b-alpha-对-alpha-的极大" class="headerlink" title="(2) 求$min_{w,b}L(w,b,\alpha)$对$\alpha$的极大"></a>(2) 求$min_{w,b}L(w,b,\alpha)$对$\alpha$的极大</h4><blockquote><ol><li>原始问题和对偶问题的关系</li></ol><p>C.1 若原始问题和对偶问题都有最优值，则</p><p>$$d^*（对偶问题） = max_{\alpha,\beta;\alpha_i\geq0}\ min_xL(x,\alpha,\beta)\leq min_x max_{\alpha,\beta;\alpha_i\geq0}L(x,\alpha,\beta)=p^*（原始问题）$$</p><p><strong>证明</strong></p><p>$$\theta_D(\alpha,\beta)=min_x\ L(x,\alpha,\beta)\leq L(x,\alpha,\beta)\leq max_{\alpha,\beta}\theta_D(\alpha,\beta)= \theta_P$$</p><p>即</p><p>$$\theta_D(\alpha,\beta)\leq \theta_P(x)$$</p><p>由于原始问题和对偶问题均有最优值，所以</p><p>$$max_{\alpha,\beta;\alpha_i\geq0}\ \theta_D(\alpha,\beta)\leq min_x\ \theta_p(x)$$</p><p>即</p><p>$$d^* = max_{\alpha,\beta;\alpha_i\geq0}min_x\ L(x,\alpha,\beta)\leq min_x max_{\alpha,\beta}\theta_D(\alpha,\beta)= p^*$$</p><p>在某些条件下，原始问题和对偶问题的最优值相等，$d^*=p^*$，这时可以用解对偶问题替代原始问题。</p><p>定理1 </p><p>假设函数$f(x)$和$c_i(x)$是凸函数，$h_j(x)$是放射函数；并且假设不等式约束$c_i(x)$是严格可行的，即存在$x$，对所有$i$有$c_i(x)&lt;0$，则存在$x^*,\alpha^*,\beta^*$，使$x^*$是原始问题的解，$\alpha^*，\beta^*$是对偶问题的解，并且</p><p>$$p^*=d^*= L(x^*,\alpha^*,\beta^*)$$</p><p>定理2 KKT条件</p><p>$x^*$和$\alpha^*，\beta^*$分别是原始问题和对偶问题的解的充分必要条件是$x^*$，$\alpha^*$，$\beta^*$满足KKT条件</p><ol><li><p>primal feasible: $y_n(w^Tx_n+b)\geq1$</p></li><li><p>dual feasible: $\alpha_n\geq0$</p></li><li><p>Dual-inner optimal:  $\sum y_n\alpha_n=0$，$w=\sum \alpha_ny_nx_n$</p></li><li><p>Primal-inner optimal（在最优的情况下，所有的拉格朗日项将消失）:</p><p>$$\alpha_n(1-y_n(w^Tx_n+b)) = 0$$</p></li></ol></blockquote><p>由上面定理可知，我们可以通过求对偶问题来求解原始问题</p><p>$$max_{\alpha}\ -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i  $$</p><p>$$s.t.\ \sum_{i=1}^N\alpha_iy_i=0,\alpha_i\geq0,i=1,2,…,N$$</p><p>等价于</p><p>$$min_{\alpha}\ \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i  $$</p><p>$$s.t.\ \sum_{i=1}^N\alpha_iy_i=0,\alpha_i\geq0,i=1,2,…,N$$</p><p>求解上面式子，最后可得</p><p>$$w^* = \sum_{i=1}^N\alpha_i^*y_ix_i$$</p><p>$$b^*=y_j-\sum_{i=1}^N\alpha^*y_i(x_i\cdot x_j)$$</p><h3 id="模型理解-1"><a href="#模型理解-1" class="headerlink" title="模型理解"></a>模型理解</h3><p>有上面的推导，分离超平面可以写成</p><p>$$\sum_{i=1}^N\alpha_i^*y_i(x\cdot x_i)+b^*=0$$</p><p>分类决策函数可以写成</p><p>$$f(x) = sign(\sum_{i=1}^N\alpha_i^*y_i(x\cdot x_i)+b^*)$$</p><p>考虑原始最优化问题中，由KKT互补条件可知：</p><p>$$\alpha_i^*(y_i(w^*\cdot x_i+b^*)-1)=0，i=1,2,…,N$$</p><p>对应于$\alpha_i^*&gt;0$的实例有$x_i$,有</p><p>$$y_i(w^*\cdot x_i+b^*)-1=0$$</p><p>即$x_i$一定在间隔边界上，所以我们在用模型对未知样本进行分类的时候，只需要将样本与支持向量做运算即可。</p><h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090109.jpg" alt="Screen Shot 2018-04-19 at 16.56.15"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090107.jpg" alt="Screen Shot 2018-04-19 at 16.56.30"></p><h1 id="线性支持向量机与软间隔最大化"><a href="#线性支持向量机与软间隔最大化" class="headerlink" title="线性支持向量机与软间隔最大化"></a>线性支持向量机与软间隔最大化</h1><h2 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h2><p>线性可分问题的支持向量机学习方法，对线性不可分训练数据是不适用的，因为这时上述方法中的不等式约束并不能都成立。怎么才能将它扩展到线性不可分问题呢？这就需要修改硬间隔最大化，使其成为软间隔最大化。</p><p>线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件。为了解决这个问题，可以对每个样本点$(x_i,y_i)$引进一个松弛变量$\xi_i \geq 0$，使得函数间隔加上松弛变量大于等于1，这样约束条件为</p><p>$$y_i(w\cdot x_i+b)+\xi_i\geq1 \rightarrow y_i(w\cdot x_i+b)\geq 1-\xi_i$$</p><p>同时，对每个松弛变量$\xi_i$，支付一个代价$\xi_i$，目标函数变成</p><p>$$\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i$$</p><p>这里，$C&gt;0$称为惩罚参数，一般由应用问题决定，$C$值大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减小。最优化目标函数包含两层含义：（1）使$\frac{1}{2}||w||^2$尽量小即间隔尽量大（2）同时使误分类点的个数尽量小，C是调和二者的系数。</p><p>线性不可分的线性支持向量机的学习问题如下</p><p>$$min_{w,b,\xi}\ \ \frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i$$</p><p>$$s.t.\ \ \ y_i(w\cdot x_i+b)\geq1-\xi_i,\ i=1,2,…,N$$</p><p>$$s.t.\ \ \xi_i\geq0,\ i=1,2,…,N$$</p><h2 id="凸二次规划求解"><a href="#凸二次规划求解" class="headerlink" title="凸二次规划求解"></a>凸二次规划求解</h2><p>上述优化问题可以用凸二次规划方法来求解，可以证明$w​$的解释唯一的，但$b​$的解不唯一，$b​$的解存在于一个区间。</p><h2 id="学习的对偶算法-1"><a href="#学习的对偶算法-1" class="headerlink" title="学习的对偶算法"></a>学习的对偶算法</h2><p>原始最优化问题的拉格朗日函数是</p><p>$$L(w,b,\xi,\alpha,\beta)=\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum_{i=1}^N\beta_i\xi_i$$</p><p>其中，$\alpha_i\geq0$，$\beta_i\geq0$</p><p>对偶问题是拉格朗日函数 的极大极小问题，即</p><p>$$max_{\alpha,\beta} min_{w,b,\xi}L(w,b,\xi,\alpha,\beta)$$</p><h3 id="求解对偶问题"><a href="#求解对偶问题" class="headerlink" title="求解对偶问题"></a>求解对偶问题</h3><h4 id="1-求-min-w-b-xi-L-w-b-xi-alpha-mu"><a href="#1-求-min-w-b-xi-L-w-b-xi-alpha-mu" class="headerlink" title="(1) 求$min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$"></a>(1) 求$min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$</h4><p>对$w,b,\xi$求偏导，令其为0</p><p>$$\triangledown_wL(w,b,\xi,\alpha,\beta)=w-\sum_{i=1}^N\alpha_iy_ix_i=0$$</p><p>$$\triangledown_bL(w,b,\xi,\alpha,\beta)=-\sum_{i=1}^N\alpha_iy_i=0$$</p><p>$$\triangledown_{\xi_i} L(w,b,\xi,\alpha,\beta)=C-\alpha_i-\beta_i=0$$</p><p>得</p><p>$$w = \sum_{i=1}^N\alpha_iy_ix_i$$</p><p>$$\sum_{i=1}^N\alpha_iy_i=0$$</p><p>$$C-\alpha_i-\beta_i=0$$</p><p>我们通过代数替换，将$\beta_i$换成$C-a_i$，并规定</p><p>$$C-a_i\geq 0 \rightarrow \alpha_i \leq C$$</p><p>将$\beta_i=C-\alpha_i$，$w = \sum_{i=1}^N\alpha_iy_ix_i$带入L中，得</p><p>$$\begin{align} L(w,b,\xi,\alpha,\beta) &amp;= \frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^N\alpha_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum_{i=1}^N(C-\alpha_i)\xi_i \\  &amp;= \frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\sum_{i=1}^N\alpha_i-\sum_{i=1}^N\alpha_i\xi_i-C\sum_{i=1}^N\xi_i+\sum_{i=1}^N\alpha_i\xi_i \\  &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) -\sum_{i=1}^N\alpha_i  \\ \end{align}$$</p><h4 id="2-求-min-w-b-xi-L-w-b-xi-alpha-mu-对-alpha-的极大"><a href="#2-求-min-w-b-xi-L-w-b-xi-alpha-mu-对-alpha-的极大" class="headerlink" title="(2) 求$min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$对$\alpha$的极大"></a>(2) 求$min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$对$\alpha$的极大</h4><p>$$max_\alpha\ -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i$$</p><p>$$s.t.\ \sum_{i=1}^N\alpha_iy_i=0$$</p><p>$$0\leq \alpha_i\leq C$$</p><p>对偶问题取得最优解需要满足如下的KKT条件</p><blockquote><p>KKT 条件</p><p>$$w^*-\sum_{i=1}^N\alpha_i^*y_ix_i=0$$</p><p>$$-\sum_{i=1}^N\alpha_i^*y_i=0$$</p><p>$$C-\alpha^*-\beta^*=0$$</p><p>$$\alpha_i^*(y_i(w^*\cdot x_i+b^*)-1+\xi^*_i)=0$$</p><p>$$\beta_i^*\xi_i^*=0$$</p><p>$$y_i(w^*\cdot x_i+b^*)-1+\xi_i^*\geq0$$</p><p>$$\xi_i^*\geq0$$</p><p>$$\alpha_i^*\geq0$$</p><p>$$\beta_i^*\geq0$$</p></blockquote><h3 id="模型理解-2"><a href="#模型理解-2" class="headerlink" title="模型理解"></a>模型理解</h3><p>设$\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)^T$是对偶问题的一个解，若存在$\alpha^*$的一个分量$\alpha_j^*$，$0&lt;\alpha_j^*&lt;C$，则原始问题的解$w^*,b^*$可按下式求得</p><p>$$w^*=\sum_{i=1}^N\alpha_i^*y_ix_i$$</p><p>$$b^*=y_j-\sum_{i=1}^Ny_i\alpha_i^*(x_i\cdot x_j)$$</p><p>在线性不可分的情况下，将对偶问题的解$\alpha^*=(\alpha_1^*,\alpha_2^*,…,\alpha_N^*)^T$中对应于$\alpha_i^*&gt;0$的样本点$(x_i,y_i)$的实例$x_i$称为支持向量（软间隔的支持向量），这时的支持向量要比线性可分时的情况复杂一些</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-20-031530.jpg" alt="Screen Shot 2018-04-20 at 10.32.56"></p><p>软间隔的支持向量$x_i$或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。</p><p>考虑两个KKT条件</p><blockquote><p>$$\alpha_i^*(y_i(w^*\cdot x_i+b^*)-1+\xi^*_i)=0$$</p><p>$$\beta_i^*\xi_i^*=0 \rightarrow (C-\alpha_i^*)\xi_i^*=0$$</p></blockquote><ul><li>若$\alpha_i = 0$，由第二个条件得，$\xi_i=0$，说明这些点没有违反边界，一般在边界之外；</li><li>若$0&lt;\alpha_i&lt; C$，由第二个条件得，$\xi_i=0$，由第一个条件得，$y_i(w^*\cdot x_i+b^*)-1+\xi^*_i=0$，综上，得$y_i(w^*\cdot x_i+b^*)-1=0$，则这些点在边界上。</li><li>若$\alpha_i = C$，样本点违反边界值，违反距离为$\xi_i=1-y_n(w^Tx_i+b)$<ul><li>$0&lt;\xi_i&lt;1$，则分类正确，$x_i$在间隔边界与分离超平面之间；</li><li>$\xi_i=1$，则$x_i$在分离超平面上；</li><li>$\xi_i&gt;1$，则$x_i$位于分离超平面误分一面；</li></ul></li></ul><h3 id="合页损失函数"><a href="#合页损失函数" class="headerlink" title="合页损失函数"></a>合页损失函数</h3><p>线性支持向量机还有另一种解释，就是最小化以下目标函数</p><p>$$min_{b,w}\ \ \sum_{i=1}^N[1-y_i(w\cdot x_i+b)]_++\lambda||w||^2$$</p><blockquote><p>直观理解</p><p>对于目标函数</p><p>$$\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i$$</p><p>我们考虑$\xi_i$</p><p>$$\begin{split} \xi_n=\begin{cases} 1-y_n(w^Tz_n+b), &amp; \text{$(x_n,y_n)$违反了边界} \\ 0, &amp; \text{$(x_n,y_n)$没有违反边界}\end{cases}\end{split}$$</p><p>所以，我们可以把$\xi_n$改写如下</p><p>$$\begin{align} \xi_n &amp;= max(1-y_n(w^Tz_n+b),0) \\  &amp;= [1-y_i(w\cdot x_i+b)]_+  \end{align}$$</p></blockquote><p>严格证明</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-20-031533.jpg" alt="Screen Shot 2018-04-20 at 11.11.00"></p><p>所以，线性支持向量机学习等价于最小化二阶范数正则化的合页函数。</p><h1 id="非线性支持向量机与核函数"><a href="#非线性支持向量机与核函数" class="headerlink" title="非线性支持向量机与核函数"></a>非线性支持向量机与核函数</h1><p>对解线性分类问题，线性分类支持向量机是一种非常有效的方法。但是，有时候分类问题是非线性的，这时可以使用非线性支持向量机。其主要特点是利用核技巧。</p><h2 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-074025.jpg" alt="two_circles"></p><p>如图所示的数据集，其理想的分界应该是一个“圆圈”而不是一条线（超平面）。如果用$X_1$和$X_2$来表示这个二维平面的两个坐标的话，一条二次曲线的方差可以写成主要的形式</p><p>$$a_1X_1+a_2X_1^2+a_3X_2+a_4X_2^2+a_5X_1X_2+a_6=0$$</p><p>注意上面的形式，如果我们构建另外一个五维的空间，其中五个坐标的值分别为$Z_1=X_1,Z_2=X_1^2,Z_3=X_2,Z_4=X_2^2,Z_5=X_1X_2$，那么显然，上面的方程在新的坐标系下可以写作</p><p>$$\sum_{i=1}^5a_iZ_i+a_6=0$$</p><p>关于新的坐标$Z$，这正是一个超平面的方差，也就是说，如果我们做一个映射$\varphi :R^2\rightarrow R^5$，将上面的规则映射为$Z$，那么在新的空间中原来的数据将变成线性可分的。</p><p>上面的例子说明，用线性分类方法求解非线性分类问题分为两步：（1）首先使用一个变换将原空间中的数据映射到新空间；（2）然后再新空间里用线性分类学习方法从训练数据中学习分类模型。</p><blockquote><p>核函数的定义</p><p>设$\chi $是输入空间（欧式空间$R^n$的子集或离散集合），又设$H$为特征空间，如果存在一个从$\chi$到$H$的映射</p><p>$$\varphi (x):\chi\rightarrow H$$</p><p>使得对所有$x,z\in\chi$，函数$K(x,z)$满足条件</p><p>$$K(x,z)=\varphi(x)\cdot \varphi(z)$$</p><p>则称$K(x,z)$为核函数，$\varphi$(x)为映射函数，式中$\varphi(x)\cdot \varphi(z)$为$\varphi(x)$和$\varphi(z)$的内积。</p></blockquote><p>核技巧的想法是，在学习与预测中只定义核函数$K(x,z)$，而不显式的定义映射函数$\varphi$。</p><h2 id="核技巧在支持向量机中的应用"><a href="#核技巧在支持向量机中的应用" class="headerlink" title="核技巧在支持向量机中的应用"></a>核技巧在支持向量机中的应用</h2><h3 id="对偶问题目标函数"><a href="#对偶问题目标函数" class="headerlink" title="对偶问题目标函数"></a>对偶问题目标函数</h3><p>$$W(\alpha)=\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i$$</p><h3 id="分类决策函数"><a href="#分类决策函数" class="headerlink" title="分类决策函数"></a>分类决策函数</h3><p>$$f(x) = sign(\sum_{i=1}^N\alpha_i^*y_i\varphi(x_i)\cdot\varphi(x)+b^*)=sign(\sum_{i=1}^N\alpha_i^*y_iK(x_i,x)+b^*)$$</p><p>这等价于经过映射函数$\varphi$将原来的输入空间变换到一个新的特征空间，将输入空间中的内积$x_i\cdot x_j$变换为特征空间中的内积$\varphi(x_i)\cdot \varphi(x_j)$，在新的特征空间里从训练样本中学习线性支持向量机。当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型。</p><p>也就是说，在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间中进行的，不需要显示的定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。</p><h2 id="核函数类型"><a href="#核函数类型" class="headerlink" title="核函数类型"></a>核函数类型</h2><p>先看一下正定核的一些定义，通常所说的核函数就是正定核函数。</p><blockquote><p>设K：$\chi \times  \chi \rightarrow R$是对称函数，则$K(x,z)$为正定核函数的充要条件是对任意$x_i\in \chi$，$i=1,2,…,m$，$K(x,z)$对应的$Gram$矩阵：</p><p>$$K=[K(x_i,x_j)]_{m\times m}$$</p><p>是半正定矩阵。</p></blockquote><p>这一定义在构造核函数时很有用，但对于一个具体函数$K(x,z)$来说，检验它是否为正定核函数并不容易，因为要求对任意有限输入集${x_1,x_2,…,x_m}$验证$K$对应的$Gram$矩阵是否为半正定的。</p><h3 id="多项式核函数"><a href="#多项式核函数" class="headerlink" title="多项式核函数"></a>多项式核函数</h3><p>$$K(x,z)=(x\cdot z+1)^p$$</p><p>对应的支持向量机是一个$p$次多项式分类器，再此情形下，分类决策函数变为</p><p>$$f(x)=sign(\sum_{i=1}^Na_i^*y_i(x_i\cdot x+1)^p+b^*)$$</p><h3 id="高斯核函数"><a href="#高斯核函数" class="headerlink" title="高斯核函数"></a>高斯核函数</h3><p>$$K(x,z)=exp(-\frac{||x-z||^2}{2\sigma^2})$$</p><p>对应的支持向量机是高斯径向基函数(radial basis function)分类器，在此情形下，分类决策函数变为</p><p>$$f(x) = sign(\sum_{i=1}^Na_i^*y_iexp(-\frac{||x-z||^2}{2\sigma^2})+b^*)$$</p><h3 id="核函数选择"><a href="#核函数选择" class="headerlink" title="核函数选择"></a>核函数选择</h3><table><thead><tr><th style="text-align:center">Time of learning</th><th style="text-align:center">linear &lt; poly &lt; rbf</th></tr></thead><tbody><tr><td style="text-align:center">Ability of  fit any data</td><td style="text-align:center">linear &lt; poly &lt; rbf</td></tr><tr><td style="text-align:center">Risk of Overfitting</td><td style="text-align:center">linear &lt; poly &lt; rbf</td></tr><tr><td style="text-align:center">risk of underfitting</td><td style="text-align:center">rbf &lt; poly &lt; linear</td></tr><tr><td style="text-align:center">number of hyperparameters</td><td style="text-align:center">linear(0) &lt; rbf(2) &lt; poly(3)</td></tr><tr><td style="text-align:center">how “local” is particular kernel</td><td style="text-align:center">linear &lt; poly &lt; rbf</td></tr></tbody></table><p>民间口诀</p><p>初级：高维用线性，不行换特征；低维试线性，不行换高斯。</p><p>中级：线性试试看，不行换高斯，卡方有奇效，绝招MKL；</p><p>玩家：Kernel度量相似性，自己做啊自己做。</p><h2 id="模型理解-3"><a href="#模型理解-3" class="headerlink" title="模型理解"></a>模型理解</h2><p>利用核技巧，可以将线性分类的学习方法应用到非线性分类问题中去。将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数。</p><h2 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-074028.jpg" alt="Screen Shot 2018-04-21 at 10.11.50"></p><h1 id="Soft-Binary-Classification"><a href="#Soft-Binary-Classification" class="headerlink" title="Soft Binary Classification"></a>Soft Binary Classification</h1><p>考虑Hinge损失函数形式的SVM目标函数</p><p>$$min_{b,w}\ \ C\cdot \sum_{i=1}^Nmax(0, 1-y_i(w\cdot x_i+b))+\frac{1}{2}||w||^2$$</p><p>把该目标函数与下面的形式进行对比</p><p>$$min\ \ \frac{1}{2}w^Tw+C\sum \hat{err}$$</p><p>所以，软间隔的SVM也可以写成带有L2正则化的优化目标函数。</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">minimize</th><th style="text-align:center">constraint</th></tr></thead><tbody><tr><td style="text-align:center">regularization by constraint</td><td style="text-align:center">$E_{in}$</td><td style="text-align:center">$w^Tw\leq C$</td></tr><tr><td style="text-align:center">Hard-margin SVM</td><td style="text-align:center">$w^Tw$</td><td style="text-align:center">$E_{in} = 0$</td></tr><tr><td style="text-align:center">L2 regularization</td><td style="text-align:center">$\frac{\lambda}{N}w^Tw+E_{in}$</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">Soft-margin SVM</td><td style="text-align:center">$\frac{1}{2}w^Tw+CN\hat{E_{in}}$</td></tr></tbody></table><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-74029.jpg" alt="Screen Shot 2018-04-21 at 10.49.44"></p><p>从上面的分析，我们可以看出来，如果我们解决了一个软间隔的SVM问题，就相当于解决了一个L2正则化的模型，反之也成立，那么我们是否可以结合两者进行处理呢？</p><p>结合SVM模型和逻辑回归模型，分如下两个步骤</p><ol><li>SVM flavor：求解SVM中的$w_{svm}$和$b_{svm}$；</li><li>LogReg flavor：调整超平面来满足极大似然估计。</li></ol><p>这样，得到的新的目标函数为</p><p>$$min_{A,B}\ \ \frac{1}{N}\sum_{i=1}^Nlog[1+exp(-y_n(A\cdot (w_{svm}^T\varphi(x_n)+b_{svm})+B))]$$</p><h1 id="Kernel-Logistic-Regression"><a href="#Kernel-Logistic-Regression" class="headerlink" title="Kernel Logistic Regression"></a>Kernel Logistic Regression</h1><blockquote><p>如果我们现在要解决的带有L2正则化的线性模型</p><p>$$min_w \ \ \frac{\lambda}{N}w^Tw+\frac{1}{N}\sum_{n=1}^Nerr(y_n, w^Tz_n)$$</p><p>那么最优$w$，会是输入的线性组合，即$w^*=\sum_{i=1}^N\beta_nz_n$</p></blockquote><p>如果是这样的话，那么我们可以将求解最佳$w$的问题，转化为求解最佳的$\beta$</p><p>$$min_{\beta}\ \frac{\lambda}{N}\sum_{i=1}^N\sum_{j=1}^N\beta_i\beta_jK(x_i,x_j)+\frac{1}{N}\sum_{i=1}^Nlog(1+exp(-y_n\sum_{j=1}^N\beta_jK(x_i,x_j)))$$</p><p>对于该模型可以有如下关于$\beta$的解释</p><ul><li>$\sum_{i=1}^N\beta_iK(x_i,x_j)$可以看做是$\beta$变量和转换过的数据做线性组合$(K(x_1,x_n),K(x_2,x_n),…,K(x_N,x_n))$</li><li>$\sum_{i=1}^N\sum_{j=1}^N\beta_i\beta_jK(x_i,x_j)$：关于$\beta$的特殊形式的正则化$\beta^TK\beta$</li><li>Kernel logistic regression 可以理解为关于$\beta$的线性模型，该模型用核函数做转换，并带有核函数的正则化。</li><li>和SVM不同的是，$\beta_i$通常都不是0</li></ul><h1 id="Support-Vector-Regression"><a href="#Support-Vector-Regression" class="headerlink" title="Support Vector Regression"></a>Support Vector Regression</h1><h2 id="Kernel-Ridge-Regression"><a href="#Kernel-Ridge-Regression" class="headerlink" title="Kernel Ridge Regression"></a>Kernel Ridge Regression</h2><blockquote><p>如果我们现在要解决的带有L2正则化的线性模型</p><p>$$min_w \ \ \frac{\lambda}{N}w^Tw+\frac{1}{N}\sum_{n=1}^Nerr(y_n, w^Tx_n)$$</p><p>那么最优$w$，会是输入的线性组合，即$w^*=\sum_{i=1}^N\beta_nx_n$</p></blockquote><p>现在，要解决的是回归的问题，我们令$err(y,w^Tx)=(y-w^Tx)^2$，带有正则化的回归模型被称为ridge regression，现在，我们看一下是否可以结合kernel函数和ridge regression？</p><p>将最佳解$w^*=\sum_{i=1}^N\beta_nx_n$带入到目标函数中</p><p>$$min_\beta\ \ \frac{\lambda}{N}\sum_{i=1}^N\sum_{j=1}^N\beta_i\beta_jK(x_n,x_m)+\frac{1}{N}\sum_{i=1}^N[y_i-\sum_{j=1}^N\beta_jK(x_i,x_j)]^2$$</p><p>我们可以使用梯度下降法求解上述表达式。求解结果为：$\beta=(\lambda I+K)^{-1}y$</p><p>所以，借助核函数，我们可以很轻易的解决非线性的回归问题。</p><p>比较线性回归和Kernel Ridge Regression</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-74026.jpg" alt="Screen Shot 2018-04-21 at 15.04.30"></p><h2 id="Tube-Regression"><a href="#Tube-Regression" class="headerlink" title="Tube Regression"></a>Tube Regression</h2><p>在之前的线性回归模型中，我们距离的计算公式为：$|s-y|$。我们现在借助SVM的想法，设置一个间隔，如果预测值里真实值的距离在该间隔内，我们就不要计算该损失。</p><p>$$err(y,s)=max(0,|s-y|-\varepsilon )$$</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-074030.jpg" alt="Screen Shot 2018-04-21 at 15.12.00"></p><p>通过这样的定义，我们希望借助SVM的求解方式，来得到稀疏的$\beta$。</p><h3 id="Tube-versus-Squared-Regression"><a href="#Tube-versus-Squared-Regression" class="headerlink" title="Tube versus Squared Regression"></a>Tube versus Squared Regression</h3><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-74031.jpg" alt="Screen Shot 2018-04-21 at 15.14.06"></p><p>模仿SVM的优化问题，我们将上述写成</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-21-074031.jpg" alt="Screen Shot 2018-04-21 at 15.17.51"></p><h1 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h1><h2 id="sklearn-svm"><a href="#sklearn-svm" class="headerlink" title="sklearn.svm"></a>sklearn.svm</h2><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><ul><li>linear: $&lt;x,x’&gt;$</li><li>polynomial:$(\gamma &lt;x,x’&gt;+r)^d$</li><li>rbf: $exp(-\gamma||x-x’||^2)$</li><li>sigmoid: $tanh(\gamma&lt;x,x’&gt;+r)$</li></ul><h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><table><thead><tr><th></th><th style="text-align:center">SVC</th><th style="text-align:center">NuSVC</th><th style="text-align:center">LinearSVC</th></tr></thead><tbody><tr><td>Multi-class</td><td style="text-align:center">one-vs-one</td><td style="text-align:center">one-vs-one</td><td style="text-align:center">one-vs-rest</td></tr><tr><td>kernel</td><td style="text-align:center">rbf</td><td style="text-align:center">rbf</td><td style="text-align:center">linear</td></tr></tbody></table><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” <em>清华大学出版社, 北京</em> (2012).</p><p>[2]. <a href="https://stackoverflow.com/questions/33778297/support-vector-machine-kernel-types" target="_blank" rel="noopener">Support Vector Machine kernel types</a></p><p>[3]. <a href="https://www.zhihu.com/question/21883548" target="_blank" rel="noopener">SVM的核函数如何选取？</a></p><p>[4]. <a href="https://www.youtube.com/watch?v=oOi7kqUTqxw&amp;index=10&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2" target="_blank" rel="noopener">林轩田 机器学习基石</a></p><p>[5]. <a href="http://blog.pluskid.org/?page_id=683" target="_blank" rel="noopener">漫谈支持向量机系列</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;http://p6sh0jwf6.bkt.clouddn.com/2018-04-19-090106.jpg&quot; alt=&quot;svm&quot;&gt;&lt;/p&gt;
&lt;p&gt;支持向量机（support vector machines,SVM）是一种二类分类模型，它的基本模型是定义
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Classification" scheme="http://conghuai.me/categories/Machine-Learning/Classification/"/>
    
      <category term="Regression" scheme="http://conghuai.me/categories/Machine-Learning/Regression/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="svm" scheme="http://conghuai.me/tags/svm/"/>
    
  </entry>
  
  <entry>
    <title>Improved Iterative Scaling</title>
    <link href="http://conghuai.me/2018/04/19/Improved-Iterative-Scaling/"/>
    <id>http://conghuai.me/2018/04/19/Improved-Iterative-Scaling/</id>
    <published>2018-04-19T00:18:16.000Z</published>
    <updated>2018-04-19T02:11:53.426Z</updated>
    
    <content type="html"><![CDATA[<p>逻辑斯蒂回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。从最优化的观点看，这时的目标函数具有很好的性质。它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法。</p><h1 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h1><h2 id="第一次下界：-A-delta-w"><a href="#第一次下界：-A-delta-w" class="headerlink" title="第一次下界：$A(\delta|w)$"></a>第一次下界：$A(\delta|w)$</h2><p>改进的迭代尺度法(improved iterative scaling, IIS)是一种最大熵模型学习的最优化算法。</p><p>已知最大熵模型为</p><p>$$P_w(y|x)=\frac{1}{Z_w(x)}exp(\sum_{i=1}^nw_if_i(x,y))$$</p><p>其中，</p><p>$$Z_w(x)=\sum_yexp(\sum_{i=1}^nw_if_i(x,y))$$</p><p>对数似然函数为</p><p>$$L(w)=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\tilde{P}(x)logZ_w(x)$$</p><p>目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值$\hat{w}$</p><p><strong>IIS的想法</strong>是：假设最大熵模型当前的参数向量是$w=(w_1,w_2,…,w_n)^T$，我们希望找到一个新的参数向量$w+\delta =(w_1+\delta_1 ,w_2+\delta_2 ,…,w_n+\delta_n)$，使得模型的对数似然函数值增大，如果能有这样一种参数向量更新的方法：$\tau \rightarrow w+\delta$，那么就可以重复使用这一方法，直到找到对数似然函数的最大值。</p><p>对于给定的经验分布$\tilde{P}(x,y)$，模型参数从$w$到$w+\delta$，对数似然函数的改变量是</p><p>$$\begin{align} L(w+\delta) - L(w) &amp;=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n(w+\delta)_if_i(x,y)-\sum_x\tilde{P}(x)logZ_{w+\delta}(x) \\  &amp;- \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\tilde{P}(x)logZ_w(x) \\  &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)-\sum_x\tilde{P}(x)log\frac{Z_{w+\delta}(x)}{Z_w(x)}  \end{align}$$</p><p>利用不等式</p><p>$$-log\alpha \geq1-\alpha,\alpha&gt;0$$</p><blockquote><p>证明上述不等式：</p><p>令 g(x) = x - lnx -1</p><p>g’(x) = 1 - $\frac{1}{x}$ = 0 得 x = 1</p><p>g(x) $\geq $ g(1) = 0，因此，x - lnx - 1 $\geq$ 0</p></blockquote><p>我们可以得到，注意$Z_w(x)=\sum_yexp(\sum_{i=1}^nw_if_i(x,y))$</p><p>$$\begin{align} L(w+\delta) - L(w) &amp;\geq \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+\sum_x\tilde{P}(x)(1-\frac{Z_{w+\delta}(x)}{Z_w(x)})  \\ &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+\sum_x\tilde{P}(x)- \sum_x\tilde{P}(x)\frac{Z_{w+\delta}(x)}{Z_w(x)} \\ &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+\sum_x\tilde{P}(x)- \sum_x\tilde{P}(x)\frac{\sum_yexp(\sum_{i=1}^n(w_i+\delta_i)f_i(x,y))}{\sum_yexp(\sum_{i=1}^nw_if_i(x,y))} \\&amp;=  \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+\sum_x\tilde{P}(x)- \sum_x\tilde{P}(x)\sum_yexp\sum_{i=1}^n\delta_i f_i(x,y) \end{align} $$</p><p>我们再利用$\sum_x \tilde{P}(x)=1$和$\sum_yP_w(y|x)=1$对上式进行变形，其中前者在式子中出现，我们将其变为1；后者我们强行将其放到式子最后一项的乘法中。</p><p>$$\begin{align} L(w+\delta) - L(w) &amp;=  \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1- \sum_x\tilde{P}(x)\sum_yP_w(y|x)\sum_yexp\sum_{i=1}^n\delta_i f_i(x,y) \\&amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1- \sum_x\tilde{P}(x)\sum_yP_w(y|x)exp\sum_{i=1}^n\delta_i f_i(x,y)\end{align} $$</p><p>将上式右端记为</p><p>$$A(\delta|w)=  \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1- \sum_x\tilde{P}(x)\sum_yP_w(y|x)exp\sum_{i=1}^n\delta_i f_i(x,y)$$</p><p>于是有</p><p>$$L(w+\delta)-L(w)\geq A(\delta|w)$$</p><p>即$A(\delta|w)$是对数似然函数改变量的一个下界。</p><p>如果能找到适当的$\delta$使下界$A(\delta|w)$提高，那么对数似然函数也会提高。然而，函数$A(\delta|w)$中的$\delta$是一个向量，含有多个变量，不易同时优化。IIS试图一次只优化其中一个变量$\delta_i$，而固定其他变量$\delta_j$，$i\neq j$。</p><h2 id="第二次下界：-B-delta-w"><a href="#第二次下界：-B-delta-w" class="headerlink" title="第二次下界：$B(\delta|w)$"></a>第二次下界：$B(\delta|w)$</h2><p>为了达到上述的目的，IIS进一步降低下界$A(\delta|w)$，具体地，IIS引进一个量$f^\#(x,y)$</p><p>$$f^\#(x,y)=\sum_if_i(x,y)$$</p><p>因为$f_i$是二值函数，估$f^\#(x,y)$表示所有特征在$(x,y)$出现的次数，这样，改写$A(\delta|w)$为</p><p>$$A(\delta|w)=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x)exp(f^\#(x,y)\sum_{i=1}^n\frac{\delta_if_i(x,y)}{f^\#(x,y)})$$</p><p>利用指数函数的凸性以及对任意$i$，有$\frac{f_i(x,y)}{f^\#(x,y)}\geq 0$且$\sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}=1$，利用Jensen不等式得到</p><blockquote><p>Jensen不等式的有限形式</p><p>$$\varphi (\sum_{i=1}^ng(x_i)\lambda_i)\leq \sum_{i=1}^n\varphi(g(x_i))\lambda_i$$</p></blockquote><p>将$\varphi \leftarrow exp$，$g(x_i) \leftarrow \delta_if^\#(x,y)$，$\lambda_i \leftarrow \frac{f_i(x,y)}{f^\#(x,y)}$带入到Jensen不等式中，得</p><p>$$exp(\sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}\delta_if^\#(x,y))\leq \sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}exp(\delta_if^\#(x,y))$$</p><p>带入到$A(\delta|w)$式子中，得</p><p>$$A(\delta|w)\geq\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x) \sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}exp(\delta_if^\#(x,y))$$</p><p>记右端为</p><p>$$B(\delta|w)= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x) \sum_{i=1}^n\frac{f_i(x,y)}{f^\#(x,y)}exp(\delta_if^\#(x,y))$$</p><p>于是得到</p><p>$$L(w+\delta) - L(w) \geq B(\delta|w)$$</p><p>这里，$B(\delta|w)$是对数似然函数该变量的一个新的下界，求$B(\delta|w)$对$\delta_i$的偏导数</p><p>$$\begin{align} \frac{\partial B(\delta|w)}{\partial \delta_i} &amp;= \sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_x\tilde{P}(x)\sum_yP_w(y|x)\frac{f_i(x,y)}{f^\#(x,y)}exp(\delta_if^\#(x,y))f^\#(x,y) \\ &amp;= \sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_x\tilde{P}(x)\sum_yP_w(y|x)f_i(x,y)exp(\delta_if^\#(x,y)) \end{align}$$</p><p>在上式中，除了$\delta_i$外不含有任何其他变量，令偏导数为0，得到</p><p>$$\sum_{x,y}\tilde{P}(x,y)f_i(x,y)+1-\sum_x\tilde{P}(x)\sum_yP_w(y|x)f_i(x,y)exp(\delta_if^\#(x,y)) = 0$$</p><p>于是，依次对$\delta_i$求解上述方程，最终可以得到$\delta$。</p><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>输入：特征函数$f_1,f_2,…,f_n;$经验分布$\hat{P}(X,Y)$，模型$P_w(y|x)$</p><p>输出：最优参数值$w_i^*$；最优模型$P_{w^*}$</p><p>(1) 对所有$i\in \{1,2,…,n\}$，取初值$w_i=0$</p><p>(2) 对每一$i\in \{1,2,…,n\}$ :</p><p>​    (a) 令$\delta_i$是方程</p><p>$$E_\hat{p}(f_i)=\sum_x\tilde{P}(x)\sum_yP_w(y|x)f_i(x,y)exp(\delta_if^\#(x,y))$$</p><p>的解，其中</p><p>$$f^\#(x,y) = \sum_{i=1}^nf_i(x,y)$$</p><p>​    (b) 更新$w_i$的值：$w_i \leftarrow w_i + \delta_i$</p><p>(3) 如果不是所有$w_i$都收敛，重复步骤(2)</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” <em>清华大学出版社, 北京</em> (2012).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;逻辑斯蒂回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。从最优化的观点看，这时的目标函数具有很好的性质。它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法。&lt;/
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Optimization" scheme="http://conghuai.me/categories/Machine-Learning/Optimization/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
      <category term="optimization" scheme="http://conghuai.me/tags/optimization/"/>
    
  </entry>
  
  <entry>
    <title>Expectation Maximization</title>
    <link href="http://conghuai.me/2018/04/18/Expectation-Maximization/"/>
    <id>http://conghuai.me/2018/04/18/Expectation-Maximization/</id>
    <published>2018-04-18T09:07:27.000Z</published>
    <updated>2018-04-18T10:00:36.865Z</updated>
    
    <content type="html"><![CDATA[<p>概率模型有时既含有观测变量，又含有隐变量或潜在变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。</p><h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><p>我们面对一个含有隐变量的概率模型，目标是最大化观测数据（不完全数据）$Y​$关于参数$\theta​$的对数似然函数，即极大化</p><p>$$\begin{align} L(\theta) &amp;= logP(Y|\theta)=log\sum_ZP(Y,Z|\theta) \\  &amp;= log(\sum_ZP(Y|Z,\theta)P(Z|\theta)) \end{align}$$</p><p>注意到这一极大化的主要困难是上述中有未观测数据并有包含和（或积分）的对数。</p><p>事实上，EM算法是通过迭代逐步近似极大化$L(\theta)$的，假设在第$i$次迭代后$\theta$的估计值是$\theta^{(i)}$，我们希望新的估计值$\theta$能使$L(\theta)$增加，即$L(\theta)&gt;L(\theta^{(i)})$，并逐步达到最大值，为此，考虑两者的差</p><p>$$L(\theta)-L(\theta^{(i)})=log(\sum_Z P(Y|Z,\theta)P(Z|\theta))-logP(Y|\theta^{(i)})$$</p><p>利用Jensen不等式，得到其下界</p><p>$$\begin{align} L(\theta) - L(\theta^{(i)}) &amp;= log(\sum_Z P(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})})-logP(Y|\theta^{(i)})\\  &amp;\geq \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-logP(Y|\theta^{(i)}) \\  &amp;= \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-\sum_ZP(Z|Y,\theta^{(i)})logP(Y|\theta^{(i)})  \\ &amp;= \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})} \end{align}$$</p><p>我们令</p><p>$\beta(\theta,\theta^{(i)})=L(\theta^{(i)}) + \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$</p><p>则</p><p>$$L(\theta)\geq \beta(\theta,\theta^{(i)})$$</p><p>因此，任何可以使$\beta(\theta,\theta^{(i)})$增大的$\theta$，也可以使$L(\theta)$增大，为了使$L(\theta)$有尽可能大的增长，选择$\theta^{(i+1)}$使$\beta(\theta, \theta^{(i)})$达到极大，即</p><p>$$\theta^{(i+1)}=argmax_\theta\beta(\theta,\theta^{(i)})$$</p><p>现在求$\theta^{(i+1)}$的表达式，省去对$\theta$的极大化而言是常数的项，有</p><p>$$\begin{align} \theta^{(i+1)} &amp;= argmax_\theta(L(\theta^{(i)}) + \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}) \\  &amp;= argmax_\theta \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}) \\  &amp;= argmax_\theta \sum_ZP(Z|Y,\theta^{(i)})logP(Y|Z,\theta)P(Z|\theta)-\sum_ZP(Z|Y,\theta^{(i)})P(Z|Y,\theta^{(i)})P(Y|\theta^{(i))})  \\ &amp;= argmax_\theta \sum_ZP(Z|Y,\theta^{(i)})logP(Y|Z,\theta)P(Z|\theta) \\ &amp;= argmax_\theta \sum_ZP(Z|Y,\theta^{(i)})logP(Y,Z|\theta) \\ &amp;= argmax_\theta\ Q(\theta,\theta^{(i)}) \end{align}$$</p><p>上述等价于EM算法的一次迭代，即求Q函数及其极大化。EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-095839.jpg" alt="Screen Shot 2018-04-18 at 17.52.20"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” <em>清华大学出版社, 北京</em> (2012).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;概率模型有时既含有观测变量，又含有隐变量或潜在变量。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Optimization" scheme="http://conghuai.me/categories/Machine-Learning/Optimization/"/>
    
      <category term="Bayesian" scheme="http://conghuai.me/categories/Machine-Learning/Optimization/Bayesian/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
      <category term="optimization" scheme="http://conghuai.me/tags/optimization/"/>
    
  </entry>
  
  <entry>
    <title>Optimization for Machine Learning</title>
    <link href="http://conghuai.me/2018/04/18/Optimization-for-Machine-Learning/"/>
    <id>http://conghuai.me/2018/04/18/Optimization-for-Machine-Learning/</id>
    <published>2018-04-18T01:47:44.000Z</published>
    <updated>2018-04-18T09:08:54.198Z</updated>
    
    <content type="html"><![CDATA[<p>优化问题在很多地方都会出现，如机器学习、数据挖掘、统计学等。最简单的优化问题的形式如下：</p><p>$$\begin{align} minimize\ \ f(x) \  with\ x\in R^d \ \end{align}$$</p><p>其中</p><ul><li>$x$是变量；</li><li>$f$是目标函数$f:R^d\rightarrow R$；</li></ul><ul><li>通常假设$f$是连续可导的。</li></ul><p>绝大部分的机器学习问题，最后都会转换为优化问题，例如：</p><ul><li><strong>Soft Linear SVM</strong></li></ul><p>$$argmin_w \sum_{i=1}^n||w||^2+C\sum_{i=1}^n\varepsilon _i$$</p><p>$$s.t. 1-y_ix_i^Tw \leq \varepsilon _i,\varepsilon \geq 0$$</p><ul><li><strong>Maximum Likelihood</strong></li></ul><p>$$argmax_\theta \sum_{i=1}^n logp_\theta(x_i)$$</p><ul><li><strong>K-means</strong></li></ul><p>$$argmin_{u_1,u_2,…,u_k}J(\mu)=\sum_{j=1}^k\sum_{i\in C_j}||x_i-\mu_j||^2$$</p><p>在机器学习中，通常我们需要先定义并建立优化的目标函数，然后用一个时间上较优的算法去求解该优化问题。目前，主要的优化方法有：</p><ul><li>Gradient Descent</li><li>Stochastic Gradient Descent(SGD)</li><li>Coordinate Descent</li></ul><p>这些优化方法的提出，其历史如下：</p><ul><li>1847: Cauchy proposes gradient descent</li><li>1950s: Linear Programs, soon followed by non-linear, SGD</li><li>1980s: General optimization, convergence theory</li><li>2005-today: Large scale optimization, convergence of SGD</li></ul><p>Coordinate Descent例子如下：</p><p>$$Goal : Find\ x^*\in R\ minimizing\ f(x)\ \ (Example: d= 2) $$</p><p><strong>Idea</strong>：Update one coordinate at a time, while keeping others fixed.</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090851.jpg" alt="Screen Shot 2018-04-18 at 10.19.46"></p><h1 id="凸优化理论"><a href="#凸优化理论" class="headerlink" title="凸优化理论"></a>凸优化理论</h1><h2 id="Convex-Sets"><a href="#Convex-Sets" class="headerlink" title="Convex Sets"></a>Convex Sets</h2><blockquote><p>A set C is <strong>convex</strong> if the line segment between any two points of C lies in C, i.e., if for any x,y $\in$ C and any $\lambda $ with $0 \leq \lambda\leq 1$,we have</p><p>$$\lambda x+(1-\lambda)y \in C$$</p></blockquote><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090842.jpg" alt="Screen Shot 2018-04-18 at 10.24.07"></p><h2 id="Convex-Functions"><a href="#Convex-Functions" class="headerlink" title="Convex Functions"></a>Convex Functions</h2><blockquote><p>Definition</p><p>A function $f: R^d \rightarrow R$ is convex if (i) dom(f) is a convex set and (ii) for all $x,y \in dom(f)$,and $\lambda$ with $0\leq \lambda \leq 1$, we have </p><p>$$f(\lambda x+(1-\lambda)y)\leq \lambda f(x)+(1-\lambda)f(y)$$</p></blockquote><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090846.jpg" alt="Screen Shot 2018-04-18 at 10.53.05"></p><p>常见的凸函数有：</p><h3 id="Linear-affine-functions"><a href="#Linear-affine-functions" class="headerlink" title="Linear/affine functions"></a>Linear/affine functions</h3><p>$$f(x)=b^Tx+c$$</p><h3 id="Quadratic-functions"><a href="#Quadratic-functions" class="headerlink" title="Quadratic functions"></a>Quadratic functions</h3><p>$$f(x)=\frac{1}{2}x^TAx+b^Tx+c$$</p><h3 id="Norms-如l1、l2范数"><a href="#Norms-如l1、l2范数" class="headerlink" title="Norms(如l1、l2范数)"></a>Norms(如l1、l2范数)</h3><p>$$||\alpha x+(1-\alpha)y||\leq ||\alpha x||+||(1-\alpha)y||=\alpha ||x||+(1-\alpha)||y||$$</p><h3 id="Composition-with-an-affine-function-f-Ax-b"><a href="#Composition-with-an-affine-function-f-Ax-b" class="headerlink" title="Composition with an affine function $f(Ax+b)$"></a>Composition with an affine function $f(Ax+b)$</h3><p>$$f(A(\alpha x+(1-\alpha)y)+b) =f(\alpha (Ax+b)+(1-\alpha)(Ay+b))\leq \alpha f(Ax+b)+(1-\alpha)f(Ay+b)$$</p><h3 id="Log-sum-exp"><a href="#Log-sum-exp" class="headerlink" title="Log-sum-exp"></a>Log-sum-exp</h3><p>$$f(x) = log(\sum_{i=1}^nexp(x_i))$$</p><p>在机器学习中， 有哪些损失函数是凸函数呢？</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090845.jpg" alt="Screen Shot 2018-04-18 at 11.12.36"></p><h3 id="SVM-loss"><a href="#SVM-loss" class="headerlink" title="SVM loss"></a>SVM loss</h3><p>$$f(w) = [1-y_ix_i^Tw]_+$$</p><h3 id="Binary-logistic-loss"><a href="#Binary-logistic-loss" class="headerlink" title="Binary logistic loss"></a>Binary logistic loss</h3><p>$$f(w)=log(1+exp(-y_ix_i^Tw))$$</p><h2 id="Convex-Optimization"><a href="#Convex-Optimization" class="headerlink" title="Convex Optimization"></a>Convex Optimization</h2><blockquote><p>An optimization problem is convex if its objective is a convex function, the inequality constrints $f_j$ are convex, and the equality constraints $h_j$ are affine</p></blockquote><p>$$minimize_x f_0(x)\ (Convex function)$$</p><p>$$s.t. f_i(x) \leq 0\ (Convex\ sets)$$</p><p>$$h_j(x) = 0\ (Affine)$$</p><p>凸优化的性质在于具有全局最优解。</p><h1 id="Unconstrained-optimization"><a href="#Unconstrained-optimization" class="headerlink" title="Unconstrained optimization"></a>Unconstrained optimization</h1><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>$$For\ t=1,…,T$$</p><p>$$x_{t+1}\leftarrow x_t-\eta \triangledown f(x_t)$$</p><p>其中，$\eta_t$称为学习率。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090850.jpg" alt="Screen Shot 2018-04-18 at 16.45.00"></p><h3 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient descent"></a>Batch gradient descent</h3><p>在整个训练数据集上计算梯度，然后更新参数：</p><p>$$\theta=\theta-\eta\cdot \triangledown _\theta J(\theta)$$</p><p>因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">    params_grad = evaluate_gradient(loss_function, data, params)</span><br><span class="line">    params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><p>对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。</p><h3 id="Stochastic-Gradient-Descent（SGD）"><a href="#Stochastic-Gradient-Descent（SGD）" class="headerlink" title="Stochastic Gradient Descent（SGD）"></a>Stochastic Gradient Descent（SGD）</h3><p>相反，随机梯度下降法（stochastic gradient descent, SGD）根据每一条训练样本$x^{(i)}$和标签$y^{(i)}$更新参数：</p><p>$$\theta = \theta-\eta\cdot \triangledown_\theta J(\theta;x^{(i)};y^{(i)}) $$</p><p>对于大数据集，因为批梯度下降法在每一个参数更新之前，会对相似的样本计算梯度，所以在计算过程中会有冗余。而SGD在每一次更新中只执行一次，从而消除了冗余。因而，通常SGD的运行速度更快，同时，可以用于在线学习。SGD以高方差频繁地更新，导致目标函数出现如图1所示的剧烈波动。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-18-090848.jpg" alt="20170410195019493"></p><p>与批梯度下降法的收敛会使得损失函数陷入局部最小相比，由于SGD的波动性，一方面，波动性使得SGD可以跳到新的和潜在更好的局部最优。另一方面，这使得最终收敛到特定最小值的过程变得复杂，因为SGD会一直持续波动。然而，已经证明当我们缓慢减小学习率，SGD与批梯度下降法具有相同的收敛行为，对于非凸优化和凸优化，可以分别收敛到局部最小值和全局最小值。与批梯度下降的代码相比，SGD的代码片段仅仅是在对训练样本的遍历和利用每一条样本计算梯度的过程中增加一层循环。注意，如6.1节中的解释，在每一次循环中，我们打乱训练样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(nb_epochs):</span><br><span class="line">    np.random.shuffle(data)</span><br><span class="line">    <span class="keyword">for</span> example <span class="keyword">in</span> data:</span><br><span class="line">        params_grad = evaluate_gradient(loss_function, example, params)</span><br><span class="line">        params = params - learning_rate * params_grad</span><br></pre></td></tr></table></figure><h2 id="Newton’s-method"><a href="#Newton’s-method" class="headerlink" title="Newton’s method"></a>Newton’s method</h2><h1 id="Constrained-optimization"><a href="#Constrained-optimization" class="headerlink" title="Constrained optimization"></a>Constrained optimization</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;优化问题在很多地方都会出现，如机器学习、数据挖掘、统计学等。最简单的优化问题的形式如下：&lt;/p&gt;
&lt;p&gt;$$\begin{align} minimize\ \ f(x) \  with\ x\in R^d \ \end{align}$$&lt;/p&gt;
&lt;p&gt;其中&lt;/p&gt;
&lt;ul&gt;
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Optimization" scheme="http://conghuai.me/categories/Machine-Learning/Optimization/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Maximum Entropy Model</title>
    <link href="http://conghuai.me/2018/04/16/Maximum-Entropy-Model/"/>
    <id>http://conghuai.me/2018/04/16/Maximum-Entropy-Model/</id>
    <published>2018-04-16T07:56:56.000Z</published>
    <updated>2018-04-17T04:09:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，熵最大的模型是最好的模型，通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。</p><p>直观地，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息的情况下，那些不确定的部分都是“等可能的”。</p><p>最大熵原理是统计学习的一个一般原理，将它应用到分类得到最大熵模型。我们先从一个例子说明一下最大熵模型的特点：</p><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p>假设我们的训练样本中有10个样本，每个样本中有两个变量$a、b$，其中$a\in \{x, y\}$，$b\in \{0,1\}$，观测到的训练样本如下：</p><table><thead><tr><th style="text-align:center">a,b</th></tr></thead><tbody><tr><td style="text-align:center">(x, 1)</td></tr><tr><td style="text-align:center">(y, 1)</td></tr><tr><td style="text-align:center">(x, 0)</td></tr><tr><td style="text-align:center">(y ,1)</td></tr><tr><td style="text-align:center">(y, 0)</td></tr><tr><td style="text-align:center">(x, 0)</td></tr><tr><td style="text-align:center">(x, 0)</td></tr><tr><td style="text-align:center">(x, 1)</td></tr><tr><td style="text-align:center">(y, 0)</td></tr><tr><td style="text-align:center">(x, 0)</td></tr></tbody></table><p>用二维表刻画联合概率分布来表示：</p><table><thead><tr><th style="text-align:center">a\b</th><th style="text-align:center">0</th><th style="text-align:center">1</th></tr></thead><tbody><tr><td style="text-align:center">x</td><td style="text-align:center">0.4</td><td style="text-align:center">0.2</td></tr><tr><td style="text-align:center">y</td><td style="text-align:center">0.2</td><td style="text-align:center">0.2</td></tr></tbody></table><p>上面这个表刻画的就是我们在训练样本上的概率分布，对于这个概率分布，我们是绝对相信吗？如果是的话，我们会用极大似然的方式来估计出模型的参数。但是，在有些场景下，对于通过训练样本得到的经验概率分布，我们有时候只关心某些特征，我们希望得到的模型对于我们关心的特征满足经验概率分布，对于我们不关心的特征，只需要达到最大熵要求即可。但是，该如何刻画我们的特征呢？</p><p>通常来说，我们通过定义特征函数来刻画我们关心的特征：</p><p>$$\begin{split}f(a,b)=\begin{cases} 1, &amp; \text{我们关心的条件} \\ 0, &amp; \text{其他}\end{cases}\end{split}$$</p><p>可以看出来，根据不同的条件，我们可以定义出多个不同的特征函数：$f_j:\varepsilon\rightarrow \{0,1\} $。例如，我们关心的是<code>b=0</code>这一列的特征，我们就可以定义特征函数为：</p><p>$$\begin{split}f(a,b)=\begin{cases} 1, &amp; \text{b=0} \\ 0, &amp; \text{其他}\end{cases}\end{split}$$</p><p>有了特征函数的定义，我们现在需要保证模型在特征函数上的期望值$E_pf$和经验概率分布在特征函数上的期望值$E_{\tilde{p}}f$是一致的，即：</p><p>$$E_{\tilde{p}}f = E_pf$$</p><p>根据我们上面定义的特征函数，得：</p><p>$$E_{\tilde{p}}f = \sum_{a,b}\tilde{p}(a,b)f(a,b)= 0.4 + 0.2 = 0.6$$</p><p>即，我们得到经验概率分布在该特征函数上的期望值为：0.6，所以，我们的模型需满足：$E_pf=p(x,0) + p(y,0) = p(x)\cdot p(0|x) + p(y)\cdot p(0|y)= 0.6$</p><p>那么，对于我们不关心的特征呢？对于这些特征，我们只需要保证熵最大即可。</p><h1 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h1><p>有了上面的例子，现在我们形式化给出最大熵模型的定义：</p><p>假设分类模型是一个条件概率分布$P(Y|X)$，$x\in X \subseteq R^n$表示输入，$y\in Y$表示输出，$X$和$Y$分别是输入和输出的集合，这个模型表示的是对于给定的输入$X$，以条件概率$P(Y|X)$输出$Y$。</p><p>首先考虑模型应该满足的条件，给定训练数据集，可以确定联合分布$P(X,Y)$的经验分布和边缘分布$P(X)$的经验分布，分别以$\tilde{P}(X,Y)$和$\tilde{P}(X)$表示。这里，</p><p>$$\tilde{P}(X=x,Y=y)=\frac{v(X=x,Y=y)}{N}$$</p><p>$$\tilde{P}(X=x)=\frac{v(X=x)}{N}$$</p><p>其中，$v(X=x, Y=y)$表示训练数据中样本$(x,y)$出现的频数，$v(X=x)$表示训练数据中输入$x$出现的频数，$N$表示训练样本容量。</p><p>用特征函数$f(x,y)$描述输入x和输出y之间的某一个事实，其定义是：</p><p>$$\begin{split}f(a,b)=\begin{cases} 1, &amp; \text{x与y满足某一事实} \\ 0, &amp; \text{否则}\end{cases}\end{split}$$</p><p>它是一个二值函数，当x和y满足这个事实时取值为1，否则取值为0。</p><p>特征函数$f(x,y)$关于经验分布$\tilde{P}(X,Y)$的期望值，用$E_\tilde{p}(f)$表示：</p><p>$$E_\tilde{p}(f) = \sum_{x,y}\tilde{P}(x,y)f(x,y)$$</p><p>特征函数$f(x,y)$关于模型$P(Y|X)$与经验分布$\tilde{P}(X)$的期望值，用$E_p(f)$表示：</p><p>$$E_p(f)=\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)$$</p><p>如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即：</p><p>$$E_p(f) = E_\tilde{p}(f)$$</p><p>或</p><p>$$\sum_{x,y}\tilde{P}(x,y)f(x,y) = \sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)$$</p><p>上式就是模型的约束条件，如果有n个特征函数$f_i(x,y),i=1,2,…,n$，那么就有n个约束条件。</p><h2 id="定义最大熵模型"><a href="#定义最大熵模型" class="headerlink" title="定义最大熵模型"></a>定义最大熵模型</h2><p>假设满足所有约束条件的模型集合为：</p><p>$$C\equiv \{p\in P|E_p(f_i)=E_{\tilde{p}}(f_i),i=1,2,…,n\}$$</p><p>定义在条件概率分布$P(Y|X)$上的条件熵为：</p><p>$$H(P) = -\sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x)$$</p><p>则模型集合$C$中条件熵$H(P)$最大的模型称为最大熵模型。</p><h1 id="最大熵模型的学习"><a href="#最大熵模型的学习" class="headerlink" title="最大熵模型的学习"></a>最大熵模型的学习</h1><p>对于给定的训练数据集$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$以及特征函数$f_i(x,y)$，$i=1,2,…,n$，最大熵模型的学习等价于约束最优化问题：</p><p>$$max_{P\in C}\ H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x)$$</p><p>$$s.t. E_p(f) = E_\tilde{p}(f), i=1,2,…,n$$</p><p>$$\sum_yP(y|x)=1$$</p><p>改写成等价的求最小值的问题：</p><p>$$min_{P\in C}\ -H(P)=\sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x)$$</p><p>$$s.t. E_p(f) = E_\tilde{p}(f), i=1,2,…,n$$</p><p>$$\sum_yP(y|x)=1$$</p><h2 id="无约束最优化的对偶问题"><a href="#无约束最优化的对偶问题" class="headerlink" title="无约束最优化的对偶问题"></a>无约束最优化的对偶问题</h2><p>将约束最优化的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题求解原始问题。首先，引进拉格朗日乘子$w_0,w_1,w_2,…,w_n$，定义拉格朗日函数$L(P,w)$:</p><p>$$\begin{align} L(P,w) &amp;=-H(P) + w_0(1-\sum_yP(y|x)) + \sum_{i=1}^nw_i(E_\tilde{p}(f)-E_p(f))\\  &amp;= \sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x) \\ &amp;+ w_0(1-\sum_yP(y|x))+ \sum_{i=1}^nw_i(\sum_{x,y}\tilde{P}(x,y)f(x,y)-\sum_{x,y}\tilde{P}(x)P(y|x)f(x,y)) \end{align}$$</p><p>最优化的原始问题是：</p><p>$$min_{P\in C}max_wL(P,w)$$</p><p>对偶问题是：</p><p>$$max_wmin_{P\in C}L(P,w)$$</p><p>由于拉格朗日函数$L(P,w)$是$P$的凸函数，原始问题和对偶问题的解是等价的。</p><h3 id="求解-min-P-in-C-L-P-w"><a href="#求解-min-P-in-C-L-P-w" class="headerlink" title="求解$min_{P\in C}L(P,w)$"></a>求解$min_{P\in C}L(P,w)$</h3><p>$min_{P\in C}L(P,w)$是$w$的函数，将其记作：</p><p>$$\psi (x)=min_{P\in C}L(P,w)=L(P_w,w)$$</p><p>$\psi (x)$称为对偶函数，同时将其解记作：</p><p>$$P_w=argmin_{p\in C}L(P,w)=P_w(y|x)$$</p><p>具体地，求$L(P,w)$对$P(y|x)$的偏导数</p><p>$$\begin{align} \frac{\partial{L(P,w)}}{\partial{P(y|x)}} &amp;=\sum_{x,y}\tilde{P}(x)(logP(y|x)+1)-\sum_yw_0-\sum_{x,y}(\tilde{P}(x)\sum_{i=1}^nw_if_i(x,y))\\  &amp;= \sum_{x,y}\tilde{P}(x)(logP(y|x)+1-w_0-\sum_{i=1}^nw_if_i(x,y)) \end{align}$$</p><p>令偏导数等于0，在$\tilde{P}&gt;0$的情况下，解得：</p><p>$$\begin{align} P(y|x) &amp;=e^{\sum_{i=1}^ww_if_i(x,y)+w_0-1}\\  &amp;= \frac{e^{\sum_{i=1}^ww_if_i(x,y)}}{e^{1-w_0}} \\  &amp;= \frac{e^{\sum_{i=1}^ww_if_i(x,y)}}{Z(x)} \end{align}$$</p><p>由于$\sum_yP(y|x)=1$，得</p><p>$$Z_w(x)=e^{1-w_0}=\sum_ye^{\sum_{i=1}^nw_if_i(x,y)}$$</p><p>其中：</p><ul><li>$Z_w(x)$称为规划化因子；</li><li>$f_i(x,y)$是特征函数；</li><li>$w_i$是特征的权值；</li><li>$w$是最大熵模型中的参数向量。</li></ul><h3 id="求解-max-w-psi-x"><a href="#求解-max-w-psi-x" class="headerlink" title="求解$max_w\psi (x)$"></a>求解$max_w\psi (x)$</h3><p>之后，求解对偶问题外部的极大化问题</p><p>$$max_w\psi (x)$$</p><p>将其解记为$w^*$，即</p><p>$$w^*=argmax_w\psi (x)$$</p><p>这就是说，可以应用最优化算法求对偶函数$\psi (x)$的极大化，得到$w^*$，用来表示$P^*$，这里，$P^*=P_{w^*}=P_{w^*}(y|x)$是学习到的最优化模型（最大熵模型）。</p><h1 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h1><p>下面证明对偶函数的极大化等价于最大熵模型的极大似然估计。</p><h2 id="对数似然函数"><a href="#对数似然函数" class="headerlink" title="对数似然函数"></a>对数似然函数</h2><p>已知训练数据的经验概率分布$\tilde{P}(X,Y)$，条件概率分布$P(Y|X)$的对数似然函数表示为：</p><p>$$L_{\tilde{P}}(P_w)=log\prod_{x,y}P(y|x)^{\tilde{P}(x,y)}=\sum_{x,y}\tilde{P}(x,y)logP(y|x)$$</p><p>当条件概率分布$P(y|x)$是最大熵模型时，对数似然函数$L_{\tilde{p}}(P_w)$为：</p><p>$$\begin{align}L_{\tilde{p}}(P_w) &amp;=\sum_{x,y}\tilde{P}(x,y)logP(y|x) \\  &amp;=\sum_{x,y}\tilde{P}(x,y)log[\frac{e^{\sum_{i=1}^nw_if_i(x,y)}}{Z_w(x)}]\\  &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)-\sum_x\tilde{P}(x)logZ_w(x) \end{align}$$</p><h2 id="对偶函数"><a href="#对偶函数" class="headerlink" title="对偶函数"></a>对偶函数</h2><p>$$\begin{align}\psi (x) &amp;=\sum_{x,y}\tilde{P}(x)P_w(y|x)logP(y|x)+\sum_{i=1}^nw_i(\sum_{x,y}\tilde{P}(x,y)f_i(x,y)-\sum_{x,y}\tilde{P}(x)P_w(y|x)f_i(x,y)) \\  &amp;=\sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)+\sum_{x,y}\tilde{P}(x)P_w(y|x)(logP_w(y|x)-\sum_{i=1}^nw_if_i(x,y))\\  &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)+\sum_{x,y}\tilde{P}(x)P_w(y|x)logZ_w(x)\\  &amp;= \sum_{x,y}\tilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y)+\sum_{x}\tilde{P}(x)logZ_w(x) \end{align}$$</p><p>我们发现，</p><p>$$\psi (x) = L_{\tilde{p}}(P_w)$$</p><p>既然对偶函数等价于对数似然函数，于是证明了最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计。</p><h1 id="改进的迭代尺度法"><a href="#改进的迭代尺度法" class="headerlink" title="改进的迭代尺度法"></a>改进的迭代尺度法</h1><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-16-125513.jpg" alt="Screen Shot 2018-04-16 at 20.53.37"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-16-125507.jpg" alt="Screen Shot 2018-04-16 at 20.54.06"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-16-125510.jpg" alt="Screen Shot 2018-04-16 at 20.54.44"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” 清华大学出版社, 北京 (2012).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型(分布)中，熵最大的模型是最好的模型，通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。&lt;/p&gt;
&lt;p&gt;直观地，最大熵原理认为要
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Classification" scheme="http://conghuai.me/categories/Machine-Learning/Classification/"/>
    
    
      <category term="algorithm" scheme="http://conghuai.me/tags/algorithm/"/>
    
      <category term="entropy" scheme="http://conghuai.me/tags/entropy/"/>
    
  </entry>
  
  <entry>
    <title>Mean Squared Error</title>
    <link href="http://conghuai.me/2018/04/14/Mean-Squared-Error/"/>
    <id>http://conghuai.me/2018/04/14/Mean-Squared-Error/</id>
    <published>2018-04-14T09:10:01.000Z</published>
    <updated>2018-04-15T11:19:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>数理统计中均方误差是指参数估计值与参数值之差平方的期望值，记为MSE。MSE是衡量“平均误差”的一种较方便的方法，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。</p><h1 id="SSE（和方差）"><a href="#SSE（和方差）" class="headerlink" title="SSE（和方差）"></a>SSE（和方差）</h1><p>在统计学中，该参数计算的是拟合数据和原始对应点的误差的平方和，计算公式为：</p><p>$$SSE =\sum_{i=1}^mw_i(y_i-\hat{y_i})^2$$</p><p>其中$y_i$是真实数据，$\hat{y_i}$是拟合的数据，$w_i&gt;0$，从这里可以看出SSE接近于0，说明模型选择和拟合更好，数据预测也越成功。</p><h1 id="MSE（均方方差）"><a href="#MSE（均方方差）" class="headerlink" title="MSE（均方方差）"></a>MSE（均方方差）</h1><p>该统计参数是预测数据和原始数据对应点误差的平方和的均值，也就是$\frac{SSE}{n}$，和SSE没有太大的区别，计算公式为：</p><p>$$MSE=\frac{SSE}{n}=\frac{1}{n}\sum_{i=1}^mw_i(y_i-\hat{y_i})^2$$</p><p>其中，n为样本的个数。</p><h1 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h1><p>该统计参数，也叫回归系统的拟合标准差，是MSE的平方根，计算公式为：</p><p>$$RMSE=\sqrt{MSE}=\sqrt{\frac{SSE}{n}}=\sqrt{\frac{1}{n}\sum_{i=1}^mw_i(y_i-\hat{y_i})^2}$$</p><h1 id="Mean-Squared-Loss的概率解释"><a href="#Mean-Squared-Loss的概率解释" class="headerlink" title="Mean-Squared Loss的概率解释"></a>Mean-Squared Loss的概率解释</h1><p>假设我们的模型是二维平面的线性回归模型：$h_{\theta}(x_i)=\theta_0+\theta_1x$，对于这个模型，我们定义损失函数为MSE，将得到如下的表达式：</p><p>$$J = \frac{1}{N}\sum_{i=1}^N(y_i-h_{\theta}(x_i))^2$$</p><p>下面我们试着通过概率的角度，推导出上述的MSE损失函数表达式。</p><p>在线性回归模型中，我们最终希望对于输入$X$进行线性组合得到值Y，考虑到输入带有噪声的情况的表达式如下：</p><p>$$Y=\theta_0+\theta_1x+\eta$$</p><p>为了使模型更合理，我们假设$\eta$服从均值为0，方差为1的高斯分布，即$\eta\sim N(0,1)$。所以有：</p><p>$$E[Y]=E[\theta_0+\theta_1x+\eta]=\theta_0+\theta_1x$$</p><p>$$Var[Y]=Var[\theta_0+\theta_1x+\eta]=1$$</p><p>所以，Y服从均值为$\theta_0+\theta_1x$，方差为1的高斯分布，则样本点$(x_i,y_i)$的概率为：</p><p>$$p(y_i|x_i)=e^{-\frac{(y_i-(\theta_0+\theta_1x_i))^2}{2}}$$</p><p>有了单个样本的概率，我们就可以计算样本集的似然概率，我们假设每个样本是独立的：</p><p>$$L(x,y)=\prod_{i=1}^Ne^{-\frac{(y_i-(\theta_0+\theta_1x_i))^2}{2}}$$</p><p>对似然函数取对数，得到对数似然函数：</p><p>$$l(x,y)=-\frac{1}{2}\sum_{i=1}^N(y_i-(\theta_0+\theta_1x_i))^2$$</p><p>这个对数似然函数的形式和我们的MSE损失函数的定义是一样的。所以，使用MSE损失函数意味着，我们假设我们的模型是对噪声的输入做估计，该噪声服从高斯分布。</p><h1 id="损失函数效果"><a href="#损失函数效果" class="headerlink" title="损失函数效果"></a>损失函数效果</h1><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>使用MSE的一个缺点就是其偏导值在输出概率值接近0或者接近1的时候非常小，这可能会造成模型刚开始训练时，偏导值几乎消失。</p><p>假设我们的MSE损失函数为：$J = \frac{1}{2}(y_i - \hat{y_i})^2$，偏导为：$\frac{dJ}{dW} = (y_i - \hat{y_i})\sigma’(Wx_i + b)x_i$，其中$\sigma’(Wx_i + b)$为$\sigma(Wx_i + b)(1 - \sigma(Wx_i + b))$。可以看出来，在$\sigma(Wx_i + b)$值接近0或者1的时候，$\frac{dJ}{dW}$的值都会接近于0，其函数图像如下：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-141825.jpg" alt="sigmoid_and_derivative_plot"></p><p>这导致模型在一开始学习的时候速率非常慢，而使用交叉熵作为损失函数则不会导致这样的情况发生。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="http://rohanvarma.me/Loss-Functions/" target="_blank" rel="noopener">Picking Loss Functions - A comparison between MSE, Cross Entropy, and Hinge Loss</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;数理统计中均方误差是指参数估计值与参数值之差平方的期望值，记为MSE。MSE是衡量“平均误差”的一种较方便的方法，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。&lt;/p&gt;
&lt;h1 id=&quot;SSE（和方差）&quot;&gt;&lt;a href=&quot;#SSE
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Loss Function" scheme="http://conghuai.me/categories/Machine-Learning/Loss-Function/"/>
    
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Regularization</title>
    <link href="http://conghuai.me/2018/04/12/Regularization/"/>
    <id>http://conghuai.me/2018/04/12/Regularization/</id>
    <published>2018-04-12T14:37:46.000Z</published>
    <updated>2018-04-22T02:48:14.855Z</updated>
    
    <content type="html"><![CDATA[<h1 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h1><p>过拟合是机器学习中一个比较常见的问题，而正则化是解决模型过拟合的一种手段。我们先看一下范数的定义：</p><blockquote><p>Going a bit further, we define $||x||_p$ as a “p-norm”. Given $x$, a vector with $i$ components, a p-norm is defined as:</p><p>$$|| x ||_p = \left(\sum_i |x_i|^p\right)^{1/p}$$</p><p>The simplest norm conceptually is Euclidean distance. This is what we typically think of as distance between two points in space:</p><p>$$|| x ||_2 = \sqrt{\left(\sum_i x_i^2\right)} = \sqrt{x_1^2 + x_2^2 + \ldots + x_i^2}$$</p><p>Another common norm is taxicab distance, which is the 1-norm:</p><p>$$|| x ||_1 = \sum_i |x_i| = |x_1| + |x_2| + \ldots + |x_i|$$</p><p>Taxicab distance is so-called because it emulates moving between two points as though you are moving through the streets of Manhattan in a taxi cab. Instead of measuring the distance “as the crow flies” it measures the right-angle distance between two points:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-15-114416.jpg" alt="1200px-Manhattan_distance.svg"></p></blockquote><p>正则化（Regularization）是机器学习中一种常用的技术，其主要目的是控制模型复杂度，减小过拟合。最基本的正则化方法是在原目标（代价）函数 中添加惩罚项，对复杂度高的模型进行“惩罚”。其数学表达形式为：</p><p>$$\tilde{J}(w;X,y)=J(w;X,y)+\alpha \Omega (w)$$</p><p>式中：</p><ul><li>$X，y$为训练样本和相应标签；</li><li>$w$为权重系数向量；</li><li>$J()$为目标函数；</li><li>$\Omega(w)$为惩罚项，即模型“规模”的某种度量；</li><li>$\alpha$控制正则化强弱。</li></ul><p>不同的$\Omega$函数对权重$w$的最优解有不同的偏好，因此会产生不同的正则化效果，最常用的$\Omega$函数有两种，即$l_1$范数和$l_2$范数，称为$l_1$正则化和$l_2$正则化：</p><p>$$ l_1: \Omega (w) = || w ||_1 = \sum_{i=1}^k |w_i| $$</p><p>$$ l_2: \Omega (w) = || w ||_2 = \sqrt{\sum_{i=1}^k w_i^2} $$</p><p>带有L1正则化的回归模型通常被称为<strong>Lasso Regression</strong>，带有L2正则化的回归模型通常被称为<strong>Ridge Regression</strong>。</p><p>L1正则化和L2正则化主要的区别在于，L1正比于参数的绝对值，而L2正比于参数的平方。这导致了两种正则化方式会产生不同的效果。</p><h1 id="公式来源分析"><a href="#公式来源分析" class="headerlink" title="公式来源分析"></a>公式来源分析</h1><h2 id="基于约束条件的最优化"><a href="#基于约束条件的最优化" class="headerlink" title="基于约束条件的最优化"></a>基于约束条件的最优化</h2><p>对于模型权重系数$w$求解释通过最小化目标函数实现的，即求解：</p><p>$$min_wJ(w;X,y)$$</p><p>通常情况下，模型复杂度与系数$w$的个数成线性关系：即$w$数量越多，模型越复杂。因此，为了限制模型的复杂度，很自然的想法就是减少系数$w$的个数，即让$w$向量中一些元素为0或者说限制$w$中非零元素的数量。因此，我们可以在原优化问题中加入一个约束条件：</p><p>$$min_wJ(w;X,y),\ s.t. ||w||_0 \leq C$$</p><p>式中，$||\cdot||_0$范数表示向量中非零元素的个数，但由于该问题是一个NP问题，不易求解，为此我们可以放松一下约束条件，为了达到近似效果，我们不严格要求某些权重$w$为0，而是要求权重$w$应接近于0，即尽量小。从而可用$l_1、l_2$范数来近似$l_0$范数，即：</p><p>$$min_wJ(w;X,y),\ s.t. ||w||_1 \leq C$$ 或</p><p>$$min_wJ(w;X,y),\ s.t. ||w||_2^2 \leq C$$（为了后续方便处理，对$||w||_2$进行平方）</p><p>利用拉格朗日算子法，我们可将上述带约束条件的最优化问题转换为不带约束项的优化问题，构建拉格朗日函数：</p><p>$$min_wJ(w;X,y) + \alpha^*||w||_1$$或</p><p>$$min_wJ(w;X,y) + \alpha^*||w||_2^2$$</p><p>因此，我们得到了对$l_1、l_2$正则化的第一种理解：</p><ul><li>$l_1$正则化等价于在原优化目标函数中增加约束条件$||w||_1 \leq C$</li><li>$l_2$正则化等价于在原优化目标函数中增加约束条件$||w||_2^2 \leq C$</li></ul><h2 id="最大后验概率估计"><a href="#最大后验概率估计" class="headerlink" title="最大后验概率估计"></a>最大后验概率估计</h2><p>在最大似然估计中，假设权重$w$是未知的参数，从而求得对数似然函数：</p><p>$$l(w)=log[P(y|X;w)]=log[\prod_iP(y^i|x^i;w)]$$</p><p>通过假设$y^i$的不同概率分布，即可得到不同的模型，例如若假设$y^i\sim N(w^Tx^i, \sigma^2)$的高斯分布，则有：</p><p>$$l(w)=log[\prod_i\frac{1}{\sqrt {2\pi}\sigma}e^{-\frac{(y^i-w^Tx^i)^2}{2\sigma^2}}]=-\frac{1}{2\sigma^2}\sum_i(y_i-w^Tx^i)^2+C$$</p><p>可令$J(w;X,y) = -l(w)$</p><p>在最大后验概率估计中，则将权重$w$看做随机变量，也具有某种分布，从而有：</p><p>$p(w|X,y)=\frac{P(w,X,y)}{P(X,y)}=\frac{P(X,y|w)P(w)}{P(X,y)}\propto P(y|X,w)P(w)$</p><p>对上述取对数有：</p><p>$$MAP = logP(y|X,w)P(w) = logP(y|X,w)+logP(w)$$</p><p>可以看到，后验概率在似然函数的基础上增加一项$logP(w)$，$P(w)$的意义是对权重系数$w$的概率分布的先验假设， 在收集到训练样本${X,y}$后，则可根据$w$在${X,y}$下的后验概率对$w$进行修改正，从而做出对$w$更好地估计。</p><p>若假设$w_j$的先验分布是均值为0的高斯分布，即$w_j\sim N(0, \sigma^2)$，则有：</p><p>$$logP(w) = log\prod_jP(w_j)=log\prod_j[\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(w_j)^2}{2\sigma^2}}]=-\frac{1}{2\sigma^2}\sum_jw_j^2+C’$$</p><p>可以看到，在高斯分布下$logP(w)$的效果等价于在代价函数中增加$l_2$正则项。</p><p>若假设$w_j$服从均值为0、参数为$a$的拉普拉斯分布，即：</p><p>$P(w_j)=\frac{1}{\sqrt{2a}}e^{\frac{-|w_j|}{a}}$</p><p>则有：</p><p>$$logP(w) = log\prod_j\frac{1}{\sqrt{2a}}e^{\frac{-|w_j|}{a}}=-\frac{1}{a}\sum_j|w_j|+C’$$</p><p>可以看到，在拉普拉斯分布下$logP(w)$的效果等价于在代价函数中增加$l_1$正则化。</p><p>因此，我们得到对于$l_1、l_2$正则化的第二种理解：</p><ul><li>$l_1$正则化可通过假设权重$w$的先验分布为拉普拉斯分布，由最大后验概率估计导出；</li><li>$l_2$正则化可通过假设权重$w$的先验分布为高斯分布，由最大后验概率估计导出。</li></ul><h1 id="正则化效果理解"><a href="#正则化效果理解" class="headerlink" title="正则化效果理解"></a>正则化效果理解</h1><h2 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h2><p>有了正则化公式来源分析，我们现在从优化的角度来看一下正则化对目标函数的影响。考虑带约束条件的优化解释，对$l_2$正则化为：</p><p>$$min_w J(w;X,y)\ s.t. ||w||_2 \leq C$$</p><p>该问题的求解示意图如下：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-031232.jpg" alt="v2-7431d8a79deec5d0ab3193b6a3611b95_hd"></p><p>图中椭圆为原目标函数$J(w)$的一条等高线，圆为半径$\sqrt{C}$的$l_2$范数球。由于约束条件的限制，$w$必须位于$l_2$范数球内。考虑边界上的一点$w$，图中蓝色箭头为$J(w)$在该处的梯度方向$\triangledown J(w)$，红色箭头为$l_2$范数球在该处的法线方向。由于$w$不能离开边界（否则就会违反约束条件），因为在使用梯度下降法更新$w$时，只能朝$\triangledown J(w)$在范数球上$w$处的切线方向更新，即图中的绿色箭头的方向。如此$w$将沿着边界移动，当$\triangledown J(w)$与范数球上$w$处的切线方向更新，即图中绿色箭头的方向。如果$w$将沿着边界移动，当$\triangledown J(w)$与范数球上$w$处的法线平行时，此时，$\triangledown J(w)$在切线方向的分量为0，$w$将无法继续移动，从而达到最优解$w^*$（图中红色点所示）。</p><p>对于$l_1$正则化：</p><p>$$min_w J(w;X,y)\ s.t. ||w||_1 \leq C$$</p><p>同理，其求解示意图如下所示：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-031238.jpg" alt="v2-592216faffaa338fc792430a538afefc_hd"></p><p>其主要差别在于$l1、l_2$范数球的形状差异。由于此时每条边界上$w$的切线和法线方向保持不变，在图中$w$一直朝着$\triangledown J(w)$的切线方向的分量沿着边界向左上移动。当$w$跨过顶点到达$w’$处时，$\triangledown J(w)$在切线方向的分量变为右上方，因而$w$将朝右上方移动。最终，$w$将稳定在顶点处，达到最优解$w^*$，此时，可以看到$w_1=0$，这也就是采用$l_1$范数会使得$w$产生稀疏性的原因。</p><p>以上的分析虽然是基于二维的情况，但不难将其推广到多维度情况，其主要目的是为了直观地说明$l_1、l_2$正则化最优解的差异，以及$l_1$范数为什么会产生稀疏性。</p><h1 id="正则化效果分析"><a href="#正则化效果分析" class="headerlink" title="正则化效果分析"></a>正则化效果分析</h1><h2 id="稀疏性"><a href="#稀疏性" class="headerlink" title="稀疏性"></a>稀疏性</h2><p>从以上的正则化效果理解其实已经可以看出$L1$正则化可以使得答案稀疏的效果，如果那个还不好理解，可以看下面这个例子。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>L1正则化和L2正则化哪个能产生稀疏的解呢？答案是<strong>L1正则化</strong>。假设我们现在要求解模型$Ax=b$，也就是在2维空间上找到一条直线来拟合样本点。我们需要两个点才能去固定一条直线，但是，假设我们现在的训练样本中只有一个点。那么我们将得到无穷多个解。假设，该点为(10, 5)，直线为$y=a*x+b$，那么，该例子可形式化为求解模型：$b = 5 - 10*a$ 的参数。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-13-093559.jpg" alt="1_sMS5qc_2O6h87L_NF0B8Mw"></p><p>那么，当我们加上正则化项后，又该如何求解呢？</p><p>假设，我们的正则化的值等于一个常数，它的图像如下所示：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-13-093553.jpg" alt="1_uHXe9qibzdqieBfje7Hggw"></p><p>我们注意到，在红色直线上，并不是所有点都是稀疏点，而只有在顶点处的点才是稀疏的，因为顶点处的点某些维度为0。现在，我们要做的是就是扩大这个红色的形状，让它慢慢的靠近上图中蓝色的直线直到两者有公共点。当我们慢慢增大后，我们发现，最有可能成为公共交点的就是红色形状的顶点。而从刚才的分析中，我们知道，红色形状的顶点是稀疏点，所以，加上L1正则化后，得到的解往往都是稀疏的，而且这些公共点对应的常数$c$也是比较小的。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-13-093556.jpg" alt="1_0QRBxi6dlivROqCSFQeYhA"></p><p>而，L2正则化没有这种稀疏性特点。</p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>L1正则化具有特征选择的功能，这是因为L1正则化通常会产生系数的解，假设我们有100个系数，在L1正则化的作用下只有10个系数非0，那么这就等价于我们从100个特征中抽出10个重要的特征。</p><h2 id="计算复杂度"><a href="#计算复杂度" class="headerlink" title="计算复杂度"></a>计算复杂度</h2><p>在计算效率上，L2优于L1，因为L1正则化通常是不可导的，这导致我们不能用矩阵方式来求解它，而大多数是依赖于近似的方式。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a" target="_blank" rel="noopener">L1 Norm Regularization and Sparsity Explained for Dummies</a></p><p>[2]. <a href="https://www.wikiwand.com/en/Regularization_(mathematics" target="_blank" rel="noopener">Regularization (mathematics)</a>)</p><p>[3]. <a href="https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms" target="_blank" rel="noopener">L1 Norms versus L2 Norms</a></p><p>[4]. <a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2I7tB6oIINGBmW50rrmFTqf" target="_blank" rel="noopener">Hsuan-Tien Lin. Machine Learning Foundations Lecture 14.</a></p><p>[5]. <a href="https://zhuanlan.zhihu.com/p/29360425" target="_blank" rel="noopener">深入理解L1、L2正则化</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;范数&quot;&gt;&lt;a href=&quot;#范数&quot; class=&quot;headerlink&quot; title=&quot;范数&quot;&gt;&lt;/a&gt;范数&lt;/h1&gt;&lt;p&gt;过拟合是机器学习中一个比较常见的问题，而正则化是解决模型过拟合的一种手段。我们先看一下范数的定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Overfiting" scheme="http://conghuai.me/categories/Machine-Learning/Overfiting/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Hinge Loss Function</title>
    <link href="http://conghuai.me/2018/04/10/Hinge-Loss-Function/"/>
    <id>http://conghuai.me/2018/04/10/Hinge-Loss-Function/</id>
    <published>2018-04-10T06:39:46.000Z</published>
    <updated>2018-04-12T00:39:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="函数特性"><a href="#函数特性" class="headerlink" title="函数特性"></a>函数特性</h1><p>在机器学习中，<strong>hinge loss</strong>是一种损失函数，它通常用于”maximum-margin”的分类任务中，如支持向量机。数学表达式为：</p><p>$$L(y)=max(0,1-\hat{y}y)$$</p><p>其中$\hat{y}$表示预测输出，通常都是软结果（就是说输出不是0，1这种，可能是0.87。），$y$表示正确的类别。</p><ul><li>如果$\hat{y}y&lt;1$，则损失为：$1-\hat{y}y$</li><li>如果$\hat{y}y&gt;=1$，则损失为：0</li></ul><p>其函数图像如下，与0-1损失对比：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-10-065844.jpg" alt="Hinge_loss_vs_zero_one_loss.svg"></p><h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p>以支持向量机为例，其模型为：$\hat{y}=w\cdot x$，如果用hinge损失，其求导结果如下：</p><p>$$\begin{split}\frac{\partial L}{\partial w_i}=\begin{cases} -y\cdot x_i, &amp; \text{if $\hat{y}y&lt;1$} \\ 0, &amp; \text{otherwise}\end{cases}\end{split}$$</p><h1 id="变种"><a href="#变种" class="headerlink" title="变种"></a>变种</h1><p>实际应用中，一方面很多时候我们的y的值域并不是[-1,1]，比如我们可能更希望y更接近于一个概率，即其值域最好是[0,1]。另一方面，很多时候我们希望训练的是两个样本之间的相似关系，而非样本的整体分类，所以很多时候我们会用下面的公式： </p><p>$$l(y,y’)=max(0, m-y+y’)$$</p><p>其中，y是正样本的得分，y’是负样本的得分，m是margin（自己选一个数）</p><p>即我们希望正样本分数越高越好，负样本分数越低越好，但二者得分之差最多到m就足够了，差距增大并不会有任何奖励。</p><p>比如，我们想训练词向量，我们希望经常同时出现的词，他们的向量内积越大越好；不经常同时出现的词，他们的向量内积越小越好。则我们的hinge loss function可以是： </p><p>$$l(w,w_+,w_-)=max(0, 1-w^T\cdot w_+ + w^T\cdot w_-) $$</p><p>其中，w是当前正在处理的词，$w_+$是w在文中前3个词和后3个词中的某一个词，$w_−$是随机选的一个词。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="https://www.wikiwand.com/en/Hinge_loss" target="_blank" rel="noopener">Wikiwand Hinge loss</a></p><p>[2]. <a href="https://blog.csdn.net/luo123n/article/details/48878759" target="_blank" rel="noopener">损失函数：Hinge Loss（max margin）</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;函数特性&quot;&gt;&lt;a href=&quot;#函数特性&quot; class=&quot;headerlink&quot; title=&quot;函数特性&quot;&gt;&lt;/a&gt;函数特性&lt;/h1&gt;&lt;p&gt;在机器学习中，&lt;strong&gt;hinge loss&lt;/strong&gt;是一种损失函数，它通常用于”maximum-margin
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Loss Function" scheme="http://conghuai.me/categories/Machine-Learning/Loss-Function/"/>
    
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Negative Maximum Likehood Loss Function</title>
    <link href="http://conghuai.me/2018/04/08/Negative-Maximum-Likehood-Loss-Function/"/>
    <id>http://conghuai.me/2018/04/08/Negative-Maximum-Likehood-Loss-Function/</id>
    <published>2018-04-08T08:15:39.000Z</published>
    <updated>2018-04-08T09:00:55.773Z</updated>
    
    <content type="html"><![CDATA[<p>损失函数是用来衡量模型好坏的一个标准，在机器学习里，我们通常希望模型有比较小的<code>loss</code>，那么该如何选择我们的损失函数呢？最小化负的似然函数，借鉴了统计学的思想，是一种常见的损失函数。</p><h1 id="Nagative-Maximum-Likehood"><a href="#Nagative-Maximum-Likehood" class="headerlink" title="Nagative Maximum Likehood"></a>Nagative Maximum Likehood</h1><p>首先，假设我们有一堆的样本点：$D={(x_1,-1),(x_2,1),…,(x_N,-1)}$，我们希望我们训练出来的模型能够准确预测$x_i$的类别。通常来说，我们定义的模型都会对每个目标类别输出一个概率值，所以，本质上，我们希望得到一个函数，它能告诉我们样本$x_i$属于+1的概率（或者属于-1的概率，本质上是一样的）：$f(x)=P(+1|x)$</p><p>$$\begin{split}P(y|x)=\begin{cases} f(x) &amp; \text{for $y=+1$} \\ 1-f(x) &amp; \text{for y=-1}\end{cases}\end{split}$$</p><p>所以，我们的模型$h$产生样本集$D$的概率有多大呢：</p><p>$$L=P(x1)h(x1)\cdot P(x_2)(1-h(x_2))\cdot …\cdot P(x_N)(1-h(x_N))$$</p><p>如果我们的模型足够好的话，那么上面的似然函数的值会很大，我们现在需要在假设空间里面，把最好的$h$给找出来，哪个$h$最好呢？就是使得$L$最大的那个模型咯。</p><p>$$L=P(x1)h_1(x1)\cdot P(x_2)(1-h_1(x_2))\cdot …\cdot P(x_N)(1-h_1(x_N))$$</p><p>$$L=P(x1)h_2(x1)\cdot P(x_2)(1-h_2(x_2))\cdot …\cdot P(x_N)(1-h_2(x_N))$$</p><p>…</p><p>$$L=P(x1)h_k(x1)\cdot P(x_2)(1-h_k(x_2))\cdot …\cdot P(x_N)(1-h_k(x_N))$$</p><p>我们发现，对于不同的模型，都要乘上$P(x_1)P(x_2)…P(x_N)$，这对于我们比较不同模型的好坏，没有帮助，所有我们可以把它略去。</p><p>所以，似然函数表达式变为：</p><p>$$L=h_k(x1)\cdot (1-h_k(x_2))\cdot …\cdot(1-h_k(x_N))$$</p><p>我们通常选择sigmoid function当做$h$，本文最后会给出sigmoid function的性质。由于采用了sigmoid函数，似然函数表达式变为：</p><p>$$L=h_k(x1)\cdot h_k(-x_2)\cdot …\cdot h_k(-x_N)$$</p><p>进一步的可以写成：</p><p>$$L=h_k(y_1x_1)\cdot h_k(y_2x_2)\cdot …\cdot h_k(y_Nx_N)=\prod_{n=1}^Nh(y_nx_n)$$</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>对上述的似然函数取负数，再取log，可以得到最终的负对数似然损失函数：</p><p>$$MLE = \frac{1}{N}\sum_{n=1}^N-ln\theta(y_nw^Tx_n)=\frac{1}{N}\sum_{n=1}^Nln(1+e^{-y_nw^Tx_n})$$</p><h2 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-08-085532.jpg" alt="Screen Shot 2018-04-08 at 16.55.05"></p><h1 id="sigmoid-function"><a href="#sigmoid-function" class="headerlink" title="sigmoid function"></a>sigmoid function</h1><p>我们通常会选择<code>sigmoid function</code>当做$h$。</p><h2 id="函数特性"><a href="#函数特性" class="headerlink" title="函数特性"></a>函数特性</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-08-085535.jpg" alt="Screen Shot 2018-04-08 at 16.40.42"></p><p>$$\theta(s)=\frac{e^s}{1+e^s}=\frac{1}{1+e^{-s}}$$</p><ul><li>$\theta(负无穷)$=0；</li><li>$\theta(0)=\frac{1}{2}$</li><li>$\theta(正无穷)=1$</li><li>连续、单调</li><li>$\theta(-s)=1-\theta(s)$</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/" target="_blank" rel="noopener">机器学习-损失函数</a></p><p>[2]. 林轩田 “机器学习基石”</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;损失函数是用来衡量模型好坏的一个标准，在机器学习里，我们通常希望模型有比较小的&lt;code&gt;loss&lt;/code&gt;，那么该如何选择我们的损失函数呢？最小化负的似然函数，借鉴了统计学的思想，是一种常见的损失函数。&lt;/p&gt;
&lt;h1 id=&quot;Nagative-Maximum-Lik
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Loss Function" scheme="http://conghuai.me/categories/Machine-Learning/Loss-Function/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Cross Entropy Loss Function</title>
    <link href="http://conghuai.me/2018/04/08/Cross%20Entropy%20Loss%20Function/"/>
    <id>http://conghuai.me/2018/04/08/Cross Entropy Loss Function/</id>
    <published>2018-04-08T02:54:16.000Z</published>
    <updated>2018-04-14T13:58:29.941Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章中，讨论的Cross Entropy损失函数常用于分类问题中，但是为什么它会在分类问题中这么有效呢？我们先从一个简单的分类例子来入手。</p><h1 id="预测政治倾向例子"><a href="#预测政治倾向例子" class="headerlink" title="预测政治倾向例子"></a>预测政治倾向例子</h1><p>我们希望根据一个人的年龄、性别、年收入等相互独立的特征，来预测一个人的政治倾向，有三种可预测结果：民主党、共和党、其他党。假设我们当前有两个模型，这两个模型最后输出都是通过softmax的方式得到对于每个预测结果的概率：</p><p><strong>模型1</strong>：</p><table><thead><tr><th style="text-align:center">COMPUTED</th><th style="text-align:center">TARGETS</th><th style="text-align:center">CORRECT?</th></tr></thead><tbody><tr><td style="text-align:center">0.3 0.3 0.4</td><td style="text-align:center">0 0 1（民主党）</td><td style="text-align:center">正确</td></tr><tr><td style="text-align:center">0.3 0.4 0.3</td><td style="text-align:center">0 1 0（共和党）</td><td style="text-align:center">正确</td></tr><tr><td style="text-align:center">0.1 0.2 0.7</td><td style="text-align:center">1 0 0 （其他党）</td><td style="text-align:center">错误</td></tr></tbody></table><p><strong>模型1</strong>对于样本1和样本2以非常微弱的优势判断正确，对于样本3的判断则彻底错误。</p><p><strong>模型2</strong>：</p><table><thead><tr><th style="text-align:center">COMPUTED</th><th style="text-align:center">TARGETS</th><th style="text-align:center">CORRECT?</th></tr></thead><tbody><tr><td style="text-align:center">0.1 0.2 0.7</td><td style="text-align:center">0 0 1（民主党）</td><td style="text-align:center">正确</td></tr><tr><td style="text-align:center">0.1 0.7 0.2</td><td style="text-align:center">0 1 0（共和党）</td><td style="text-align:center">正确</td></tr><tr><td style="text-align:center">0.3 0.4 0.3</td><td style="text-align:center">1 0 0 （其他党）</td><td style="text-align:center">错误</td></tr></tbody></table><p><strong>模型2</strong>对于样本1和样本2判断非常准确，对于样本3判断错误，但是相对来说没有错得太离谱。</p><p>好了，有了模型之后，我们需要通过定义损失函数来判断模型在样本上的表现了，那么我们可以定义哪些损失函数呢？</p><h2 id="Classification-Error（分类错误率）"><a href="#Classification-Error（分类错误率）" class="headerlink" title="Classification Error（分类错误率）"></a>Classification Error（分类错误率）</h2><p>最为直接的损失函数定义为：$classification\ error=\frac{count\ of\ error\ items}{count\ of \ all\ items}$</p><p><strong>模型1：</strong>$classification\ error=\frac{1}{3}$</p><p><strong>模型2：</strong>$classification\ error=\frac{1}{3}$</p><p>我们知道，<strong>模型1</strong>和<strong>模型2</strong>虽然都是预测错了1个，但是相对来说<strong>模型2</strong>表现的更好，损失函数值照理来说应该更小，但是，很遗憾的是，$classification\ error$并不能判断出来，所以这种损失函数虽然好理解，但表现不太好。</p><h2 id="Mean-Squared-Error-平方和"><a href="#Mean-Squared-Error-平方和" class="headerlink" title="Mean Squared Error (平方和)"></a>Mean Squared Error (平方和)</h2><p>平方和损失也是一种比较常见的损失函数，其定义为：$MSE=\frac{1}{n}\sum_{i}^n(\hat{y_i}-y_i)$</p><p><strong>模型1：</strong>$MSE=\frac{0.54+0.54+1.34}{3}=0.81$</p><p><strong>模型2：</strong>$MSE=\frac{0.14+0.14+0.74}{3}=0.34$</p><p>MSE能够判断出来<strong>模型2</strong>优于<strong>模型1</strong>，那为什么不采样这种损失函数呢？原因在于，使用该损失函数时，得到的表达式是非凸函数，有很多局部的极值点。在做优化的时候不太好处理。</p><p>有了上面的直观分析，我们可以清楚的看到，对于分类问题的损失函数来说，分类错误率和平方和损失都不是很好的损失函数，下面我们来看一下交叉熵损失函数是怎么解决这个问题的。</p><h1 id="Cross-Entropy-Error-Function"><a href="#Cross-Entropy-Error-Function" class="headerlink" title="Cross Entropy Error Function"></a>Cross Entropy Error Function</h1><p>交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和softmax函数一起出现。</p><h2 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h2><h3 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h3><p>在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为$p$和$1-p$。此时表达式为：$−(ylog(p)+(1−y)log(1−p))$</p><h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><p>多分类的情况实际上就是对二分类的扩展：$-\sum_{c=1}^My_{o,c}\log(p_{o,c})$</p><p>其中：</p><ul><li>$M$——类别的数量；</li><li>$y$——指示变量（0或1）,如果该类别和样本观测到的类别相同就是1，否则是0；</li><li>$p$——对于观测样本属于类别c的预测概率。</li></ul><h2 id="函数图像"><a href="#函数图像" class="headerlink" title="函数图像"></a>函数图像</h2><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-08-024353.jpg" alt="cross_entropy"></p><p>可以看出，该函数是凸函数，求导时能够得到全局最优值。</p><h2 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h2><p>我们用神经网络最后一层输出的情况，来看一眼整个模型预测及获得损失的的流程：</p><ol><li>神经网络最后一层得到每个类别的得分<strong>scores</strong>；</li><li>该得分经过softmax转换为概率输出；</li><li>模型预测的类别概率输出与真实类别的one hot形式进行cross entropy损失函数的计算。</li></ol><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-08-064410.jpg" alt="Screen Shot 2018-04-08 at 11.56.55"></p><p>下面，我们以二分类的情形来推导一下整个求导公式，我们将求导分成两个过程，即拆成两项偏导的乘积。：</p><p>$$\frac{\partial E}{\partial score_i}=\frac{\partial E}{\partial p_i}\cdot \frac{\partial p_i}{\partial score_i}$$</p><h3 id="计算第一项：-frac-partial-E-partial-p-i"><a href="#计算第一项：-frac-partial-E-partial-p-i" class="headerlink" title="计算第一项：$\frac{\partial E}{\partial p_i}$"></a>计算第一项：$\frac{\partial E}{\partial p_i}$</h3><p>\begin{align}<br>\frac{\partial E}{\partial p_i} &amp;= \frac{\partial −(ylog(p)+(1−y)log(1−p))}{\partial p_i} \\<br> &amp;= -\frac{\partial y_ilogp_i}{\partial p_i}-\frac{\partial (1-y_i)log(1-p_i)}{\partial p_i} \\<br> &amp;= -\frac{y_i}{p_i}-[(1-y_i)\cdot \frac{1}{1-p_i}\cdot (-1)] \\<br> &amp;= -\frac{y_i}{p_i}-\frac{1-y_i}{1-p_i} \\<br>\end{align}</p><h3 id="计算第二项：-frac-partial-p-i-partial-score-i"><a href="#计算第二项：-frac-partial-p-i-partial-score-i" class="headerlink" title="计算第二项：$\frac{\partial p_i}{\partial score_i}$"></a>计算第二项：$\frac{\partial p_i}{\partial score_i}$</h3><p>这一项要计算的是softmax函数对于score的导数，我们先回顾一下分数求导的公式：</p><blockquote><p>$$f(x) = \frac{g(x)}{h(x)}=\frac{g’(x)h(x)-g(x){h}’(x)}{h^2(x)}$$</p></blockquote><p>考虑$k$等于$i$的情况：</p><p>\begin{align}<br>\frac{\partial p_i}{\partial score_i} &amp;= \frac{({e^{y_i}})’\cdot (\sum_ie^{y_i})-e^{y_i}\cdot {(\sum_j e^{y_i})}’}{(\sum_je^{y_i})^2} \\<br> &amp;= \frac{e^{y_i}\cdot \sum_ie^{y_i}-{(e^{y_i})}^2}{(\sum_je^{y_i})^2} \\<br> &amp;= \frac{e^{y_i}}{\sum_je^{y_i}} - \frac{(e^{y_i})^2}{(\sum_je^{y_i})^2} \\<br> &amp;= \frac{e^{y_i}}{\sum_je^{y_i}}\cdot (1 - \frac{e^{y_i}}{\sum_je^{y_i}}) \\<br> &amp;= \sigma(y_i)(1-\sigma(y_i)) \\<br>\end{align}</p><p>考虑k不等于i的情况：</p><p>\begin{align}<br>\frac{\partial p_i}{\partial score_i} &amp;= \frac{(e^{y_k})’\cdot (\sum_ie^{y_i})-e^{y_i}\cdot {(\sum_j e^{y_i})}’}{(\sum_je^{y_i})^2} \\<br> &amp;= \frac{0\cdot \sum_ie^{y_i}-(e^{y_i})\cdot (e^{y_k})}{(\sum_je^{y_i})^2} \\<br> &amp;= -\frac{e^{y_i}\cdot e^{y_k}}{(\sum_je^{y_i})^2} \\<br> &amp;= -\frac{e^{y_i}}{\sum_je^{y_i} }\cdot \frac{e^{y_k} }{\sum_je^{y_i}} \\<br> &amp;= -\sigma(y_i)\cdot \sigma(y_k) \\<br>\end{align}</p><p>综上可得softmax损失函数的求导结果：</p><p>\begin{split}\frac{\partial pi}{\partial score_i}=\begin{cases} \sigma(y_i)(1-\sigma(y_i)) &amp; \text{$if\ j=k$} \\ -\sigma(y_i)\cdot \sigma(y_k) &amp; \text{$if\ j \neq k$}\end{cases}\end{split}</p><h3 id="计算结果-frac-partial-E-partial-score-i"><a href="#计算结果-frac-partial-E-partial-score-i" class="headerlink" title="计算结果$\frac{\partial E}{\partial score_i}$"></a>计算结果$\frac{\partial E}{\partial score_i}$</h3><p>\begin{align}<br>\frac{\partial E}{\partial score_i} &amp;= \frac{\partial E}{\partial p_i}\cdot \frac{\partial p_i}{\partial score_i} \\<br> &amp;= [-\frac{y_i}{\sigma(y_i)}-\cdot \frac{1-y_i}{1-\sigma(y_i)}]\cdot  \sigma(y_i)(1-\sigma(y_i) \\<br> &amp;= -\frac{c_i}{\sigma(y_i)}\cdot \sigma(y_i)\cdot (1-\sigma(y_i))+\frac{1-c_i}{1-\sigma(y_i)}\cdot \sigma(y_i)\cdot (1-\sigma(y_i)) \\<br> &amp;= -c_i+c_i\cdot \sigma(y_i)+\sigma(y_i)-c_i\cdot \sigma(y_i) \\<br> &amp;= \sigma(y_i)-c_i \\<br>\end{align}</p><p>可以看到，我们得到了一个非常漂亮的结果，所以，Cross Entropy损失函数，不仅可以很好的衡量模型的效果，又可以很容易的的进行求导计算。</p><h1 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h1><p>我们对结果进一步对参数求导：即$\frac{\partial E}{\partial w_i}=\frac{\partial E}{\partial score_i}\cdot \frac{\partial score_i}{\partial w_i}=x_i\cdot [\sigma(y_i)-c_i]$</p><p>在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、学习率；二、偏导值。其中，学习率是我们需要设置的超参数，所以我们重点关注偏导值。从上面的式子中，我们发现，偏导值的大小取决于$x_i$和$[\sigma(y_i)-c_i]$，我们重点关注后者，后者的值大小反映了我们模型的错误程度，该值越大，说明模型效果越差，但是该值越大同时也会使得模型学习速度更快。所以，用交叉熵当损失函数在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢，这是我们希望得到的。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. <a href="http://jackon.me/posts/why-use-cross-entropy-error-for-loss-function/" target="_blank" rel="noopener">神经网络的分类模型 LOSS 函数为什么要用 CROSS ENTROPY</a></p><p>[2]. <a href="http://sefiks.com/2017/11/08/softmax-as-a-neural-networks-activation-function/" target="_blank" rel="noopener">Softmax as a Neural Networks Activation Function</a></p><p>[3]. <a href="https://sefiks.com/2017/12/17/a-gentle-introduction-to-cross-entropy-loss-function/" target="_blank" rel="noopener">A Gentle Introduction to Cross-Entropy Loss Function</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章中，讨论的Cross Entropy损失函数常用于分类问题中，但是为什么它会在分类问题中这么有效呢？我们先从一个简单的分类例子来入手。&lt;/p&gt;
&lt;h1 id=&quot;预测政治倾向例子&quot;&gt;&lt;a href=&quot;#预测政治倾向例子&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Loss Function" scheme="http://conghuai.me/categories/Machine-Learning/Loss-Function/"/>
    
    
      <category term="math" scheme="http://conghuai.me/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>K-Means</title>
    <link href="http://conghuai.me/2017/12/07/K-Means/"/>
    <id>http://conghuai.me/2017/12/07/K-Means/</id>
    <published>2017-12-07T01:01:09.000Z</published>
    <updated>2018-04-16T07:57:49.446Z</updated>
    
    <content type="html"><![CDATA[<p>​聚类，是机器学习的任务之一。同分类算法一样，聚类算法也被广泛的应用在各个领域，如根据话题，对文章、网页和搜索结果做聚类；根据社区发现对社交网络中的用户做聚类；根据购买历史记录对消费者做聚类。和分类算法不同的是，聚类算法的样本是没有标签的，也就是说，我们并不知道样本有哪些类别，算法需要根据样本的特征，对样本进行聚类，形成不同的聚类中心点。这篇文章，主要介绍比较著名的聚类算法——K-means算法。</p><p>​首先，我们看一下基于目标来做聚类的算法定义:</p><p><strong>Input</strong> : A set S of n points, also a distance/dissimilarity measure specifying the distance d(x, y) between pairs (x, y). </p><p><strong>Goal</strong>: output a partition of the data</p><p>​基于这个定义，选择不同的距离计算公式，有以下三种具体的算法:</p><ul><li><strong>k-means</strong>: find center partitions $c_1, c_2, …, c_k$ to minimize<br>$$ \sum min_{j \in{i, …,k}}d^2(x^i, c_j) $$</li><li><strong>k-median</strong>: find center partitions $c_1, c_2, …, c_k$ to minimize<br>$$ \sum min_{j \in{i, …,k}}d(x^i, c_j) $$ </li><li><strong>k-center</strong>: find partition to minimize the maximum radius</li></ul><h1 id="Euclidean-k-means-clustering"><a href="#Euclidean-k-means-clustering" class="headerlink" title="Euclidean k-means clustering"></a>Euclidean k-means clustering</h1><p>采用欧拉距离公式的k-means算法定义如下:</p><p><strong>Input</strong>: A set of n datapoints $x^1, x^2, …, x^n$ in $R^d$ (target #clusters k)</p><p><strong>Output</strong>: k representatives $c_1, c_2, …, c_k \in R^d$ </p><p><strong>Objective</strong>: choose $c_1, c_2, …, c_k \in R^d$ to minimize<br>$$ \sum min_{j \in {1,…,k}}||x^i - c_j||^2 $$</p><p>求解该算法的最优解是一个NP难的问题，所有我们没有办法获得最优解，当然，当k=1或d=1这种特殊情况下，是可以获得最优解，有兴趣的可以自行推导一下， 这里不在赘述，这里我们主要介绍Lloyd’s method[1]，该方法的核心算法如下:</p><p><strong>Input</strong>: A set of n datapoints $x^1, x^2, …, x^n$ in $R^d$</p><p><strong>Initialize</strong> centers $c_1, c_2, …, c_k \in R^d$ and clusters $C_1, C_2, …, C_k$ in any way.</p><p><strong>Repeat</strong> until there is no further change in the cost.</p><ol><li>For each j: $C_j \leftarrow {x \in S\ whose\ closest\ center\ is\ c_j}$</li><li>For each j: $c_j \leftarrow mean\ of\ C_j $</li></ol><p>对于该算法，难度不是特别大，最重要的地方在Repeat中的1，2两个步骤，其中，步骤1将固定住聚类中心$c_1, c_2, …, c_k$，更新聚类集$C_1, C_2, …, C_k$。步骤2固定住聚类集$C_1, C_2, …, C_k$，更新聚类中心$c_1, c_2, …, c_k$（本文用大写字符表示集合，这里用C指聚类簇；用小写字符表示单个点，这里用c指聚类中心。在本人的其他博文中，也会采用这种表述方式）。</p><p>大部分学习k-means算法的人理解了步骤1和步骤2就觉得已经理解了k-means了，其实不然，先不说k-means中比较重要的聚类中心的初始化问题，任何一个机器学习算法，它要是有效的，必须证明其可收敛，也需要给出其时间复杂度和空间复杂度。</p><h1 id="Converges"><a href="#Converges" class="headerlink" title="Converges"></a>Converges</h1><ul><li>目标函数的值在每一轮的迭代都会降低，这个特性由算法中步骤1和步骤2保证，因为对于每个样本点，我们每次都是选择最接近的聚类中心；而且，在每个聚类簇里，我们选择平均值作为其聚类中心。</li><li>目标函数有最小值0。</li></ul><p>由于目标函数有最小值，而且在每一轮中都是值都是减少的，所有算法必然会收敛。</p><h1 id="Running-Time"><a href="#Running-Time" class="headerlink" title="Running Time"></a>Running Time</h1><ul><li>O(nkd)  n为样本数 k为聚类中心数 d为维度</li></ul><h1 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h1><p>介绍完了整个算法过程、收敛性和时间复杂度之后，该算法的两个核心点需要我们思考: 1. 如何选择k的值; 2. 算法刚开始，并没有聚类中心，如何初始化聚类中心。对于问题1，我目前还没有过多的认识。这里主要介绍问题2，如何初始化聚类中心。</p><h2 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h2><p>这种初始化方式是最简单的方式，就是随机选k个点作为聚类中心，虽然简单，但是会存在问题，我们看下面的这个例子:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010551.jpg" alt="random init"></p><p>由于，我们采用了随机初始化的方式，对于这个样本，我们随机初始化的三个点如上图的绿、红、黑三个样本点，再后面的迭代中，我们最后的聚类簇如上图的箭头所示，这样的效果好吗？显然是不好的，为什么呢？因为很显然最左边三个、中间三个、最右边三个应该是被归为一个聚类簇的:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010534.jpg" alt="random init2"></p><p>我们可以看到，聚类中心初始化得不好，直接影响我们最后聚类的效果，可能上面举的例子样本分布和初始化聚类中心太极端，不能说明问题， 我们现在假设样本的分布是多个高斯分布的情况下，聚类中心初始化不好导致的最后聚类的效果:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010548.jpg" alt="random init4"></p><p>我们现在计算一下假设有k个高斯分布，我们随机初始化正确的概率有大(所谓正确是指任何两个随机初始化中心不在同一个高斯分布中):$\frac {k!}{k^k} \approx \frac {1}{e^k}$，当k增大时，这个概率会越来越低。</p><h2 id="Furthest-Point-Heuristic"><a href="#Furthest-Point-Heuristic" class="headerlink" title="Furthest Point Heuristic"></a>Furthest Point Heuristic</h2><p>这种方法是一个中心点一个中心点依次进行初始化的，首先随机初始化$c_1$，然后选择距离$c_1$最远的点来初始化$c_2$，以此类推。</p><p>算法描述如下:</p><p>Choose $c_1$ arbitrarily (or at random).</p><p>For j = 2, …, k</p><p>Pick $c_j$ among datapoints $x^1, x^2, …, x^n$ that is farthest from previously chosen $c_1, c_2, …, c_{j-1}$</p><p>这种方法解决了随机初始化高斯分布例子中的问题:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010552.jpg" alt="dist 1"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010546.jpg" alt="dist 2"></p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010549.jpg" alt="dist 3"></p><p>但是，这种方法的问题是容易受噪声点干扰，请看下面的例子:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-07-010545.jpg" alt="dist 4"></p><p>所以这种方式进行初始化也是不行的，一旦出现噪声点，就极大的影响了最后聚类的结果。虽然实际上，几乎没有哪一个k-means算法会采用上面两种初始化方式，但是这里这样介绍是顺着我们的思维方式进行的，一般思考的方式都是从简单到复杂，所以下面，我们也顺理成章的引出<code>k-means++</code>这个初始化算法， 该算法很好的反映出随机化思想在算法中的重要性。</p><h2 id="k-means"><a href="#k-means" class="headerlink" title="k-means++"></a>k-means++</h2><p>算法描述如下:</p><ul><li><p>Choose $c_1$ at random.</p></li><li><p>For j = 2, …, k</p></li><li><p>Pick $c_j$ among $x^1, x^2, …, x^n$ according to the distribution</p><p>  $ Pr(c_j = x^i) \propto min_{j’ &lt; j}\left | x^i - c_{j’} \right |^2 $</p></li></ul><p>这就是k-means++的初始化过程，这个过程比较不好理解。关于这个过程，作以下几点说明:</p><ul><li>这个初始化算法引入随机化，下一个被选为中心点的样本不是固定的，而是一个概率值，这个概率值正比于“离最近中心点的距离“。</li><li>”离最近中心点的距离“如何计算，实际上非常简单，就是当前样本距离各个中心点的距离中，最小的那个距离。</li><li>既然概率正比于 ”距离“ ，那么离群点的”距离“肯定是最大的，它的概率肯定是最大的，可是为什么算法不一定会选择它呢？举个例子说明，如果我们现在有一个聚类集合$S={x_1,x_2,x_3}$,和离群点$x_o$，假设选中 $x_o$的概率为 $1/3$ , 选中 $x_1, x_2, x_3$的概率分别为 $2/9$，这样看，即使$x_o$的概率很大，但是它只有1个，而 $x_1, x_2, x_3$ 即使每个概率不大，但是我们只要随便选中其中一个都是可以的(这是因为它们都在一个聚类簇中，只要选择聚类簇中任何一个点当聚类中心都可以)，所以可以把他们的概率相加，最后得到的概率就大于选中 $x_o$的概率。</li></ul><h1 id="In-Action"><a href="#In-Action" class="headerlink" title="In Action"></a>In Action</h1><p>当然，在实际项目中，我们可能不会自己实现<code>k-means</code>算法， 一般我们都会用现成的比较好的一些机器学习库，我们这里结合<code>scikit-learn</code>来看一下，它是如何实现<code>k-means</code>算法的。</p><p>首先看一下，<code>sklearn.cluster.k_means</code>模块下的函数<code>k_means</code>方法:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">k_means</span><span class="params">(X, n_clusters, init=<span class="string">'k-means++'</span>, precompute_distances=<span class="string">'auto'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            n_init=<span class="number">10</span>, max_iter=<span class="number">300</span>, verbose=False,</span></span></span><br><span class="line"><span class="function"><span class="params">            tol=<span class="number">1e-4</span>, random_state=None, copy_x=True, n_jobs=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            algorithm=<span class="string">"auto"</span>, return_n_iter=False)</span>:</span></span><br></pre></td></tr></table></figure><p>首先，我们看到参数有一个<code>init</code>，这里是指定k-means初始化方法，这里我们看下注释:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">    init : &#123;'k-means++', 'random', or ndarray, or a callable&#125;, optional</span></span><br><span class="line"><span class="string">        Method for initialization, default to 'k-means++':</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        'k-means++' : selects initial cluster centers for k-mean</span></span><br><span class="line"><span class="string">        clustering in a smart way to speed up convergence. See section</span></span><br><span class="line"><span class="string">        Notes in k_init for more details.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        'random': generate k centroids from a Gaussian with mean and</span></span><br><span class="line"><span class="string">        variance estimated from the data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If an ndarray is passed, it should be of shape (n_clusters, n_features)</span></span><br><span class="line"><span class="string">        and gives the initial centers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If a callable is passed, it should take arguments X, k and</span></span><br><span class="line"><span class="string">        and a random state and return an initialization.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>可以看到，<code>sklearn</code>实现了2种初始化算法，一个是随机初始化算法，另一个是<code>k-means++</code>算法，默认采用的是<code>k-means++</code>算法。然后，我们先看一下<code>sklearn</code>实现<code>k-means++</code>的代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_k_init</span><span class="params">(X, n_clusters, x_squared_norms, random_state, n_local_trials=None)</span>:</span></span><br><span class="line">    <span class="string">"""Init n_clusters seeds according to k-means++</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    -----------</span></span><br><span class="line"><span class="string">    X : array or sparse matrix, shape (n_samples, n_features)</span></span><br><span class="line"><span class="string">        The data to pick seeds for. To avoid memory copy, the input data</span></span><br><span class="line"><span class="string">        should be double precision (dtype=np.float64).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    n_clusters : integer</span></span><br><span class="line"><span class="string">        The number of seeds to choose</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    x_squared_norms : array, shape (n_samples,)</span></span><br><span class="line"><span class="string">        Squared Euclidean norm of each data point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    random_state : numpy.RandomState</span></span><br><span class="line"><span class="string">        The generator used to initialize the centers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    n_local_trials : integer, optional</span></span><br><span class="line"><span class="string">        The number of seeding trials for each center (except the first),</span></span><br><span class="line"><span class="string">        of which the one reducing inertia the most is greedily chosen.</span></span><br><span class="line"><span class="string">        Set to None to make the number of trials depend logarithmically</span></span><br><span class="line"><span class="string">        on the number of seeds (2+log(k)); this is the default.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Notes</span></span><br><span class="line"><span class="string">    -----</span></span><br><span class="line"><span class="string">    Selects initial cluster centers for k-mean clustering in a smart way</span></span><br><span class="line"><span class="string">    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.</span></span><br><span class="line"><span class="string">    "k-means++: the advantages of careful seeding". ACM-SIAM symposium</span></span><br><span class="line"><span class="string">    on Discrete algorithms. 2007</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,</span></span><br><span class="line"><span class="string">    which is the implementation used in the aforementioned paper.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">    centers = np.empty((n_clusters, n_features), dtype=X.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> x_squared_norms <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, <span class="string">'x_squared_norms None in _k_init'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the number of local seeding trials if none is given</span></span><br><span class="line">    <span class="keyword">if</span> n_local_trials <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># This is what Arthur/Vassilvitskii tried, but did not report</span></span><br><span class="line">        <span class="comment"># specific results for other than mentioning in the conclusion</span></span><br><span class="line">        <span class="comment"># that it helped.</span></span><br><span class="line">        n_local_trials = <span class="number">2</span> + int(np.log(n_clusters))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pick first center randomly</span></span><br><span class="line">    center_id = random_state.randint(n_samples)</span><br><span class="line">    <span class="keyword">if</span> sp.issparse(X):</span><br><span class="line">        centers[<span class="number">0</span>] = X[center_id].toarray()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        centers[<span class="number">0</span>] = X[center_id]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize list of closest distances and calculate current potential</span></span><br><span class="line">    closest_dist_sq = euclidean_distances(</span><br><span class="line">        centers[<span class="number">0</span>, np.newaxis], X, Y_norm_squared=x_squared_norms,</span><br><span class="line">        squared=<span class="keyword">True</span>)</span><br><span class="line">    current_pot = closest_dist_sq.sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pick the remaining n_clusters-1 points</span></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">1</span>, n_clusters):</span><br><span class="line">        <span class="comment"># Choose center candidates by sampling with probability proportional</span></span><br><span class="line">        <span class="comment"># to the squared distance to the closest existing center</span></span><br><span class="line">        rand_vals = random_state.random_sample(n_local_trials) * current_pot</span><br><span class="line">        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),</span><br><span class="line">                                        rand_vals)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute distances to center candidates</span></span><br><span class="line">        distance_to_candidates = euclidean_distances(</span><br><span class="line">            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decide which candidate is the best</span></span><br><span class="line">        best_candidate = <span class="keyword">None</span></span><br><span class="line">        best_pot = <span class="keyword">None</span></span><br><span class="line">        best_dist_sq = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">for</span> trial <span class="keyword">in</span> range(n_local_trials):</span><br><span class="line">            <span class="comment"># Compute potential when including center candidate</span></span><br><span class="line">            new_dist_sq = np.minimum(closest_dist_sq,</span><br><span class="line">                                     distance_to_candidates[trial])</span><br><span class="line">            new_pot = new_dist_sq.sum()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store result if it is the best local trial so far</span></span><br><span class="line">            <span class="keyword">if</span> (best_candidate <span class="keyword">is</span> <span class="keyword">None</span>) <span class="keyword">or</span> (new_pot &lt; best_pot):</span><br><span class="line">                best_candidate = candidate_ids[trial]</span><br><span class="line">                best_pot = new_pot</span><br><span class="line">                best_dist_sq = new_dist_sq</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Permanently add best center candidate found in local tries</span></span><br><span class="line">        <span class="keyword">if</span> sp.issparse(X):</span><br><span class="line">            centers[c] = X[best_candidate].toarray()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            centers[c] = X[best_candidate]</span><br><span class="line">        current_pot = best_pot</span><br><span class="line">        closest_dist_sq = best_dist_sq</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> centers</span><br></pre></td></tr></table></figure><p>该算法的是基于 k-means++:the advantages of careful seeding[2]实现的，有兴趣的可以看一下这篇论文。代码第49行，可以看到，第一个初始中心是随机初始化的。代码62行，通过循环，依次初始化其他的聚类中心。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>Lloyd, Stuart P. Least squares quantization in PCM[J]. IEEE Transactions on Information Theory, 1982, 28(2):129-137.</li><li>Arthur D, Vassilvitskii S. k-means++:the advantages of careful seeding[C]// Eighteenth Acm-Siam Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, 2007:1027-1035.</li><li>Julyedu 机器学习算法班</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;​聚类，是机器学习的任务之一。同分类算法一样，聚类算法也被广泛的应用在各个领域，如根据话题，对文章、网页和搜索结果做聚类；根据社区发现对社交网络中的用户做聚类；根据购买历史记录对消费者做聚类。和分类算法不同的是，聚类算法的样本是没有标签的，也就是说，我们并不知道样本有哪些类
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Cluster" scheme="http://conghuai.me/categories/Machine-Learning/Cluster/"/>
    
    
      <category term="algorithm" scheme="http://conghuai.me/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Multiclass Classification</title>
    <link href="http://conghuai.me/2017/10/02/Multiclass-Classification/"/>
    <id>http://conghuai.me/2017/10/02/Multiclass-Classification/</id>
    <published>2017-10-02T06:47:25.000Z</published>
    <updated>2018-04-17T06:47:59.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Classification" scheme="http://conghuai.me/categories/Machine-Learning/Classification/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>Logistic Regression</title>
    <link href="http://conghuai.me/2017/09/17/Logistic-Regression/"/>
    <id>http://conghuai.me/2017/09/17/Logistic-Regression/</id>
    <published>2017-09-17T01:17:17.000Z</published>
    <updated>2018-04-24T06:17:57.191Z</updated>
    
    <content type="html"><![CDATA[<p>逻辑斯蒂回归（logistic regression）是统计学习中的经典方法，逻辑斯蒂回归模型与最大熵模型都属于对数线性模型。</p><h1 id="Logistic-Function"><a href="#Logistic-Function" class="headerlink" title="Logistic Function"></a>Logistic Function</h1><p>逻辑函数（逻辑曲线、sigmoid函数）表达式如下：</p><p>$$f(x)=\frac{L}{1+e^{-k(x-x_0)}}$$</p><p>其中：</p><ul><li>$e$是自然对数；</li><li>$x_0$是sigmoid曲线的中点；</li><li>$L$是曲线的最大值；</li><li>$k$是曲线的坡度</li></ul><p>其标准形式为：$L=1,k=1,x_0=0$</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-17-040033.jpg" alt="640px-Logistic-curve.svg"></p><h2 id="函数性质"><a href="#函数性质" class="headerlink" title="函数性质"></a>函数性质</h2><p>标准的逻辑函数表达式如下：</p><p>$$\begin{align} f(x) &amp;= \frac{1}{1+e^{-x}} \\  &amp;= \frac{e^x}{1+e^x} \\  &amp;= \frac{1}{2}+\frac{1}{2}\cdot tanh(\frac{x}{2}) \end{align}$$</p><ul><li>对称性：$1-f(x) = f(-x)$</li></ul><h2 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h2><p>逻辑函数的一个优点在于其容易求导：</p><p>$$\begin{align} \frac{df(x)}{dx} &amp;= \frac{e^x(1+e^x)-e^x\cdot e^x}{(1+e^{-x})^2} \\  &amp;= \frac{e^x}{(1+e^x)^2} \\  &amp;= f(x)\cdot (1-f(x)) \end{align}$$</p><h1 id="逻辑斯蒂回归模型"><a href="#逻辑斯蒂回归模型" class="headerlink" title="逻辑斯蒂回归模型"></a>逻辑斯蒂回归模型</h1><h2 id="二项逻辑斯蒂回归模型"><a href="#二项逻辑斯蒂回归模型" class="headerlink" title="二项逻辑斯蒂回归模型"></a>二项逻辑斯蒂回归模型</h2><p>二项逻辑斯蒂回归模型是一种分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的逻辑斯蒂分布，这里随机变量$X$取值为实数，随机变量$Y$取值为1或-1。</p><p>$$P(Y=1|x) = \frac{e^{w\cdot x+b}}{1+e^{w\cdot x+b}}$$</p><p>$$P(Y=-1|x) = \frac{1}{1+e^{w\cdot x+b}}$$</p><p>对于给定的输入实例$x$，按照上面式可以求得$P(Y=1|x)$和$P(Y=-1|x)$。逻辑斯蒂回归比较两个条件概率值的大小，将实例$x$分到概率值较大的那一类。</p><p>有时为了方便，将权值向量和输入向量进行扩充，仍记作$w,x$，即$w=(w^{(1)},w^{(2)},…,w^{(n)},b)^T$，$x=(x^{(1)},x^{(2)},…,x^{(n)},1)^T$，这时候，逻辑斯蒂回归模型如下：</p><p>$$P(Y=1|x) = \frac{e^{w\cdot x}}{1+e^{w\cdot x}}$$</p><p>$$P(Y=-1|x) = \frac{1}{1+e^{w\cdot x}}$$</p><h3 id="对数几率"><a href="#对数几率" class="headerlink" title="对数几率"></a>对数几率</h3><p>现在考察逻辑斯蒂回归模型的特点，一个事件的几率是指该事件发生的概率与该事件不发生的概率的比值，如果事件发生的概率是$p$，那么该事件的几率是$\frac{p}{1-p}$，该事件的对数几率或logit函数是</p><p>$$logit(p)= log\frac{p}{1-p}$$</p><p>对于逻辑斯蒂回归而言，得</p><p>$$\begin{align} log\frac{P(Y=1|x)}{1-P(Y=1|x)} &amp;= log\frac{\frac{e^{w\cdot x}}{1+e^{w\cdot x}}}{1- \frac{e^{w\cdot x}}{1+e^{w\cdot x}}} \\  &amp;= log\frac{e^{w\cdot x}}{1+e^{w\cdot x}-e^{w\cdot x}} \\  &amp;= loge^{w\cdot x}  \\&amp;= w\cdot x  \end{align}$$</p><p>这就是说，在逻辑斯蒂回归模型中，输出$Y=1$的对数几率是输入$x$的线性函数，或者说，输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯蒂回归模型。</p><p>换一个角度看，考虑对输入$x$进行分类的线性函数$w\cdot x$，其值域为实数域，通过逻辑斯蒂回归模型可以将线性函数$w\cdot x$转化为概率：</p><p>$$P(Y=1|x) = \frac{e^{w\cdot x}}{1+e^{w\cdot x}}$$</p><p>这时，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0。</p><h2 id="模型参数估计"><a href="#模型参数估计" class="headerlink" title="模型参数估计"></a>模型参数估计</h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>逻辑斯蒂回归模型学习时，对于给定的训练数据集$T=\{(x_1, 1),(x_2,-1),…,(x_N,-1)\}$，其中，$x_i\in R^n，y_i\in\{-1,1\}$，可以应用极大似然估计法估计模型参数，即用这个模型生成该数据集的可能性：</p><p>$$\begin{align} L(h) &amp;= P(x_1)\cdot h(1|x_1)\cdot P(x_2)\cdot h(-1|x_2)…P(x_N)\cdot h(-1|x_N) \\  &amp;= P(x_1)\cdot h(1|x_1)\cdot P(x_2)\cdot [1-h(1|x_2)]…P(x_N)\cdot [1-h(1|x_N)] \end{align}$$</p><p>对于$h$我们用sigmoid函数带入，用对称性$1-f(x) = f(-x)$得</p><p>$$\begin{align} L(h) &amp;= P(x_1)\cdot h(1|x_1)\cdot P(x_2)\cdot [1-h(1|x_2)]…P(x_N)\cdot [1-h(1|x_N)] \\ &amp;= P(x_1)\cdot h(1|x_1)\cdot P(x_2)\cdot h(-(1|x_2))…P(x_N)\cdot h(-(1|x_N))\end{align} $$</p><p>对于不同的模型，$P(x_1),P(x_2)…P(x_N)$是一样的，乘上这些值对于我们选择最优的模型没有帮助，故略去。</p><p>$$L(\sigma)\propto\prod_{n=1}^N\sigma(y_nw^Tx_n)$$</p><p>下面，通过最小化损失函数得到最终模型</p><p>$$\begin{align} g &amp;= argmax_\sigma \prod_{n=1}^N\sigma(y_nw^Tx_n) \\  &amp;= argmax_{\sigma}ln\sum_{n=1}^N\sigma(y_nw^Tx_n) \\  &amp;= argmin_w\frac{1}{N}\sum_{n=1}^N-ln\sigma(y_nw^Tx_n)  \\ &amp;= argmin_w\frac{1}{N} \sum_{n=1}^Nln(1+e^{-y_nw^Tx_n})\\ \end{align}$$</p><p>所以，损失函数为</p><p>$$E_{in}(w)=\frac{1}{N}\sum_{i=1}^Nln(1+e^{-y_nw^Tx_n})$$</p><h3 id="证明损失函数为交叉熵"><a href="#证明损失函数为交叉熵" class="headerlink" title="证明损失函数为交叉熵"></a>证明损失函数为交叉熵</h3><h4 id="1-从交叉熵公式推导到逻辑损失的cost-function"><a href="#1-从交叉熵公式推导到逻辑损失的cost-function" class="headerlink" title="(1) 从交叉熵公式推导到逻辑损失的cost function"></a>(1) 从交叉熵公式推导到逻辑损失的cost function</h4><p>如果不太熟悉交叉熵损失函数，可先回顾一下：<a href="http://conghuai.me/2018/04/08/Cross%20Entropy%20Loss%20Function/">Cross Entropy Loss Function</a></p><p>对于二分类损失问题，交叉熵损失函数定义为<br>$$E =\sum_{i=1}^N -(y\cdot log(p)+(1-y)\cdot log(1-p))$$</p><p>其中</p><ul><li>$y\in \{0,1\}$，一般规定正类为1，负类为0；</li><li>$p$为取正类的概率；</li><li>对于每个样本上式，其中一项为0；</li></ul><p>对于逻辑回归来说，其交叉熵损失函数中，$p$用逻辑函数带入，得</p><p>$$\begin{align} E &amp;=\sum_{i=1}^N -(y\cdot log(p)+(1-y)\cdot log(1-p)) \\  &amp;= \sum_{i=1}^N -(y\cdot log(\frac{1}{1+e^{-w\cdot x}})+(1-y)\cdot log(1-\frac{1}{1+e^{-w\cdot x}})) \\  &amp;= \sum_{i=1}^N -(-y\cdot log(1+e^{-w\cdot x})+(1-y)\cdot log(\frac{e^{-w\cdot x}}{1+e^{-w\cdot x}}))  \\ &amp;= \sum_{i=1}^N -(-y\cdot log(1+e^{-w\cdot x})+(1-y)\cdot log(\frac{1}{1+e^{w\cdot x}}))  \\ &amp;=  \sum_{i=1}^N -(-y\cdot log(1+e^{-w\cdot x})-(1-y)\cdot log(1+e^{w\cdot x})) \\ &amp;= \sum_{i=1}^N y\cdot log(1+e^{-w\cdot x})+(1-y)\cdot log(1+e^{w\cdot x})\end{align}$$</p><p>我们发现</p><p>$$\begin{split}E=\begin{cases} \sum_{i=1}^Nlog(1+e^{-w\cdot x}), &amp; \text{$y=+1$} \\ \sum_{i=1}^Nlog(1+e^{w\cdot x}), &amp; \text{$y=0$}\end{cases}\end{split}$$</p><p>我们可以简化上式，得</p><p>$$E= \sum_{i=1}^Nln(1+e^{-y_nw^Tx_n}),y_n\in \{1,-1\}$$</p><p>所以，我们发现，对逻辑斯蒂回归模型求负极大似然估计得到的损失函数，本质上就是交叉熵损失函数。</p><h4 id="2-直接定义逻辑回归的loss-function为交叉熵"><a href="#2-直接定义逻辑回归的loss-function为交叉熵" class="headerlink" title="(2) 直接定义逻辑回归的loss function为交叉熵"></a>(2) 直接定义逻辑回归的loss function为交叉熵</h4><p>逻辑回归模型的概率为</p><p>$$P(y=1|x) = \frac{e^{w\cdot x}}{1+e^{w\cdot x}}$$</p><p>$$P(y=0|x) = \frac{1}{1+e^{w\cdot x}}$$</p><p>我们可以将上面两种形式写成一种</p><p>$$P(y|x) =  (\frac{e^{w\cdot x}}{1+e^{w\cdot x}})^y(\frac{1}{1+e^{w\cdot x}})^{(1-y)}$$</p><p>当然，我们希望$P(y|x)$的概率越大越好，因为$P(y|x)$代表该样本label为$y$的概率，所以，我们可以把$-logP(y|x)$当做loss function：</p><p>$$l = -logP(y|x)=-[log\ y\cdot \frac{e^{w\cdot x}}{1+e^{w\cdot x}}+log\ (1-y)\cdot \frac{1}{1+e^{w\cdot x}}]$$</p><p>主要定义的损失函数即为交叉熵损失函数，最后推导的结果和用极大似然估计得到的目标函数是一样的。</p><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>现在，我们推导一下求导过程</p><p>$$E_{in}(w)=\frac{1}{N}\sum_{i=1}^Nln(1+e^{-y_nw^Tx_n})$$</p><p>对$w_i$求偏导，令$A = 1+e^{-y_nw^Tx_n}$，$B = e^{-y_nw^Tx_n}$</p><p>$$\frac{\partial E_{in}(w)}{\partial w_i}= \frac{1}{N} \sum_{i=1}^{N} \frac{\partial lnA}{\partial A}\cdot \frac{\partial (1+e^B)}{\partial B}\cdot \frac{\partial -y_nw^Tx_n}{\partial w_i}$$</p><h4 id="第一项-frac-partial-lnA-partial-A"><a href="#第一项-frac-partial-lnA-partial-A" class="headerlink" title="第一项 $\frac{\partial lnA}{\partial A}$"></a>第一项 $\frac{\partial lnA}{\partial A}$</h4><p>$$\frac{\partial lnA}{\partial A} = \frac{1}{A}$$</p><h4 id="第二项-frac-partial-1-e-B-partial-B"><a href="#第二项-frac-partial-1-e-B-partial-B" class="headerlink" title="第二项 $\frac{\partial (1+e^B)}{\partial B}$"></a>第二项 $\frac{\partial (1+e^B)}{\partial B}$</h4><p>$$\frac{\partial (1+e^B)}{\partial B}= e^B$$</p><h4 id="第三项-frac-partial-y-nw-Tx-n-partial-w-i"><a href="#第三项-frac-partial-y-nw-Tx-n-partial-w-i" class="headerlink" title="第三项 $\frac{\partial -y_nw^Tx_n}{\partial w_i}$"></a>第三项 $\frac{\partial -y_nw^Tx_n}{\partial w_i}$</h4><p>$$\frac{\partial -y_nw^Tx_n}{\partial w_i} = -y_nx_{n,i}$$</p><p>最后得</p><p>$$\begin{align} \frac{\partial E_{in}(w)}{\partial w_i} &amp;= \frac{1}{N} \sum_{i=1}^{N} \frac{\partial lnA}{\partial A}\cdot \frac{\partial (1+e^B)}{\partial B}\cdot \frac{\partial -y_nw^Tx_n}{\partial w_i} \\  &amp;= \frac{1}{N}\sum_{i=1}^N\frac{1}{A}\cdot e^B \cdot (-y_nx_{n,i}) \\ &amp;= \frac{1}{N}\sum_{i=1}^N\frac{e^{-y_nw^Tx_n}}{1+e^{-y_nw^Tx_n}}\cdot  (-y_nx_{n,i}) \\ &amp;= \frac{1}{N}\sum_{i=1}^N\sigma(-y_nw^Tx_n)\cdot (-y_nx_{n,i})\end{align}$$</p><p>通过梯度更新参数：</p><p>$$w_{t+1} \leftarrow w_t-\eta \triangledown E_{in}(w_t)$$</p><h2 id="多项逻辑回归"><a href="#多项逻辑回归" class="headerlink" title="多项逻辑回归"></a>多项逻辑回归</h2><h1 id="逻辑回归与最大熵"><a href="#逻辑回归与最大熵" class="headerlink" title="逻辑回归与最大熵"></a>逻辑回归与最大熵</h1><p>我们现在尝试把最大熵模型推导成logistic回归模型：</p><p>最大熵模型定义了在给定输入变量$x$时，输出变量$y$的条件分布：</p><p>$$P(y|x,w)=\frac{e^{w\cdot f(x,y)}}{\sum_{y\in Dom(y)} e^{w\cdot f(x,y)}}$$</p><p>如果我们我们限定$y$为二元变量，即$Dom(y) = \{y_0,y_1\}$，那么就可以把最大熵模型转换为logistic回归模型，我们还需要定义特征函数为：</p><p>$$\begin{split} f(x,y)=\begin{cases} g(x), &amp; \text{$y=y_1$} \\ 0, &amp; \text{$y=y_0$}\end{cases}\end{split}$$</p><p>即仅在$y=y_1$时抽取$x$的特征，在$y=y_0$时不抽任何特征（直接返回全为0的特征向量。）</p><p>将这个特征函数带回最大熵模型，我们得到$y=y_1$时：</p><p>$$\begin{align} P(y_1|x) &amp;= \frac{e^{w\cdot f(x,y_1)}}{e^{w\cdot f(x,y_0)}+e^{w\cdot f(x,y_1)}} \\  &amp;= \frac{e^{w\cdot g(x)}}{e^{w\cdot 0}+e^{w\cdot g(x)}} \\  &amp;= \frac{e^{w\cdot g(x)}}{1+e^{w\cdot g(x)}}  \\ &amp;= \sigma(w\cdot g(x)) \\ \end{align}$$</p><p>当$y=y_0$时，得</p><p>$$\begin{align} P(y_0|x) &amp;= \frac{e^{w\cdot f(x,y_0)}}{e^{w\cdot f(x,y_0)}+e^{w\cdot f(x,y_1)}} \\  &amp;= \frac{e^{w\cdot 0}}{e^{w\cdot 0}+e^{w\cdot g(x)}} \\  &amp;= \frac{1}{1+e^{w\cdot g(x)}}  \\ &amp;= \sigma(-w\cdot g(x)) \\&amp;=1-\sigma(w\cdot x) \\ \end{align}$$</p><p>我们发现，当类别标签只有两个时，最大熵模型就是logistic回归模型，表面上看，logistic回归模型里面的特征函数的确只考虑$x$不考虑$y$，然而通过上面的推导，我们发现其实$g$抽取的特征及仅仅在$y=y_1$时被用到。</p><h1 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h1><h2 id="自己实现LR模型"><a href="#自己实现LR模型" class="headerlink" title="自己实现LR模型"></a>自己实现LR模型</h2><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>$$sigmoid(w^Tx+b)=\frac{1}{1+e^{-(w^Tx+b)}}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h3 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h3><ul><li>w : 系数</li><li>b </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_with_zeros</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    w = np.zeros((dim,<span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0.0</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>前向计算cost function</p><ul><li>输入训练数据</li><li>计算 $A=\sigma(w^Tx+b)=(a^{(1)},a^{(2)},…,a^{(m)})$</li><li>计算cost function : $J=-\frac{1}{m}\sum_{i=1}^my^{(i)}log(a^{(i)})+(1-y^{(i)})log(1-a^{(i)})$</li></ul><p>反向计算偏导</p><ul><li>$\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$</li><li>$\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: propagate</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X)+b)                                    <span class="comment"># compute activation</span></span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(np.log(A)*Y + np.log(<span class="number">1</span>-A)*(<span class="number">1</span>-Y))                                <span class="comment"># compute cost</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dw = <span class="number">1</span>/m * X.dot((A - Y).T)</span><br><span class="line">    db = <span class="number">1</span>/m * np.sum(A-Y)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><p>使用梯度下降：$\theta = \theta - \alpha\cdot d\theta$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: optimize</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost and gradient calculation (≈ 1-4 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># update rule (≈ 2 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b - learning_rate*db</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>使用学习算法得到的最佳参数进行预测。$\hat{Y}=A=\sigma(w^TX+b)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m))</span><br><span class="line">    <span class="comment"># w = w.reshape(X.shape[0], 1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="keyword">if</span>(A[<span class="number">0</span>][i] &gt; <span class="number">0.5</span>):</span><br><span class="line">            Y_prediction[<span class="number">0</span>][i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>][i] = <span class="number">0</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><h3 id="完整模型"><a href="#完整模型" class="headerlink" title="完整模型"></a>完整模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize parameters with zeros (≈ 1 line of code)</span></span><br><span class="line">    w, b = np.zeros((X_train.shape[<span class="number">0</span>],<span class="number">1</span>)), <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent (≈ 1 line of code)</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = print_cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict test/train set examples (≈ 2 lines of code)</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1]. 李航. “统计学习方法.” <em>清华大学出版社, 北京</em> (2012).</p><p>[2]. <a href="https://www.wikiwand.com/en/Logistic_function" target="_blank" rel="noopener">维基百科 Logistic function</a></p><p>[3]. <a href="https://www.zhihu.com/question/24094554/answer/108271031" target="_blank" rel="noopener">如何理解最大熵模型里面的特征？</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;逻辑斯蒂回归（logistic regression）是统计学习中的经典方法，逻辑斯蒂回归模型与最大熵模型都属于对数线性模型。&lt;/p&gt;
&lt;h1 id=&quot;Logistic-Function&quot;&gt;&lt;a href=&quot;#Logistic-Function&quot; class=&quot;header
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Classification" scheme="http://conghuai.me/categories/Machine-Learning/Classification/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
      <category term="linear model" scheme="http://conghuai.me/tags/linear-model/"/>
    
  </entry>
  
  <entry>
    <title>Java多线程总结</title>
    <link href="http://conghuai.me/2017/06/20/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%80%BB%E7%BB%93/"/>
    <id>http://conghuai.me/2017/06/20/Java多线程总结/</id>
    <published>2017-06-20T15:26:49.000Z</published>
    <updated>2018-04-17T16:10:50.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="并发基础"><a href="#并发基础" class="headerlink" title="并发基础"></a>并发基础</h1><p>CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停地切换线程执行，让我们感觉多个线程是同时执行的。    CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下一次切换回这个任务时，可以再加载这个任务的状态。</p><ul><li>上下文切换需要时间和开销。</li><li>减少上下文切换的方法：无锁并发编程、CAS算法、使用最少线程和使用协程。</li></ul><h2 id="三个重要概念"><a href="#三个重要概念" class="headerlink" title="三个重要概念"></a>三个重要概念</h2><ul><li><strong>原子性</strong>：一个操作或者多个操作，要么都成功；要么都失败，中间不能由于任何的因素取消。</li><li><strong>可见性</strong>：一个线程对共享变量值的修改，能够及时地被其他线程看到。<ul><li>共享变量：如果一个变量在多个线程的工作内存中都存在副本，那么这个变量就是几个线程的共享变量。 </li></ul></li><li><strong>有序性</strong><ul><li>代码的执行顺序，编写在前面的发生在编写在后面的</li><li>unlock必须发生在lock之后</li><li>volatile修饰的变量，对变量的写操作先于该变量的读操作</li><li>传递规则，操作A先于B，B先于C，那么A肯定先于C</li><li>线程启动规则，start方法肯定先于线程run</li><li>线程中断规则，interrupt这个动作，必须发生在捕获该动作之前</li><li>对象销毁规则，初始化必须发生在finalize之前</li><li>线程终结规则，所有的操作都发生在线程死亡之前</li></ul></li></ul><h2 id="线程安全"><a href="#线程安全" class="headerlink" title="线程安全"></a>线程安全</h2><ul><li>安全性的含义是，永远不发生糟糕的事情，当多个线程访问某个类时，这个类始终都能表现出正确的行为。</li><li>核心在于要对状态访问操作进行管理，特别是对共享的和可变的状态的访问。对象的状态指存储在状态变量中的数据。</li><li>修复多个线程访问同一个可变的状态变量，不使用同步的方法：<ul><li>不在线程之间共享该状态变量</li><li>将状态遍历修改为不可变的变量</li></ul></li></ul><h3 id="无状态性"><a href="#无状态性" class="headerlink" title="无状态性"></a>无状态性</h3><ul><li>不包含任何域，也不包含任何对其他类中域的引用。计算过程中的临时状态仅存在于线程栈上的局部变量中，并且只能由正在执行的线程访问。多个线程之间没有共享状态。</li><li>无状态对象一定是线程安全的。</li></ul><h3 id="竞态条件"><a href="#竞态条件" class="headerlink" title="竞态条件"></a>竞态条件</h3><ul><li>由于多线程共享相同的内存地址空间，并且是并发运行的，因此它们可能会访问或修改其他线程正在使用的变量。</li><li>当某个计算正确性取决于多个线程的交替执行时序时，就会发生竟态条件。观察结果的失效性是大多数竟态条件的本质：<strong>基于一种可能失效的观察结果来做出判断或者执行某个计算</strong>。</li><li>常见的竟态条件<ul><li><strong>先检查后执行</strong>：延迟初始化</li></ul></li></ul><h1 id="重要的概念"><a href="#重要的概念" class="headerlink" title="重要的概念"></a>重要的概念</h1><h2 id="Thread"><a href="#Thread" class="headerlink" title="Thread"></a>Thread</h2><ul><li><p>Thread是线程对象，用来创建和开始一个线程。</p></li><li><p>Thread().start()方法用来启动线程，里面会执行Runnable对象的run()方法，这是采用的是策略设计模式。</p></li><li><p>start0()是native方法，用c++写的，与底层交互。</p></li><li><p>Thread的命名规则：创建线程对象，默认有一个线程名，从Thread-0开始一次加1。</p></li><li><p>如果在构造Thread的时候，没有传递Runnable或者没有复写，则该方法不会调用任何东西。</p></li><li><p>如果构造线程对象时，未传入ThreadGroup，此时会用父线程的ThreadGroup。</p></li><li><p>构造Thread的时候传入stacksize代表着该线程占用的stack大小，如果没有指定stacksize的大小，默认是0, 0代表着会忽略该参数，该参数会被JNI函数去使用。注意：该参数在一些平台有效，在一些平台无效。</p></li><li><p>可以通过ThreadFactory的工厂类来创建线程对象。</p></li><li><p>| name         |<br>| ———— |<br>| priority     |<br>| group        |<br>| tid          |<br>| threadStatus |</p></li></ul><h2 id="Runnable"><a href="#Runnable" class="headerlink" title="Runnable"></a>Runnable</h2><p>Runnable是给线程执行的对象，必须重写run方法，该方法供对象调用。</p><h2 id="Volatile"><a href="#Volatile" class="headerlink" title="Volatile"></a>Volatile</h2><ul><li><strong>volatile</strong>：Java编程语言允许线程访问共享变量，为了确保变量能够准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。将当前处理器缓存行的数据写回到系统内存。<strong>这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效</strong>。<ul><li>volatile变量不会被缓存在寄存器或者其他处理器不可见的地方，因此在读取volatile类型的变量时总会返回<strong>最新</strong>写入的值。</li><li>在访问volatile变量时不会执行加锁操作，但是我们可以把它的行为想成加锁的get和set方法。</li><li><strong>加锁机制既可以确保可见性又可以确保原子性，而volatile变量只能确保可见性</strong>。</li><li>使用条件：<ul><li>对变量的写入操作不依赖变量的当前值，或者你能确保只有单个线程更新变量的值。</li><li>该变量不会与其他状态变量一起纳入不变性条件中。</li><li>在访问变量时不需要加锁。</li></ul></li><li>典型用法：<ul><li>检查某个状态标记以判断是否退出循环。</li></ul></li></ul></li><li>CPU引入cache解决了速度问题，但是又引入了缓存不一致的问题。当CPU在操作的时候，会先在缓存中进行操作，然后将缓存中的内容刷新到CPU中。<br>​          <strong>main memory -&gt; cache （进行操作 如 i++）-&gt; cache -&gt; main memory</strong><br>​          <strong>main memory -&gt; cache （进行操作如 i++）-&gt; cache -&gt; main memory</strong></li></ul><ul><li>解决缓存不一致的方法如下：</li><li><ul><li>给数据总线加锁：当第一个线程访问主内存的时候就对这块内存区域加锁，在线程操作完并写回主内存之前，其他的线程都不可以读取这块主内存的内容。（效率低下）</li><li>CPU高速缓存一致性协议：Intel MESI，核心思想如下：</li><li><ul><li>当cpu写入数据的时候，如果发现该变量被共享（也就是说，在其他cpu也存在该变量的副本），会发出一个信号，通知其他cpu该变量的缓存无效。</li><li>当其他线程访问该变量，重新到内存中读取该变量。</li></ul></li></ul></li></ul><p>结合Java内存模型分析图如下:</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-162034.jpg" alt=""></p><p>volatile关键字可以理解为就是缓存一致性协议。保证各个线程中的缓存内容是一致的。</p><h2 id="Wait-Set"><a href="#Wait-Set" class="headerlink" title="Wait Set"></a>Wait Set</h2><ul><li>如果某个线程调用<strong>LOCK.wait()</strong>，那么该线程会把自己放到<strong>该锁</strong>的wait set中。 </li><li>每一个对象都会有一个wait set，用来存放调用了该对象wait方法之后进入block状态线程</li><li>wait set 本身是一个抽象的概念，具体的实现方式交给不同的JVM去实现。</li><li>线程被notify之后，不一定立即得到执行，线程从wait set中被唤醒顺序不一定是FIFO。</li><li>线程从wait set中被唤醒后<strong>还需要去竞争LOCK</strong> ，但是抢完锁之后，会直接从<strong>地址恢复后</strong>的位置继续向下执行。</li></ul><h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><ul><li>对于费时的任务，我们不希望当前线程等待着任务执行结束才做其他事情，我们希望当前线程可以继续执行后面的操作，我们通过Future对象这个凭证来告知之前任务的情况。（实际上，我们会另开一个线程来执行任务）。</li><li>这种模式中的角色：<ul><li>Future: 代表的是未来的一个凭据</li><li>FutureTask: 将你的调用逻辑进行隔离</li><li>FutureService: 桥接 Future和 FutureTask</li></ul></li></ul><h2 id="读写锁"><a href="#读写锁" class="headerlink" title="读写锁"></a>读写锁</h2><p>读写分离，提高效率；</p><ol><li>read read 并行化</li><li>read write 不允许</li><li>write write 不允许</li></ol><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">READ</th><th style="text-align:center">WRITE</th></tr></thead><tbody><tr><td style="text-align:center"><strong>READ</strong></td><td style="text-align:center">NO</td><td style="text-align:center">YES</td></tr><tr><td style="text-align:center"><strong>WRITE</strong></td><td style="text-align:center">YES</td><td style="text-align:center">YES</td></tr></tbody></table><h2 id="不可变对象"><a href="#不可变对象" class="headerlink" title="不可变对象"></a>不可变对象</h2><p>不可变对象一定是线程安全的，可变对象不一定是不安全的。</p><ul><li>属性定义为private final</li><li>不提供任何改变这个变量的方法</li><li>不让子类继承，即类定义为final</li></ul><p>在JDK中有哪些是不可变的对象呢？</p><ul><li>String</li></ul><p>谈到了String，我们来讲一讲String，StringBuffer和StringBuilder的区别：</p><ol><li>三者中，只有String是不可变的，StringBuffer和StringBuilder都是可变的。</li><li>三者中，String和StringBuffer都是线程安全的，StringBuilder不是线程安全的。前两者保证线程安全的方式不一样，其中String是通过不可变对象来保证的，而StringBuffer是通过synchronized关键字进行加锁保证的，所以，StringBuffer效率比较低。</li></ol><h2 id="线程封闭"><a href="#线程封闭" class="headerlink" title="线程封闭"></a>线程封闭</h2><ul><li>仅在单线程内访问数据，不共享数据。</li><li>栈封闭是线程封闭的一种特例，局部变量的固有属性之一就是封闭在执行线程中，它们位于执行线程的栈中，其他线程无法访问这个栈。</li><li>维持线程封闭性的一种更规范方法是使用<strong>ThreadLocal</strong>, 这个类能使线程中的某个值与保存值的对象关联起来。ThreadLocal提供了get与set等访问接口或方法，这些方法为每个使用该变量的线程都存有一份独立的副本， 因此get总是返回由当前执行线程在调用set时设置的最新值。<ul><li>当某个线程初次调用ThreadLocal.get方法时，就会调用initialValue来获取初始值。</li><li>从概念上来讲，可以将<code>ThreadLocal&lt;T&gt;</code>视为包含了<code>Map&lt;Thread,T&gt;</code>对象，其中保存了特定于该线程的值。</li><li>ThreadLocalRandom是线程本地变量，每个生成随机数的线程都有一个不同的生成器。但都在同一个类中被管理，对于程序员来说时透明的。</li><li>相比于使用共享的Random对象为所有线程生成随机数，这种机制具有更好的性能。</li></ul></li></ul><h1 id="原子操作、原子变量"><a href="#原子操作、原子变量" class="headerlink" title="原子操作、原子变量"></a>原子操作、原子变量</h1><p>原子操作是指，对于访问同一个状态的所有操作来说，这个操作是一个以原子方式执行的操作。</p><p>要保持状态的一致性，就需要在单个原子操作中更新所有相关的状态变量。</p><p>每个Java对象都可以被用做一个实现同步的锁，这些锁被称为内置锁或监视锁。线程在进入同步代码块之前会自动获得锁，并且在退出后自动释放锁。</p><p>Java的内置锁相当于一个互斥体，这意味着最多只有一个线程能持有这种锁。这个锁保护的同步代码块会以原子方式执行。</p><p>每个共享的和可变的变量都应该只由一个锁来保护。</p><p>对于每个包含多个变量的不变性条件，其中涉及的所有变量都需要由同一个锁来保护。</p><p>在访问某个共享且可变的变量时要求所有线程在同一个锁上同步，是为了确保某个线程写入该变量的值对于其他线程来说是可见的。</p><p>加锁的含义不仅仅局限于互斥行为，还包括<strong>内存可见性</strong>，为了确保所有线程都能看到共享变量的最新值，所有执行读操作或者写操作的线程都必须在同一个锁上同步。</p><h2 id="同步锁"><a href="#同步锁" class="headerlink" title="同步锁"></a>同步锁</h2><p><strong>Synchronized</strong>：使得共享资源在多个线程之间访问时采用了同步的方式，避免线程之间的安全问题。</p><ul><li>直接使用synchronized给对象和方法加锁，默认使用的<strong>this</strong>这把锁。同一时刻只有一个执行线程被允许访问，其他线程如果试图访问这个对象的其他方法，都会被挂起。</li><li>如果给静态的方法加锁，那么此时使用的是类名.class 这把锁。对静态方法加锁，同时只能够被一个执行线程访问，但是其他线程可以访问这个对象的非静态方法。</li><li>Java中的每一个对象都可以作为锁。<ul><li>对于普通同步方法，锁是当前实例对象。</li><li>对于静态同步方法，锁是当前类的Class对象。</li><li>对于同步方法块，锁是synchonized括号里配置的对象。</li></ul></li></ul><h2 id="Lock"><a href="#Lock" class="headerlink" title="Lock"></a>Lock</h2><p>Lock是一种比synchonized关键字更强大也更灵活的机制。</p><ul><li>支持更灵活的同步代码块结构。</li><li>提供<strong>tryLock()</strong>方法的实现。这个方法试图获取锁，如果锁已被其他线程获取，他将返回false并继续往下执行代码。</li><li><strong>Lock</strong>允许分离读和写操作，允许多个读线程和一个写线程。</li><li><strong>Condition：</strong><ul><li>一个锁可能关联一个或者多个条件，这些条件通过<strong>Condition</strong>接口声明，目的是允许线程获取锁并且查看等待的某一个条件是否满足。如果不满足就挂起直到某个线程唤醒它们。</li><li>与锁绑定的所有条件对象都是通过<strong>Lock</strong>接口声明的<strong>newCondition</strong>()方法创建的，在使用条件的时候，必须获取这个条件绑定的锁。所以带条件的代码必须在调用Lock对象的lock()和unlock()方法之间。</li><li>当线程调用条件<strong>await</strong>()方法时，它将自动释放这个条件绑定的锁，其他某个线程才可以获取这个锁并且执行相同的操作，或者执行这个锁保护的另一个临界区代码。</li><li><strong>awaitUninterruptibly</strong>()是不可中断的，这个线程将休眠直到其他某个线程调用了将它挂起的条件的<strong>signal</strong>()或<strong>signalAll</strong>()方法。</li></ul></li></ul><h2 id="原子变量"><a href="#原子变量" class="headerlink" title="原子变量"></a>原子变量</h2><ul><li>它能保证<strong>可见性</strong>、<strong>有序性</strong>、<strong>原子性</strong>。</li><li>原子变量是从Java5开始引入的，它提供了单个变量上的原子操作。</li><li>一般来说，这种操作先获取变量值，然后在本地改变变量的值，然后试图用这个改变的值去替换之前的值。如果之前的值没有被其他线程改变，就可以执行这个替换操作。否则，方法将再执行这个操作。这种操作称为<strong>CAS</strong>原子操作。</li><li>采用比较和交换机制不需要使用同步机制，不仅可以避免死锁，而且性能更好。</li><li>在Java中可以通过<strong>锁</strong>和<strong>循环CAS</strong>的方式来实现原子操作。存在的问题：<ul><li><strong>ABA</strong>问题 如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化。那么解决该问题的方法是使用版本号，每次变量更新的时候把版本号加1。JDK提供了一个类AtomicStampedReference来解决ABA问题。</li><li>循环时间开销大。</li><li>只能保证一个共享变量的原子操作。</li></ul></li></ul><h3 id="CAS-Compare-and-swap"><a href="#CAS-Compare-and-swap" class="headerlink" title="CAS(Compare-and-swap)"></a>CAS(Compare-and-swap)</h3><p>In <a href="https://www.wikiwand.com/en/Computer_science" target="_blank" rel="noopener">computer science</a>, <strong>compare-and-swap</strong> (<strong>CAS</strong>) is an <a href="https://www.wikiwand.com/en/Atomic_(computer_science" target="_blank" rel="noopener">atomic</a>) <a href="https://www.wikiwand.com/en/Instruction_(computer_science" target="_blank" rel="noopener">instruction</a>) used in <a href="https://www.wikiwand.com/en/Thread_(computer_science" target="_blank" rel="noopener">multithreading</a>#Multithreading) to achieve <a href="https://www.wikiwand.com/en/Synchronization_(computer_science" target="_blank" rel="noopener">synchronization</a>). It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">compare_and_swap</span><span class="params">(<span class="keyword">int</span>* reg, <span class="keyword">int</span> oldval, <span class="keyword">int</span> newval)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  ATOMIC();</span><br><span class="line">  <span class="keyword">int</span> old_reg_val = *reg;</span><br><span class="line">  <span class="keyword">if</span> (old_reg_val == oldval)</span><br><span class="line">     *reg = newval;</span><br><span class="line">  END_ATOMIC();</span><br><span class="line">  <span class="keyword">return</span> old_reg_val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CAS比较与交换的伪代码可以表示为：</p><p>do{<br>​       备份旧数据；<br>​       基于旧数据构造新数据；<br>}while(!CAS( 内存地址，备份的旧数据，新数据 ))  </p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-17-160954.jpg" alt="191145387966044"></p><p>（上图的解释：CPU去更新一个值，但如果想改的值不再是原来的值，操作就失败，因为很明显，有其它操作先改变了这个值。）</p><p>就是指当两者进行比较时，如果相等，则证明共享数据没有被修改，替换成新值，然后继续往下运行；如果不相等，说明共享数据已经被修改，放弃已经所做的操作，然后重新执行刚才的操作。容易看出 CAS 操作是基于共享数据不会被修改的假设，采用了类似于数据库的 commit-retry 的模式。当同步冲突出现的机会很少时，这种假设能带来较大的性能提升。</p><h3 id="CAS的问题："><a href="#CAS的问题：" class="headerlink" title="CAS的问题："></a>CAS的问题：</h3><p>Some CAS-based algorithms are affected by and must handle the problem of a <a href="https://www.wikiwand.com/en/Type_I_error#False_negative_vs._false_positive" target="_blank" rel="noopener">false positive</a> match, or the <a href="https://www.wikiwand.com/en/ABA_problem" target="_blank" rel="noopener">ABA problem</a>. It is possible that between the time the old value is read and the time CAS is attempted, some other processors or threads change the memory location two or more times such that it acquires a bit pattern which matches the old value. </p><p>解决的方式一般都是多设置一个计数器或者说设置一个版本号。</p><h1 id="线程封闭-1"><a href="#线程封闭-1" class="headerlink" title="线程封闭"></a><strong>线程封闭</strong></h1><ul><li>仅在单线程内访问数据，不共享数据。</li><li>栈封闭是线程封闭的一种特例，局部变量的固有属性之一就是封闭在执行线程中，它们位于执行线程的栈中，其他线程无法访问这个栈。</li><li>维持线程封闭性的一种更规范方法是使用<strong>ThreadLocal</strong>, 这个类能使线程中的某个值与保存值的对象关联起来。ThreadLocal提供了get与set等访问接口或方法，这些方法为每个使用该变量的线程都存有一份独立的副本， 因此get总是返回由当前执行线程在调用set时设置的最新值。<ul><li>当某个线程初次调用ThreadLocal.get方法时，就会调用initialValue来获取初始值。</li><li>从概念上来讲，可以将<code>ThreadLocal&lt;T&gt;</code>视为包含了<code>Map&lt;Thread,T&gt;</code>对象，其中保存了特定于该线程的值。</li><li>ThreadLocalRandom是线程本地变量，每个生成随机数的线程都有一个不同的生成器。但都在同一个类中被管理，对于程序员来说时透明的。</li><li>相比于使用共享的Random对象为所有线程生成随机数，这种机制具有更好的性能。</li></ul></li></ul><h2 id="ThreadLocal"><a href="#ThreadLocal" class="headerlink" title="ThreadLocal"></a>ThreadLocal</h2><ul><li>可以理解为用HashMap存储数据，Key是Thread，Value是数据。</li><li>通过重写<code>initialValue</code>方法来定义初始值。</li><li>可以用于线程上下文带来的参数传递。</li></ul><h1 id="并发数据结构"><a href="#并发数据结构" class="headerlink" title="并发数据结构"></a>并发数据结构</h1><h2 id="并发集合"><a href="#并发集合" class="headerlink" title="并发集合"></a>并发集合</h2><p>Java提供了一些可以用于并发程序中的数据结合，分阻塞式和非阻塞式的。</p><h3 id="ConcurrentHashMap"><a href="#ConcurrentHashMap" class="headerlink" title="ConcurrentHashMap"></a>ConcurrentHashMap</h3><p>ConcurrentHashMap也是基于散列的Map，但它使用了不同的加锁策略来提供更高的并发性和伸缩性。采用了一种粒度更细的加锁机制来实现更大程度的共享，称为分段锁。</p><h3 id="CopyOnWriteArrayList"><a href="#CopyOnWriteArrayList" class="headerlink" title="CopyOnWriteArrayList"></a>CopyOnWriteArrayList</h3><p>在迭代期间不需要对容器进行加锁或复制。在每次修改时，都会创建并重新发布一个新的容器副本，从而实现可变性。</p><p>仅当迭代操作远远多于修改操作时，才应该使用写入时复制。</p><h3 id="BlockingQueue"><a href="#BlockingQueue" class="headerlink" title="BlockingQueue"></a>BlockingQueue</h3><p>BlockingQueue简化了生产者-消费者设计的实现过程，它支持任意数量的生产者和消费者。</p><p>一种最常见的生产者-消费者设计模式就是线程池与工作队列的组合，在Executor任务执行框架中就体现这种模式。</p><ul><li>LinkedBlockingQueue</li><li>ArrayBlockingQueue</li><li>SynchronousQueue</li><li>Deque</li><li>ConcurrentLinkedDeque</li><li>LinkedBlockingDeque</li><li>PriorityBlockingQueue</li><li>DelayQueue</li><li>ConcurrentSkipListMap</li></ul><h2 id="线程通信"><a href="#线程通信" class="headerlink" title="线程通信"></a>线程通信</h2><p>我们希望多个线程之间能够相互之间交换信息，从而能够更好的协作，完成任务。线程之间通过wait notify方法进行通信。同步工具类可以是任何一个对象，只要它根据其自身的状态来协调线程的控制流，阻塞队列可以作为同步工具类，其他类型的同步工具类包括<strong>信号量</strong>、<strong>栅栏</strong>、以及<strong>闭锁</strong>。</p><p>线程之间的通信机制有两种：共享内存和消息传递。</p><ul><li>共享内存，线程之间共享程序的公共状态，通过写-读内存中的公共状态来隐式通信。</li><li>消息传递，线程之间必须通过发送消息来显式通信。</li></ul><h3 id="Interrupt-wait-notify"><a href="#Interrupt-wait-notify" class="headerlink" title="Interrupt wait notify"></a>Interrupt wait notify</h3><ul><li>Thread提供了<strong>interrupt</strong>方法，用于中断线程或者查询线程是否已经被中断。每个线程都有一个布尔类型的属性，表示线程的中断状态。</li><li>中断是一种协作机制。当线程A中断B时，A仅仅是要求B在执行到某个可以暂停的地方停止正在执行的操作-<strong>前提是如果线程B愿意停止下来</strong>。</li><li>调用interrupt并不意味着立即停止目标线程正在进行的工作，而是传递了请求中断的消息。</li><li>由于每个线程拥有各自的中断策略，因此除非你知道中断对该线程的含义，否则就不应该中断这个线程。</li><li>所有的对象都会有一个wait set， 用来存放调用了该对象wait方法之后进入block状态线程。</li><li>线程从wait set中被唤醒顺序不一定是FIFO。</li><li>wait方法会释放锁对象，当被唤醒之后需要再去请求锁对象，得到锁对象之后，代码会从wait的地方开始往下执行。</li></ul><h3 id="Latch"><a href="#Latch" class="headerlink" title="Latch"></a>Latch</h3><p>闭锁是一种不同工具类，可以延迟线程的进度直到其到达终止状态。闭锁相当于一扇门，在闭锁到达结束状态之前，这扇门一直是关闭的。闭锁可以用来确保某些活动直到其他活动都完成后才继续执行。</p><ul><li><strong>CountDownLatch</strong><ul><li>CountDownLatch是一种灵活的闭锁实现，可以在上述各种情况使用，它可以是一个或多个线程等待一组事件发生。</li><li>这个类使用一个<strong>整数</strong>进行初始化，这个整数就是线程要等待完成的操作的数目。当一个线程要等待某些操作先执行完时，需要调用await()方法，这个方法让线程进入休眠直到等待的所有操作都完成，当某个操作完成后，它将调用countDown()方法将CountDownLatch类的内部计数器减1,，当计数器变为0的时候，CountDownLatch将唤醒所有调用await()方法而进入休眠的线程。</li><li>CountDownLatch类有三个基本元素：<ul><li>一个初始值，即定义必须等待的先行完成的操作的数目。</li><li>await()方法</li><li>countDown()方法，每个被等待的事件在完成的时候调用。</li></ul></li><li>CountDownLatch机制<ul><li>CountDownLatch机制不是用来保护共享资源或者临界区的，它是用来同步执行多个任务的一个或者多个线程；</li><li>CountDownLatch只准许进入一次。</li></ul></li></ul></li></ul><h3 id="Counting-Semaphore"><a href="#Counting-Semaphore" class="headerlink" title="Counting Semaphore"></a>Counting Semaphore</h3><p>计算信号量用来控制同时访问某个特定资源的操作数量，或者执行某个指定操作的数量。</p><p>Semaphore中管理着一组虚拟的许可，许可的初始数量可通过构造函数来指定，在执行操作时可以首先获得许可，并在使用以后释放许可。如果没有许可，那么acquire将阻塞直到有许可。release方法将返回一个许可给信号量。</p><p>Semaphore可以用来实现资源池。</p><h3 id="Barrier"><a href="#Barrier" class="headerlink" title="Barrier"></a>Barrier</h3><p>栅栏类似于闭锁，它能阻塞一组线程直到某个事件发生。栅栏和闭锁的关键区别在于，所有线程必须同时到达栅栏位置，才能继续执行。闭锁用于等待事件，而栅栏用于等待其他线程。</p><ul><li><strong>CyclicBarrier</strong><ul><li>CyclicBarrier可以使一定数量的参与方反复的在栅栏位置汇集。</li><li>CyclicBarrier类使用一个整型数进行初始化，这个数是需要在某个点上同步的线程数。</li><li>当一个线程到达的指定的点后，它将调用await()方法等待其他的线程，当线程调用await()方法后，CyclicBarrier类将阻塞这个线程并使之休眠直到所有其他线程到达。当最后一个线程调用CyclicBarrier类的await()方法时，CyclicBarrier对象将唤醒所有在等待的线程，然后这些线程将继续执行。</li><li>CyclicBarrier类有一个很有意义的改进，即它可以传入另一个Runnable对象作为初始化参数。当所有的线程都到达集合点后，CyclicBarrier类将这个Runnable对象作为线程执行。</li></ul></li></ul><h3 id="Phaser"><a href="#Phaser" class="headerlink" title="Phaser"></a><strong>Phaser</strong></h3><p>允许执行并发多阶段任务。当我们有并发任务并且需要分解成几步执行时，这种机制就非常适用。Phaser类机制是在每一步结束的位置对线程进行同步，当所有的线程都完成了这一步，才允许执行下一步。</p><h3 id="Exchanger"><a href="#Exchanger" class="headerlink" title="Exchanger"></a>Exchanger</h3><p>Exchanger允许在并发任务之间交换数据。Exchanger类允许在两个线程之间定义同步点，当两个线程都到达同步点时，它们交换数据结构，因此第一个线程的数据结构进入到第二个线程中，同时第二个线程的数据结构进入到第一个线程中。</p><h2 id="并发编程框架"><a href="#并发编程框架" class="headerlink" title="并发编程框架"></a>并发编程框架</h2><h3 id="Executor框架"><a href="#Executor框架" class="headerlink" title="Executor框架"></a>Executor框架</h3><p>任务是一组逻辑工作单元，而线程则是使任务异步执行的机制。串行执行的问题在于其糟糕的响应性和吞吐量，而“为每个任务分配一个线程”的问题在于资源管理的复杂性。</p><p>通过使用Executor，可以实现各种調优、管理、监视、记录日志、错误报告和其他功能，如果不使用任务执行框架，那么要增加这些功能非常困难。</p><p>当运行大量的并发任务，手动创建Runnable对象和Thread对象来执行它们有如下缺点:</p><ul><li>必须实现所有与Thread对象管理相关的代码，比如线程的创建、结束以及结果获取。</li></ul><p>执行器的另一个优势是Callable接口，这个接口的主方法名称为call()，可以返回结果。当发送一个Callable对象给执行器时，将获得一个实现了Future接口的对象，可以使用这个对象控制Callable对象的状态和结果。</p><p>执行器框架(Executor Framework)将任务的创建和执行进行了分离，通过这个框架，只需要实现Runnable接口的对象和使用Executor对象，然后将Runnable对象发送给执行器。执行器再负责运行这些任务所需要的线程，包括线程的创建，线程的管理以及线程的结束。</p><p>它提供了一种标准的方法将任务的提交过程与执行过程解偶开来，并用Runnable来表示任务。</p><p>Executor的实现还提供了生命周期的支持，以及统计信息收集，应用程序管理机制和性能监视等机制。</p><p>Executor基于生产者-消费者模式，提交任务的操作相当于生产者，执行任务的线程则相当于消费者。</p><h3 id="ExecutorService"><a href="#ExecutorService" class="headerlink" title="ExecutorService"></a>ExecutorService</h3><ul><li>为了解决执行服务的生命周期问题，Executor扩展了ExecutorService接口，添加了一些用于生命管理的方法。</li><li>在ExecutorService关闭后提交的任务将由“拒绝执行处理器”来处理，它会抛弃任务，或者使得execute方法将转入终止状态。</li></ul><h3 id="ThreadPool"><a href="#ThreadPool" class="headerlink" title="ThreadPool"></a>ThreadPool</h3><ul><li>线程池，是指管理一组同构工作线程的资源池。</li><li>在线程池中执行任务比为每个任务分配一个线程优势更多，通过重用现有的线程而不是创建新线程，可以在处理多个请求时分摊在线程创建和销毁过程中产生的巨大开销。</li><li>可以通过调用Executors中的静态工厂方法之一来创建一个线程池。</li><li>当有界队列被填满后，饱和策略开始发挥作用。ThreadPoolExecutor的饱和策略可以通过调用setRejectedExecutionHandler来修改。<ul><li><strong>AbortPolicy</strong>：中止策略，是默认的饱和策略，该策略抛出未经检查的RejectedExecutionException。</li><li><strong>CallerRunsPolicy</strong>：调用者策略，实现了一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。</li></ul></li><li>每当线程池需要创建一个线程时，都是通过线程工厂方法来完成的。默认的线程工厂方法将创建一个新的、非守护的线程，并且不包含特殊的配置信息。在ThreadFactory中只定义了一个方法newThread，每当线程池需要创建一个新线程时，都会调用这个方法。</li></ul><table><thead><tr><th style="text-align:center">方法</th><th style="text-align:center">作用</th></tr></thead><tbody><tr><td style="text-align:center">newFixedThreadPool</td><td style="text-align:center">创建一个固定长度的线程池；<br>在默认情况下将使用一个无界的LinkedBlockingQueue。</td></tr><tr><td style="text-align:center">newCachedThreadPool</td><td style="text-align:center">1. 创建一个可缓存的线程池，如果线程池的当前规模超过了处理需求时，那么将回收空闲的线程，而当需求增加时，则可以添加新的线程，线程池的规模不存在任何限制。<br>2. 对于非常大的或者无界的线程池，可以通过使用SynchronousQueue来避免任务排队。<br>3. 只有当线程池是无界的或者可以拒绝任务时，SynchronousQueue才有实际价值。</td></tr><tr><td style="text-align:center">newSingleThreadExecutor</td><td style="text-align:center">1. 是一个单线程的Executor，它创建单个工作者线程来执行任务，如果这个线程异常结束，会创建另一个线程来替代。newSingleThreadExecutor能确保依照任务在队列中的顺序来串行执行。<br></td></tr><tr><td style="text-align:center">newScheduledThreadPool</td><td style="text-align:center">1. 创建一个固定长度的线程池，而且以延迟或定时的方式来执行任务，类似于Timer。<br>2.为了在定时执行器中等待一段给定的时间后执行一个任务，需要使用schedule()方法。</td></tr><tr><td style="text-align:center">Customized ThreadPoolExecutor</td><td style="text-align:center">1. Java并发API提供了大量接口和类来实现并发应用程序，这些接口和类既包含底层机制，如Thread类，Runnable接口或Callable接口，synchronized关键字，也包含了高层机制，如Executor框架，尽管如此，开发应用中，仍会发现已有的Java类无法满足需求。<br>2. 步骤：<br>2.1 实现一个接口以拥有接口定义的功能，如 ThreadFactory接口。<br>2.2 覆盖类的一些方法，改变这些方法的行为，来满足需求，例如，覆盖Thread类的run()方法。<br>3. 任务：<br>3.1 通过Runnable接口实现的任务，不返回结果。<br>3.2 通过Callable接口实现的任务，它返回结果。</td></tr></tbody></table><h3 id="Callable"><a href="#Callable" class="headerlink" title="Callable"></a>Callable</h3><ul><li>Callable这个接口声明了call()方法，可以在这个方法里实现任务的具体逻辑操作。Callable接口时一个泛型接口，这就意味着必须声明call()方法返回的数据类型。</li><li>Future这个接口声明了一些方法来获取由Callabel对象产生的结果，并管理它们的状态。</li></ul><h3 id="ThreadFactory"><a href="#ThreadFactory" class="headerlink" title="ThreadFactory"></a>ThreadFactory</h3><p>工厂模式在面向对象编程中是一个应用广泛的设计模式，它是一种创建模式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;并发基础&quot;&gt;&lt;a href=&quot;#并发基础&quot; class=&quot;headerlink&quot; title=&quot;并发基础&quot;&gt;&lt;/a&gt;并发基础&lt;/h1&gt;&lt;p&gt;CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停
      
    
    </summary>
    
      <category term="Software Engineer" scheme="http://conghuai.me/categories/Software-Engineer/"/>
    
      <category term="Java" scheme="http://conghuai.me/categories/Software-Engineer/Java/"/>
    
      <category term="Concurrency" scheme="http://conghuai.me/categories/Software-Engineer/Java/Concurrency/"/>
    
    
      <category term="java" scheme="http://conghuai.me/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Perceptron Learning Algorithm</title>
    <link href="http://conghuai.me/2017/04/14/Perceptron-Learning-Algorithm/"/>
    <id>http://conghuai.me/2017/04/14/Perceptron-Learning-Algorithm/</id>
    <published>2017-04-14T07:25:03.000Z</published>
    <updated>2018-04-14T08:53:24.480Z</updated>
    
    <content type="html"><![CDATA[<p>感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。</p><h1 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h1><p>假设输入空间（特征空间）是$\chi \subseteq  R^n$，输出空间是$Y={+1, -1}$。输入$x\in \chi$表示实例的特征向量，对应于输入空间（特征空间）的点；输出为$y\in Y$表示实例的类别，由输入空间到输出空间的如下函数：</p><p>$$f(x) = sign(w\cdot x+b)$$</p><p>称为感知机。其中，$w$和$b$为感知机模型参数，$w\in R^n$称为权值向量，$b\in R$称为偏置，sign是符号函数，即：</p><p>$$\begin{split}sign(x)=\begin{cases} +1, &amp; \text{$x\geq 0$} \\ -1, &amp; \text{$x&lt;0$}\end{cases}\end{split}$$</p><p>感知机模型的假设空间是定义在特征空间中的所有线性分类模型，即函数集合$\{f|f(x)=w\cdot x + b\}$。</p><h2 id="几何解释"><a href="#几何解释" class="headerlink" title="几何解释"></a>几何解释</h2><p>线性方程$w\cdot x+b=0$，对应于特征空间$R^n$中的一个超平面$S$，其中$w$是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分，位于两部分的点分别被称为正、负两类。</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-084814.jpg" alt="Screen Shot 2018-04-14 at 16.14.35"></p><h1 id="学习策略"><a href="#学习策略" class="headerlink" title="学习策略"></a>学习策略</h1><p>假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数$w,b$，需要确定一个学习策略，即定义（经验）损失函数并将损失函数极小化。</p><p>损失函数选择是误分类点到超平面$S$的总距离：</p><p><img src="http://p6sh0jwf6.bkt.clouddn.com/2018-04-14-084817.jpg" alt="distance"></p><p>对于误分类的数据$(x_i,y_i)$来说，当$w\cdot x+b &gt;0$时，$y_i=-1$，而当$w\cdot x_i+b&lt;0$时，$y_i=+1$，则我们可以得出，误分类点$x_i$到超平面$S$的距离是：</p><p>$$-\frac{1}{||w||}y_i(w\cdot x_i+b)$$</p><p>这样，假设超平面$S$的误分类点集合为M，那么所有误分类点到超平面S的总距离为：</p><p>$$-\frac{1}{||w||}\sum _{x_i\in M}y_i(w\cdot x_i+b)$$</p><p>不考虑$\frac{1}{||w||}$，就得到感知机学习的损失函数：</p><p>$$L(w,b)=-\sum_{x_i \in M}y_i(w\cdot x_i+b)$$</p><h1 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h1><p>感知机学习算法是对以下最优化问题的算法那，给定一个训练数据集$T=\{(x_1, y_1),(x_2,y_2),…,(x_N,y_N)\}$，求参数$w,b$，使其为以下损失函数极小化问题的解：</p><p>$$min_{w,b} L(w,b)=-\sum_{x_i \in M}y_i(w\cdot x_i + b)$$</p><p>感知机学习算法是误分类驱动的，具体采用随机梯度下降法。首选，任意选取一个超平面$w_0,b_0$，然后用梯度下降法不断地极小化目标函数。极小化过程中不是一次使M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。</p><p>假设误分类点集合M是固定的，那么损失函数$L(w,b)$的梯度由</p><p>$$\triangledown_wL(w,b)=-\sum_{x_i\in M}y_ix_i$$</p><p>$$\triangledown_bL(w,b)=-\sum_{x_i\in M}y_i$$</p><p>给出，随机选择一个误分类点$(x_i, y_i)$，对$w,b$进行更新：</p><p>$$w\leftarrow w+\eta y_ix_i$$</p><p>$$b\leftarrow b+\eta y_i$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。&lt;/p&gt;
&lt;h1 id=&quot;感知机模型&quot;&gt;&lt;a href=&quot;#感知机模型&quot; class=&quot;header
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://conghuai.me/categories/Machine-Learning/"/>
    
      <category term="Classification" scheme="http://conghuai.me/categories/Machine-Learning/Classification/"/>
    
    
      <category term="ml" scheme="http://conghuai.me/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>网页正文抽取算法</title>
    <link href="http://conghuai.me/2016/10/05/%E7%BD%91%E9%A1%B5%E6%AD%A3%E6%96%87%E6%8A%BD%E5%8F%96%E7%AE%97%E6%B3%95/"/>
    <id>http://conghuai.me/2016/10/05/网页正文抽取算法/</id>
    <published>2016-10-05T07:51:56.000Z</published>
    <updated>2018-04-09T05:47:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>在爆炸式增长的互联网时代，互联网上有大量的资源，如何爬取这些资源成为一个热门的研究点。其中如何高效的对网页正文进行抽取、清洗和存储成为一个重要的研究领域。但是，在网页上，除了正文部分，通常还会包含大量的导航栏、广告、版权等信息。相较于正文，这些信息对于我们来说用处不是很大，这部分信息，在网页正文抽取中，被称为噪声信息。为了提高网页正文采集的性能，我们需要把这这些噪声去除。</p><p>在这篇博文中，会介绍几个比较经典、效果也比较好的算法，一是<strong>CETD：Content Extraction via Text Density</strong>；二是<strong>CETR：Content Extraction via Tag Ratios。</strong>三是<strong>CEPR：Content Extraction via Path Ratios</strong>。</p><h1 id="CETD：Content-Extraction-via-Text-Density"><a href="#CETD：Content-Extraction-via-Text-Density" class="headerlink" title="CETD：Content Extraction via Text Density"></a>CETD：Content Extraction via Text Density</h1><p>该算法是主要思想是：在典型的网页结构中，噪声信息（指正文信息以外）通常被高度格式化，因此包含的文本信息通常很少，而正文通常包含大量文本。而且，正文通常在页面中保持完整性，即其内容通常不会被分到多个DOM树节点中。</p><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ul><li>没有对页面结构作任何假设。</li><li>保持原页面的信息。</li></ul><h2 id="Text-Density"><a href="#Text-Density" class="headerlink" title="Text Density"></a>Text Density</h2><ul><li>CharNumber：该节点下所有子树中的字符数；</li><li>TagNumber：该节点下所有子树的标签数；</li></ul><p>定义Text Density为：$TD_i=\frac{C_i}{T_i}$,$C_i$表示CharNumber，$T_i$表示TagNumber，当$T_i$为0时，将其设置为1。通常来说，该值越高，该节点内容越有可能是正文。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Algorithm <span class="number">1</span> <span class="function">Pseudocode of <span class="title">ComputeDensity</span><span class="params">(N)</span></span></span><br><span class="line">1: INPUT: N </span><br><span class="line"><span class="number">2</span>: OUTPUT: N </span><br><span class="line"><span class="number">3</span>: <span class="keyword">for</span> all child node C in N <span class="keyword">do</span> </span><br><span class="line"><span class="number">4</span>:    ComputeDensity(C)</span><br><span class="line"><span class="number">5</span>: end <span class="keyword">for</span> </span><br><span class="line"><span class="number">6</span>: N.CharNumber ←CountChar(N) </span><br><span class="line"><span class="number">7</span>: N.TagNumber ←CountTag(N) </span><br><span class="line"><span class="number">8</span>: <span class="keyword">if</span> N.TagNumber == <span class="number">0</span> then </span><br><span class="line"><span class="number">9</span>:    N.TagNumber ←<span class="number">1</span></span><br><span class="line"><span class="number">10</span>: end <span class="keyword">if</span> <span class="number">11</span>: N.Density ←N.CharNumber/N.TagNumber</span><br></pre></td></tr></table></figure><h2 id="Composite-Text-Density"><a href="#Composite-Text-Density" class="headerlink" title="Composite Text Density"></a>Composite Text Density</h2><p>在Text Density的基础上加了关于超链接的统计信息。论文作者经过研究发现，大部分的噪声节点中都包含超链接，这个信息可以用来进一步判断该节点内容是正文还是噪声。基于这个发现，定义另外两个统计信息：</p><ul><li>LinkCharNumber：该节点下所有子树中的超链接字符数；</li><li>LinkTagNumber：该节点下所有子树中的超链接标签数；</li></ul><p>定义Composite Text Density为：$CTD_i=\frac{C_i}{T_i}log_{ln(\frac{C_i}{-LC_i}LC_i+\frac{LC_b}{C_b}C_i+e)}(\frac{C_i}{LC_i}\frac{T_i}{LT_i})$</p><ul><li>$C_i$：字符数；</li><li>$T_i$：标签数</li><li>$LC_i$：链接字符；</li><li>$-LC_i$：非链接字符；</li><li>$LT_i$：链接标签；</li><li>$LC_b$：<code>&lt;body&gt;</code>标签下的超链接字符数</li><li>$C_b$：<code>&lt;body&gt;</code>标签下的字符数</li></ul><h2 id="Content-Extraction"><a href="#Content-Extraction" class="headerlink" title="Content Extraction"></a>Content Extraction</h2><p>通过计算每个节点的TD或CTD，我们可以根据该值来判断是否抽取该节点下的文本当做正文，判断方式就是设置一个阈值，如果大于该阈值就抽取，小于该阈值就不抽取。论文作者用了一个非常巧妙的值当做阈值，即<code>&lt;body&gt;</code>的Text Density。</p><h2 id="DensitySum"><a href="#DensitySum" class="headerlink" title="DensitySum"></a>DensitySum</h2><p>在实践中发现，有一些正文包含的Text Density值很低，如正文的日期，正文的引用等。论文作者发现，一般来说，正文块都是属于DOM树中的某个祖先节点的，又因为正文节点的text density大于噪声节点，所以正文块对应的节点，如果把它所有孩子的text densities相加，将会得到最大的text densities值。从而可以通过DensitySum可以解决该问题，定义DensitySum为：$DensitySum_N=\sum_{i\in C}TextDensity_i$，$C$是N的孩子集合。<br>在算法的具体实现中，如果网页只包含一个content block，我们只需要在<code>&lt;body&gt;</code>标签下寻找最大的DensitySum，然后将其标记为content即可。对于有多个content block的情况，我们需要对于text density大于阈值的所有节点，都用上述方法进行抽取。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Algorithm <span class="number">2</span> <span class="function">Pseudocode of <span class="title">ExtractContent</span><span class="params">(N)</span></span></span><br><span class="line">1: INPUT: N </span><br><span class="line"><span class="number">2</span>: <span class="keyword">if</span> N.TextDensity &gt;= threshold then </span><br><span class="line"><span class="number">3</span>:       T ←FindMaxDensitySumTag(N)</span><br><span class="line"><span class="number">4</span>:       MarkContent(T) </span><br><span class="line"><span class="number">5</span>:       <span class="keyword">for</span> all child node C in N <span class="keyword">do</span></span><br><span class="line"><span class="number">6</span>:          ExtractContent(C)</span><br><span class="line"><span class="number">7</span>:       end <span class="keyword">for</span></span><br><span class="line"><span class="number">8</span>: end <span class="keyword">if</span></span><br></pre></td></tr></table></figure><p>上面说过，论文作者将<code>&lt;body&gt;</code>标签的text density当做是阈值，但是在实践中，有一些content block的text density会低于该阈值，这会导致正文内容的丢失。为了解决这一问题，先把阈值设为0，然后找到最大的DensitySum的标签。紧接着，从<code>&lt;body&gt;</code>标签到该标签路径上，将最小的text density值设为阈值。</p><h1 id="CEPR：Content-Extraction-via-Path-Ratios"><a href="#CEPR：Content-Extraction-via-Path-Ratios" class="headerlink" title="CEPR：Content Extraction via Path Ratios"></a>CEPR：Content Extraction via Path Ratios</h1><p>该算法能够通过$TPR/ETPR$直方图快速、准确的从网页中提取新闻内容。</p><h2 id="Document-Object-Model"><a href="#Document-Object-Model" class="headerlink" title="Document Object Model"></a>Document Object Model</h2><p><img src="https://github.com/conghuaicai/cs-skill-tree/raw/master/spider/web%20content%20extraction/images/1.png" alt="文档树"></p><h2 id="Extended-Labeled-Ordered-Tree"><a href="#Extended-Labeled-Ordered-Tree" class="headerlink" title="Extended Labeled Ordered Tree"></a>Extended Labeled Ordered Tree</h2><p>正文和噪声的主要区别有：</p><ol><li>正文通常只包含在一个部分中，但是噪声信息在很多部分中都会出现；</li><li>正文通常都有相同的tag paths；</li></ol><p><strong>定义：</strong></p><ul><li>$L={l_0,l_1,l_2,…}$,$l_i$表示标记，即tag；</li><li>$T=(V,E,v_0,\prec ,L,l(\cdot),c(\cdot))$<ul><li>$l:V\rightarrow L$是label function，即求出节点所属的标签$l(v)$；</li><li>$c：V\rightarrow String$是content function，即求出节点的本文内容$c(v)$；</li></ul></li></ul><h2 id="Tag-Path"><a href="#Tag-Path" class="headerlink" title="Tag Path"></a>Tag Path</h2><p>$l(v_0),l(v_1)…l(v_k)$称为节点v的tag path，表示为$path(v)$。</p><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><ol><li>正文节点有相似的tag paths；</li><li>噪声节点有相似的tag paths；</li><li>正文节点包含更多的文本数据；</li><li>噪声节点包含更少的文本数据；</li><li>所有的节点都是叶子节点；</li></ol><h2 id="Text-to-Tag-Path-Ratio"><a href="#Text-to-Tag-Path-Ratio" class="headerlink" title="Text to Tag Path Ratio"></a>Text to Tag Path Ratio</h2><ul><li><strong>pathNum</strong>：tag path在tree T中出现的次数，这个概念比较容易造成误解，现在的理解是，叶节点是不算在tag path里面的。tag path是一个tag序列标识，tag下可能包含多个叶节点，这些叶结点对应的tag path就是一样的，所以这个序列标识是有可能重复的。</li><li><strong>txtNum</strong>：节点中所有字符的个数；</li><li>$accNodes(p)={v_p^1,v_p^2,…,v_p^m}$是tag path p上可访问的节点的集合；</li><li><strong>Text to Tag Path Radio：</strong>$TPR(p)=\frac{\sum_{v\in accNodes(p)}length(c(v))}{|accNodes(p)|}$<ul><li>对于包含长文本的路径，该值很高；</li><li>对于其他路径，该值很低；</li></ul></li></ul><p>举例子说明：</p><p>上述文档图的TPR计算方式如下:</p><ol><li>#1 text node : <code>tag path = &lt;div.div.div.h1&gt;</code>，txtNum = 40, pathNum=1, TPR=40；</li><li>#2 text node: <code>tag path = &lt;div.div.div.p&gt;</code>，txtNum=645，pathNum=2(因为这个序列出现两次)，TPR=322.5；</li><li>#3 text node: <code>tag path=&lt;div.div.div.p.a&gt;</code>，txtNum=7，pathNum=1，TPR=7；</li><li>#4 text node: <code>tag path=&lt;div.div.div.p&gt;</code>，txtNum=645，pathNum=2，TPR=322.5；</li></ol><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p><img src="https://github.com/conghuaicai/cs-skill-tree/raw/master/spider/web%20content%20extraction/images/2.png" alt="算法流程"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>[1] Sun, F., Song, D., &amp; Liao, L. (2011). DOM based content extraction via text density. Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information - SIGIR ’11, l, 245. </p><p>[2] Wu, G., Li, L., Hu, X., &amp; Wu, X. (2013). Web news extraction via path ratios. Proceedings of the 22nd ACM International Conference on Conference on Information &amp; Knowledge Management - CIKM ’13, 2059–2068.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在爆炸式增长的互联网时代，互联网上有大量的资源，如何爬取这些资源成为一个热门的研究点。其中如何高效的对网页正文进行抽取、清洗和存储成为一个重要的研究领域。但是，在网页上，除了正文部分，通常还会包含大量的导航栏、广告、版权等信息。相较于正文，这些信息对于我们来说用处不是很大，
      
    
    </summary>
    
      <category term="Spider" scheme="http://conghuai.me/categories/Spider/"/>
    
      <category term="Content Extraction" scheme="http://conghuai.me/categories/Spider/Content-Extraction/"/>
    
    
      <category term="algorithm" scheme="http://conghuai.me/tags/algorithm/"/>
    
  </entry>
  
</feed>
